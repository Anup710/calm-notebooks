{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f22c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2813db",
   "metadata": {},
   "source": [
    "for rapid prototyping with new ideas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d346d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save to a file\n",
    "with open('input.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3171d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65ec383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec7f4d",
   "metadata": {},
   "source": [
    "`198` is `\\n` or newline just to verify correctness sometimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3438a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch = tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]]) \n",
      "output batch = tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "# create inputs and outputs using .view operations\n",
    "buf = torch.tensor(tokens[:24+1])\n",
    "\n",
    "x = buf[:-1].view(4,6)\n",
    "# output is input shifted by 1\n",
    "y = buf[1:].view(4,6) \n",
    "\n",
    "print(f\"input batch = {x} \\noutput batch = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63b348",
   "metadata": {},
   "source": [
    "### Weight tying - exploring openai wts for lm_head and token embedding (wte) tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795fd280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AN80050181\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\AN80050181\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AN80050181\\.cache\\huggingface\\hub\\models--GPT2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "hf = GPT2LMHeadModel.from_pretrained(\"GPT2\") # 124M, use GPT2-XL for actual 1.5b model\n",
    "sd_hf = hf.state_dict() # stores parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f7cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight | torch.Size([50257, 768])\n",
      "transformer.wpe.weight | torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight | torch.Size([768])\n",
      "transformer.h.0.ln_1.bias | torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.0.ln_2.weight | torch.Size([768])\n",
      "transformer.h.0.ln_2.bias | torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.1.ln_1.weight | torch.Size([768])\n",
      "transformer.h.1.ln_1.bias | torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.1.ln_2.weight | torch.Size([768])\n",
      "transformer.h.1.ln_2.bias | torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.2.ln_1.weight | torch.Size([768])\n",
      "transformer.h.2.ln_1.bias | torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.2.ln_2.weight | torch.Size([768])\n",
      "transformer.h.2.ln_2.bias | torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.3.ln_1.weight | torch.Size([768])\n",
      "transformer.h.3.ln_1.bias | torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.3.ln_2.weight | torch.Size([768])\n",
      "transformer.h.3.ln_2.bias | torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.4.ln_1.weight | torch.Size([768])\n",
      "transformer.h.4.ln_1.bias | torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.4.ln_2.weight | torch.Size([768])\n",
      "transformer.h.4.ln_2.bias | torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.5.ln_1.weight | torch.Size([768])\n",
      "transformer.h.5.ln_1.bias | torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.5.ln_2.weight | torch.Size([768])\n",
      "transformer.h.5.ln_2.bias | torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.6.ln_1.weight | torch.Size([768])\n",
      "transformer.h.6.ln_1.bias | torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.6.ln_2.weight | torch.Size([768])\n",
      "transformer.h.6.ln_2.bias | torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.7.ln_1.weight | torch.Size([768])\n",
      "transformer.h.7.ln_1.bias | torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.7.ln_2.weight | torch.Size([768])\n",
      "transformer.h.7.ln_2.bias | torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.8.ln_1.weight | torch.Size([768])\n",
      "transformer.h.8.ln_1.bias | torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.8.ln_2.weight | torch.Size([768])\n",
      "transformer.h.8.ln_2.bias | torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.9.ln_1.weight | torch.Size([768])\n",
      "transformer.h.9.ln_1.bias | torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.9.ln_2.weight | torch.Size([768])\n",
      "transformer.h.9.ln_2.bias | torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.10.ln_1.weight | torch.Size([768])\n",
      "transformer.h.10.ln_1.bias | torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.10.ln_2.weight | torch.Size([768])\n",
      "transformer.h.10.ln_2.bias | torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.h.11.ln_1.weight | torch.Size([768])\n",
      "transformer.h.11.ln_1.bias | torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight | torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias | torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight | torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias | torch.Size([768])\n",
      "transformer.h.11.ln_2.weight | torch.Size([768])\n",
      "transformer.h.11.ln_2.bias | torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight | torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias | torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight | torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias | torch.Size([768])\n",
      "transformer.ln_f.weight | torch.Size([768])\n",
      "transformer.ln_f.bias | torch.Size([768])\n",
      "lm_head.weight | torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "for i,j in sd_hf.items():\n",
    "    print(f\"{i} | {j.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74767a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sd_hf['transformer.wte.weight'].shape)\n",
    "print(sd_hf['lm_head.weight'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173e750",
   "metadata": {},
   "source": [
    "Lets verify if they match in the openai pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873d3c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(sd_hf['transformer.wte.weight'], sd_hf['lm_head.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bf470",
   "metadata": {},
   "source": [
    "See! So lets go back in sec1.py and enforce this condition. This is called as weight tying, which saves us 50257*768 ~ 38M parameters. [Here is a long post](https://www.reddit.com/r/MachineLearning/comments/1eqm0lr/r_why_and_when_tying_embedding_a_story/) on _why it works_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776adce",
   "metadata": {},
   "source": [
    "### effect of initialization on residual streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d5f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without init: 10.133536338806152 \n",
      "with init factor 1/sqrt(in_features): 1.013353705406189\n"
     ]
    }
   ],
   "source": [
    "# standard deviation grows inside the residual stream\n",
    "x = torch.zeros(768)\n",
    "y = torch.zeros(768)\n",
    "\n",
    "n = 100 # e.g. 100 layers\n",
    "for i in range(n):\n",
    "    num = torch.randn(768)\n",
    "    x += n**-0.5 * num\n",
    "    y+= num\n",
    "\n",
    "print(f'without init: {y.std()} \\nwith init factor 1/sqrt(in_features): {x.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b0a0d",
   "metadata": {},
   "source": [
    "__What happened__: despite adding `num` sampled from normal distribution, the std deviation grows to $\\sqrt(in\\_features)$; so we divide by this factor each time after sampling before adding it to variable `x`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
