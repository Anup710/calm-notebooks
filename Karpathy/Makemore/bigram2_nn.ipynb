{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f855539",
   "metadata": {},
   "source": [
    "# Bigram using NN\n",
    "\n",
    "As seen previously, we defined the negative log likelihood (NLL) loss function; so now using backprop lets train a NN to make a better bigram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61ccee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad616df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01f8865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stoi \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc6c1de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training dataset of bigrams\n",
    "# input x (first char) -> output y (second char): bigram is ready\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ya = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed1c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), [5, 13, 13, 1, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from char1 to char2\n",
    "xs,ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d15dd",
   "metadata": {},
   "source": [
    "### Intuition for training\n",
    "\n",
    "The prized question now is: how do we feed in this data into the NN?\n",
    "\n",
    "Feeding in integers 5,13,8 etc which are just indices of characters and calculating losses is incorrect, since a = 1 and e = 5 dont have a 'five times relation'. Its a bit like a categorical variable we are dealing with, under integral labels. \n",
    "\n",
    "__So we will one hot encode the input and output vectors and compute the softmax loss!__\n",
    "\n",
    "Check data structures nb for [one hot](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) experimentation in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79d0bc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26710fb8b90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# specify num_classes since by looking at a part of the dataset one_hot may guess no of classes incorrectly. \n",
    "xenc = F.one_hot(xs, num_classes=27).float() # default dtype is int32, but while training float is preferred. \n",
    "\n",
    "plt.imshow(xenc) # to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecf524",
   "metadata": {},
   "source": [
    "### Single neuron construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93a624ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2740],\n",
       "        [ 0.3014],\n",
       "        [-0.5839],\n",
       "        [-0.5839],\n",
       "        [ 1.0128]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,1)) # single neuron\n",
    "xenc @ W # (5,27) * (27,1) = (5,1)\n",
    "\n",
    "# basically the scalar output of the neuron (wo bias) for 5 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "506ce314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1149,  0.1075,  0.9584,  1.1891,  1.6155,  0.8580,  0.4314,  0.4452,\n",
       "          0.8832, -1.6896,  0.5869, -0.0805,  0.3022,  0.3824,  0.9041, -0.7012,\n",
       "         -1.0831, -1.2393, -1.3438,  1.3184,  0.0106,  0.5186, -0.4701,  0.2824,\n",
       "          0.4838, -0.4408,  1.6015],\n",
       "        [-1.1298, -0.2569, -1.5779, -0.8528,  0.7822, -0.5080,  0.0917,  1.0650,\n",
       "          0.6506, -1.4826, -0.8991, -0.7431, -0.9734,  1.0236, -0.3331,  0.8194,\n",
       "          1.4100, -1.2551, -0.8014, -0.2671,  0.1772,  2.2444, -0.7689,  0.4353,\n",
       "         -0.6708, -0.7759,  1.2784],\n",
       "        [-0.8892,  0.5963,  1.1437, -0.5157,  1.7922,  0.1629, -0.6869, -0.9062,\n",
       "          2.4856, -0.5689, -1.1411,  1.1324,  0.2268, -0.2285, -0.1037,  0.1417,\n",
       "         -0.0987, -0.3472,  0.2856,  1.0905,  0.4423, -0.5826,  1.3623, -0.3924,\n",
       "          0.6072,  0.5803,  1.3126],\n",
       "        [-0.8892,  0.5963,  1.1437, -0.5157,  1.7922,  0.1629, -0.6869, -0.9062,\n",
       "          2.4856, -0.5689, -1.1411,  1.1324,  0.2268, -0.2285, -0.1037,  0.1417,\n",
       "         -0.0987, -0.3472,  0.2856,  1.0905,  0.4423, -0.5826,  1.3623, -0.3924,\n",
       "          0.6072,  0.5803,  1.3126],\n",
       "        [ 0.3352, -1.2940,  0.3810,  0.1966, -1.0646, -2.3772,  1.5759, -0.4449,\n",
       "         -0.9685,  2.0474, -0.5115,  1.6011,  0.0963, -0.6618, -1.3880,  0.1184,\n",
       "          0.0363,  0.5595, -0.2721, -0.5265,  0.0674, -0.0420, -0.5797,  0.1401,\n",
       "          1.3329,  0.8958,  0.6180]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,27)) # 27 neurons\n",
    "xenc @ W # matrix multiplication (5,27) * (27,27) = (5,27)\n",
    "\n",
    "# (27,1) output of the neuron (wo bias) for each of 5 samples makes it (5,27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76237a6",
   "metadata": {},
   "source": [
    "Interpretation of `W = torch.randn((27,27))` or `W = torch.randn((27,20))` to avoid confusion:\n",
    "\n",
    "- dim 0 = 27 = must match # of input features (depends on data) \n",
    "- dim 1 = 1 or 20 or 27 -- no of neurons (choice of user)\n",
    "\n",
    "What does the value `(xenc @ W)[3,13]` indicate?\n",
    "- The response of the 13th neuron on looking at the 3rd input!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1f53c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5270)\n",
      "tensor(-0.5270)\n"
     ]
    }
   ],
   "source": [
    "print((xenc @ W)[3,13])\n",
    "\n",
    "print(xenc[3] @ W[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2961f",
   "metadata": {},
   "source": [
    "## Futher intuition \n",
    "\n",
    "1. We will restrict the NN to just a single layer of 27 neurons with no bias, for this tutorial. \n",
    "\n",
    "2. We will use the Softmax activation to interpret outputs of the NN. Remember we saw the importance of activation in MLP -- without which a NN is reduced to Lin Regression -- in CS229. \n",
    "\n",
    "3. The output above of `(xenc @ W)` is not a prob distribution (sum $\\neq$ 1, not between 0 and 1 either)\n",
    "\n",
    "4. So how do we connect `(xenc @ W)` to a prob distribution to sample next character for our bigram model?\n",
    "\n",
    "### Le voil√†: introduce softmax\n",
    "\n",
    "__Process:__ Exponentiate the entries of `(xenc @ W)` $\\rightarrow$ sum them up row-wise $\\rightarrow$ divide them to create a prob map equivalent to `P` or `N2` in the simple bigram1 notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a4ed2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0080, 0.0191, 0.0051, 0.0105, 0.0540, 0.0149, 0.0271, 0.0717, 0.0473,\n",
       "         0.0056, 0.0101, 0.0117, 0.0093, 0.0688, 0.0177, 0.0561, 0.1012, 0.0070,\n",
       "         0.0111, 0.0189, 0.0295, 0.2331, 0.0115, 0.0382, 0.0126, 0.0114, 0.0887]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W \n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "# check any row and it sum\n",
    "probs[1], probs[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cf197",
   "metadata": {},
   "source": [
    "Alternatively, the [nn.Softmax()](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html) class can be directly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b11eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0080, 0.0191, 0.0051, 0.0105, 0.0540, 0.0149, 0.0271, 0.0717, 0.0473,\n",
       "        0.0056, 0.0101, 0.0117, 0.0093, 0.0688, 0.0177, 0.0561, 0.1012, 0.0070,\n",
       "        0.0111, 0.0189, 0.0295, 0.2331, 0.0115, 0.0382, 0.0126, 0.0114, 0.0887])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Softmax(dim = 1)\n",
    "out = m(logits)\n",
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88265ec",
   "metadata": {},
   "source": [
    "So now the task is: __how to find the optimum `W` using backprop__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468697bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
