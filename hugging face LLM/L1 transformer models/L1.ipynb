{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e2ab77",
   "metadata": {},
   "source": [
    "# Transformer models\n",
    "\n",
    "[Follow along of this tutorial.](https://huggingface.co/learn/llm-course/en/chapter1/3)\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "Abstracts away pre-processing, inference and post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766b0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AN80050181\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ?pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2842b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AN80050181\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task = \"sentiment-analysis\")  # there are some default task classes included; for more see documentation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4e21ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9992932081222534}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997404217720032}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9927442073822021}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9569949507713318}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I absolutely love hugging face transformers API\", \"I'm not as good at HF yet\", \"I'm having dinner at BK today\", \"The man sat beneath the apple tree.\"]\n",
    "\n",
    "for s in sentences:\n",
    "    print(classifier(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efc4e1",
   "metadata": {},
   "source": [
    "As is evident, this binary classifier is not very good. Neutral sentences have been classified with strong scores. \n",
    "\n",
    "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    "\n",
    "There are __text, image, audio and multimodal__ pipelines (details available [here](https://huggingface.co/learn/llm-course/en/chapter1/3)). But let us see more inbuilt functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3647f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_zs = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75406225",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_zs(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d7877",
   "metadata": {},
   "source": [
    "For each such task, the HF library caches the model _locally_ and it is unsustainable to keep downloading them, so I'm writing the code for educational purpose but not running it!\n",
    "\n",
    "__Tasks before this used default models__, but you can access _all models on HF_, through the `Models` tab and filter as per the needs of the task - say text generation - and this will [shortlist available models](https://huggingface.co/models?pipeline_tag=text-generation). The api is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37da1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AN80050181\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AN80050181\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM2-360M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model = \"HuggingFaceTB/SmolLM2-360M\")\n",
    "\n",
    "generator(\"In this course we wil explore: \", \n",
    "          max_length = 30, \n",
    "          num_return_sequences = 5\n",
    "          )\n",
    "# deepseek-ai/DeepSeek-V3.2-Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48202e",
   "metadata": {},
   "source": [
    "running the above cell will download the `SmolLM2-360M` model for inference and use it for inference in the text generation task. \n",
    "\n",
    "You can infer from models while rapid prototyping by directly using the model api as such: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730412e7",
   "metadata": {},
   "source": [
    "Any models' online API can be referred directly during inference using the following API_URL:\n",
    "\n",
    "`API_URL = f\"https://api-inference.huggingface.co/models/{model}\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
