{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7ab0a9",
   "metadata": {},
   "source": [
    "# Batch normalization \n",
    "\n",
    "The raw (before activation) outputs of the hidden layer - `hpreact` should ideally be not too small or not too big to avoid push to the extremes by `tanh`. This was the whole motivation for emphasis on correct initialization for network params: `W1, b1, W2, b2` etc\n",
    "\n",
    "But what if: <br>\n",
    "We just normalize `hpreact` directly? instead of trying to get the initialization right? Something akin to $X \\rightarrow \\frac{X-\\mu}{\\sigma}$ -- introduced by [Ioffe and Szegedy from Google brain in their paper on Batch Normalization](https://arxiv.org/pdf/1502.03167). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c128a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05b4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a98d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character mapping \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad1ff0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182437, 3]) torch.Size([182437])\n",
      "torch.Size([22781, 3]) torch.Size([22781])\n",
      "torch.Size([22928, 3]) torch.Size([22928])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939fc2c",
   "metadata": {},
   "source": [
    "The paper for batch normalization can be found [here](https://arxiv.org/pdf/1502.03167). Section $3.1$ is most relevant to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# lookup matrix\n",
    "C = torch.randn((vocab_size,n_embed), generator=g)\n",
    "# hidden layer - 100 neurons\n",
    "W1 = torch.randn((block_size*n_embed,n_hidden), generator=g) * (5/3)/((n_embed*block_size)**0.5)   # use kaiming factor\n",
    "b1 = torch.randn((n_hidden,), generator=g) * 0.01\n",
    "# Output layer\n",
    "W2 = torch.randn((n_hidden,vocab_size), generator=g ) * 0.1\n",
    "b2 = torch.randn((vocab_size,), generator=g) * 0\n",
    "\n",
    "# batch normalization params\n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias = torch.zeros((1,n_hidden))\n",
    "\n",
    "# running mean and std\n",
    "# W1 init using kaiming factor such that mean = 0, std = 1 so running must be init as:\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(sum(pm.nelement() for pm in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1c39bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3042903097250923"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5/3)/((n_embed*block_size)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c5b00",
   "metadata": {},
   "source": [
    "<img src=\"../papers/batch_normalization.png\" style=\"width:70%;\">\n",
    "\n",
    "\n",
    "Steps 1,2,3 are standard steps to mean center and normalize by std dev. Step 4 advocates for introducing a batch_gain and batch_bias as trainable parameters for the NN to learn from data. \n",
    "```\n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias = torch.zeros((1,n_hidden))\n",
    "```\n",
    "being init this way allows for __an exactly normal initialization__, and later as the NN trains, the normal constraint is relaxed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e756097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: loss = 3.6993\n",
      "  10000/ 200000: loss = 1.8060\n",
      "  20000/ 200000: loss = 1.8391\n",
      "  30000/ 200000: loss = 2.4287\n",
      "  40000/ 200000: loss = 2.4655\n",
      "  50000/ 200000: loss = 1.8893\n",
      "  60000/ 200000: loss = 2.5348\n",
      "  70000/ 200000: loss = 2.2075\n",
      "  80000/ 200000: loss = 1.8094\n",
      "  90000/ 200000: loss = 2.3167\n",
      " 100000/ 200000: loss = 2.3735\n",
      " 110000/ 200000: loss = 2.1251\n",
      " 120000/ 200000: loss = 2.1736\n",
      " 130000/ 200000: loss = 1.7423\n",
      " 140000/ 200000: loss = 2.2378\n",
      " 150000/ 200000: loss = 2.3517\n",
      " 160000/ 200000: loss = 1.9762\n",
      " 170000/ 200000: loss = 2.1643\n",
      " 180000/ 200000: loss = 2.5964\n",
      " 190000/ 200000: loss = 1.8833\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # construct minibatch \n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "\n",
    "    mu = hpreact.mean(dim=0, keepdim=True)\n",
    "    std = hpreact.std(dim=0, keepdim=True)\n",
    "\n",
    "    hpreact = bngain * ((hpreact-mu)/std) + bnbias\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(input = logits, target=Yb)\n",
    "\n",
    "    #backpass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data -= lr*p.grad\n",
    "    \n",
    "    # track loss\n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: loss = {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bab2e9",
   "metadata": {},
   "source": [
    "Despite introducing BN and kaiming init factor, the loss on validation set has not reduced dramatically because of the simplicity of our network. It would seem that we have already exploited our basic architecture to the maximum. \n",
    "\n",
    "How would BN work in case of multiple hidden layers? \n",
    "\n",
    "- It is common to sprinkle BN after every linear (Y @ W1 + b1 type) layer \n",
    "\n",
    "__Interesting fact:__<br>\n",
    "Before BN, mini batch allowed parallelization to enhance efficiency but still each data point progressed _independently_ (hpreact -> logits -> loss). But now, when we do BN, each datapoint in a become linked to all the other points in _that_ batch!<br>\n",
    "This effect is may __sometimes be good__ as it introduces a regularization effect and prevents overfitting to any single datapoint.\n",
    "\n",
    "However, this random coupling of examples in a batch is hard to predict mathematically and can lead to bizzare results. Instead: \n",
    "- Instance normalization\n",
    "- Layer normalization \n",
    "- group normalization , may be preferred\n",
    "\n",
    "\n",
    "### Fixing mean and std on training data for test time\n",
    "\n",
    "By current design, the NN expects input batches (to calculate $\\mu$ and $\\sigma$) when in irl there may be individual examples and their forward passes. __This is similar to:__ finding training statistics and using them as it is on dev and test set for `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e481ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit way\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass through training set to get statistics\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f60b31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0905\n",
      "2.1481\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] \n",
    "  embcat = emb.view(emb.shape[0], -1) \n",
    "  \n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact-bnmean)/bnstd + bnbias   # normalize into training statistics \n",
    "  \n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2+ b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  # print(split, loss.item())\n",
    "  return loss\n",
    "\n",
    "print(f\"{split_loss('train'):.4f}\")\n",
    "print(f\"{split_loss('val'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414d6b",
   "metadata": {},
   "source": [
    "This step of explicit calculation can be avoided by iteratively arriving at the batch mean _while_ it is training by using something as such in the body of the loop: \n",
    "\n",
    "```with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "```\n",
    "where `bnmeani` and `bnstdi` are stats for that batch. \n",
    "\n",
    "We can verify that at the end of training, `bnmean_running` ~ `bnmean` and `bnstd_running` ~ `bnstd`!\n",
    "\n",
    "### Rationale:\n",
    "\n",
    "The above formula for `bnmean_running` and `bnstd_running` comes from Exponential moving averages (EMA). <br>\n",
    "running  = $(1-\\alpha)\\times$ running + $\\alpha \\times$ new_value | at $\\alpha = 0.001$\n",
    "\n",
    "\"Update the current estimate with 0.1% of the new value, and retain 99.9% of the current running estimate.\"\n",
    "\n",
    "Mathematically unrolling the recursion gives: \n",
    "\n",
    "$running_t = \\alpha x_t + \\alpha(1-\\alpha)x_{t-1} + \\alpha(1-\\alpha)^2x_{t-2} + ...$\n",
    "\n",
    "In practice:\n",
    "\n",
    "PyTorch's default for momentum in BatchNorm is 0.1, meaning $\\alpha = 0.1$. Your version with __0.001 is more conservative and stable__, but slower to adapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376adc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO - CODE\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  #....\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True) # i-th batch mean\n",
    "  bnstdi = hpreact.std(0, keepdim=True) # i-th batch std\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "#......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cd50c",
   "metadata": {},
   "source": [
    "## Redundancy of bias b1\n",
    "\n",
    "Consider these lines of code: \n",
    "```\n",
    "hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True) # i-th batch mean\n",
    "  bnstdi = hpreact.std(0, keepdim=True) # i-th batch std\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "```\n",
    "bias `b1` is added to compute hpreact but subtracted in form of `bnmeani`, so might as well remove b1 altogether! This is applicable for all linear layers before a BN step. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
