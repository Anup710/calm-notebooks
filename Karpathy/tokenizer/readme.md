Sub-word tokenizers are most commonly used, using algorithms such as __Byte pair encoding__. 

__Tokenization is essentially the process of converting text into sequences of integers and vice versa.__

<hr>

Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.

- Why can't LLM spell words? __Tokenization__.<br>
- Why can't LLM do super simple string processing tasks like reversing a string? __Tokenization__.<br>
- Why is LLM worse at non-English languages (e.g. Japanese)? __Tokenization__.<br>
- Why is LLM bad at simple arithmetic? __Tokenization__.<br>
- Why did GPT-2 have more than necessary trouble coding in Python? __Tokenization__.<br>
- Why did my LLM abruptly halt when it sees the string "<|endoftext|>"? __Tokenization__.<br>
- What is this weird warning I get about a "trailing whitespace"? __Tokenization__.<br>
- Why the LLM break if I ask it about "[SolidGoldMagikarp](https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/-the-enigma-of-solidgoldmagikarp-ais-strangest-token)"? __Tokenization__.<br>
- Why should I prefer to use YAML over JSON with LLMs? __Tokenization__.<br>
- Why is LLM not actually end-to-end language modeling? __Tokenization__.<br>
- What is the real root of suffering? __Tokenization__.<br>