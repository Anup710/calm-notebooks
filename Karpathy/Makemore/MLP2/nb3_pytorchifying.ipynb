{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d2aad6",
   "metadata": {},
   "source": [
    "# Pytorchifying Batch normalization \n",
    "\n",
    "In this nb we will use ready made classes from pytorch rather than writing custon NN layers with BN and activation. This is how it will be deployed in production. Lets get through some prerquisites though. \n",
    "\n",
    "1. [nn.linear](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- To initialize a linear layer (Wx + b) with in_features, out_features, bais as parameters. In case applying BN, bias = Flase can be set. \n",
    "- Also see _how_ the weights and biases are initialized when nn.Linear() is called:<br>\n",
    "Values are sampled from $U(-\\sqrt{k},\\sqrt{k})$, where $k = \\frac{1}{in\\_features}$ (uniformly) - note that this is similar to `kaiming init` without the gain factor $\\frac{5}{3}$ for tanh! \n",
    "\n",
    "\n",
    "2. [nn.BatchNorm1d](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d1)\n",
    "- Mathematically: $ y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\varepsilon}} \\cdot \\gamma + \\beta $, where $\\gamma = $ bngain, $\\beta = $ bnbias\n",
    "\n",
    "Lets break down its arguments: <br>\n",
    "- $\\epsilon$ prevents blowing up around $0$\n",
    "- `momentum` is the update rate for the `bnmeani` to update the running_mean ($\\alpha$ in the previous files)\n",
    "- `affine` must be true to ensure _bngain_ and _bnbias_ are learnable \n",
    "- `track_running_stats` allows computing overall mean, std etc while training itself. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa03322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4193af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760d8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "allchars = sorted(set(''.join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allchars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90fe02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb65fc",
   "metadata": {},
   "source": [
    "Lets make our network deeper and layers more generalizable unlike explicit definition for each layer. \n",
    "\n",
    " The classes we create here are the same API as nn.Module in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c527d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(200989800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, bias = True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) * 1/fan_in**0.5\n",
    "        self.bais = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = [self.weight] + ([] if self.bais is None else [self.bias])\n",
    "        return params\n",
    "    \n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(dim = 0, keepdim = True)\n",
    "            xvar = x.var(dim = 0, keepdim = True)\n",
    "\n",
    "        else: \n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        # apply to data\n",
    "        xhat = (x-xmean)/torch.sqrt(xvar + self.eps)\n",
    "\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad:\n",
    "                self.running_mean = self.momentum * xmean + (1 - self.momentum) * self.running_mean\n",
    "                self.running_var = self.momentum * xvar + (1 - self.momentum) * self.running_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
