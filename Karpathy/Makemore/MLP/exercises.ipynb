{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9295c829",
   "metadata": {},
   "source": [
    "## Further questions\n",
    "\n",
    "__Exercises:__ <br>\n",
    "E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "\n",
    "E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "\n",
    "E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\n",
    "\n",
    "I will also improve the readability of the code so that running these different settings becomes easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd413645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33920b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stoi \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87deb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words1):\n",
    "    X1 , Y1 = [], []\n",
    "    block_size= 3 # can be reset to whatever you like\n",
    "\n",
    "    for w in words1:\n",
    "        context = [0]*block_size # contains indcies of context letters\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            Y1.append(ix) \n",
    "            X1.append(context)\n",
    "            context = context[1:] + [ix] # update context and append new index\n",
    "\n",
    "    X1 = torch.tensor(X1)\n",
    "    Y1 = torch.tensor(Y1)\n",
    "    print(X1.shape, Y1.shape)\n",
    "    return X1,Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5d23b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182574, 3]) torch.Size([182574])\n",
      "torch.Size([22816, 3]) torch.Size([22816])\n",
      "torch.Size([22756, 3]) torch.Size([22756])\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random.seed(378987987)\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xt, Yt = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99230aee",
   "metadata": {},
   "source": [
    "fixing the context window at 3 for now and calculating other hyperparams around it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be5566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network \n",
    "\n",
    "def init_network(g, hidden_neurons:int, embed_size:int):\n",
    "    C = torch.randn((27,embed_size), generator=g)\n",
    "    # hidden layer - 100 neurons\n",
    "    W1 = torch.randn((3*embed_size,hidden_neurons), generator=g)\n",
    "    b1 = torch.randn((hidden_neurons,), generator=g)\n",
    "    # Output layer\n",
    "    W2 = torch.randn((hidden_neurons,27), generator=g )\n",
    "    b2 = torch.randn((27,), generator=g)\n",
    "\n",
    "    return (C, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fd539f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the function more precise, can include W,b as arguments as well\n",
    "def loss_on_set(X, Y, C_emb, W1, b1, W2, b2):\n",
    "    emb_fullset = C_emb[X] # 228146,3,2\n",
    "    H = torch.tanh(emb_fullset.view(-1, C_emb.shape[1]*3) @ W1 + b1) # 228146,6 @ 6,100 => 228146, 100\n",
    "    logits_fullset = H @ W2 + b2\n",
    "    loss_on_set = F.cross_entropy(logits_fullset, target=Y)\n",
    "    return loss_on_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23c4f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters: \n",
    "g = torch.Generator().manual_seed(378987987)\n",
    "\n",
    "parameters = init_network(g, 200, 5)\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "C, W1, b1, W2, b2 = parameters # global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc785bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 5])\n",
      "torch.Size([15, 200])\n",
      "torch.Size([200])\n",
      "torch.Size([200, 27])\n",
      "torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db0ff3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b0181f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process: \n",
    "# function - arguments : iters, batch size, learning rate\n",
    "# returns loss \n",
    "\n",
    "def train (iters, batch_size, alpha):\n",
    "    \"\"\"\"\n",
    "    arguments : iters, batch size, learning rate\n",
    "    returns loss\n",
    "\n",
    "    Call this function everytime you wish to train your neural net!\n",
    "    \"\"\"\n",
    "    for iter in range(iters): \n",
    "        # 1000 mini batches of size 32 each \n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,)) # assuming each batch has 32 data points \n",
    "        # Forward pass: \n",
    "        emb = C[Xtr[ix]]\n",
    "        H = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1) # H dimension = (batch_size,neurons)\n",
    "        logits = H @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, target=Ytr[ix])\n",
    "        # Back pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        # update\n",
    "        lr = alpha\n",
    "        with torch.no_grad():\n",
    "            for p in parameters:\n",
    "                p.data -= lr * p.grad \n",
    "\n",
    "        # track stats\n",
    "        step.append(iter)\n",
    "        lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7178c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(iters = 10000,batch_size = 64, alpha = 0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08694b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.524190902709961, 2.036754608154297, 2.6068787574768066, 2.5831031799316406, 2.719836950302124, 2.141599178314209, 2.457942485809326, 2.1791064739227295, 2.468146324157715, 2.4783520698547363]\n"
     ]
    }
   ],
   "source": [
    "print(lossi[-10:]) # loss on batch logged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd0e418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4377, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4476, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_on_set(Xtr, Ytr, C, W1, b1, W2, b2))\n",
    "print(loss_on_set(Xdev, Ydev, C, W1, b1, W2, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18e127",
   "metadata": {},
   "source": [
    "OK now the code look much more generalizable and tractable! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
