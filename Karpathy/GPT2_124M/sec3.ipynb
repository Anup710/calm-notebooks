{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96b4dc4",
   "metadata": {},
   "source": [
    "## Section 3: algorithmic improvements to GPT2\n",
    "\n",
    "The paper for gpt2 or code (inference code) doesn't talk too much about the algorithmic details or hyperparameters being used. So we refer to elements of gpt3 paper, since their architecture is very similar\n",
    "\n",
    "Gpt2: less details, open weights <br>\n",
    "Gpt3: more details, no weights\n",
    "\n",
    "Key differences:<br>\n",
    "- context length: 1024 vs 2048\n",
    "- gpt3: trained for lot longer, on bigger dataset, more validation\n",
    "- 1.6b vs 175b parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2c8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import tiktoken\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sec2 import GPT, GPTConfig, DataLoaderLite ,get_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2260806",
   "metadata": {},
   "source": [
    "Refer [gpt3 paper](https://arxiv.org/pdf/2005.14165): __Section B - Details of model training__\n",
    "\n",
    "<img src=\"images/details of model training.png\" style=\"width:50%;\">`\n",
    "\n",
    "In actual gpt3: ~__300 billion__ training tokens used, we use much lesser.\n",
    "\n",
    "1. AdamW parameters: $\\beta = $[0.99, 0.95], $\\epsilon = 10^{-8}$\n",
    "\n",
    "2. Gradient clipping at $\\text{max norm} = 1.0 $\n",
    "    - how it works:  scales all grads by $\\frac{\\text{max norm}}{\\text{norm}}$ if norm > 1\n",
    "    - why: to prevent _shocks_ in model training, in case when batch sampling is too unusual\n",
    "    - useful to track it during training (is grad norm $\\uparrow \\text{or} \\downarrow$ abnormally etc)\n",
    "\n",
    "3. Cosine decay learning rate with warmup\n",
    "    - handwritten _here_, plug and play function also available in pytorch\n",
    "    - Other schedules can be used (active research area) \n",
    "\n",
    "4. Changing batch size (not implemented here)\n",
    "    - Starts with 32k tokens per batch and increase linearly up to 4-12 billions tokens \n",
    "    - Gain is incremental anyway and complicates the arithmatic, so skipped _here_\n",
    "\n",
    "5. weight decay, Fused AdamW \n",
    "    - weight decay for 2D tensors (matmuls and embeddings) and not for biases or layernorms (1D)\n",
    "    - [Intuition behind weight decay](https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/): regularization, prevent individual weight becoming too big\n",
    "    - [Fused adamw](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html) introduces fused kernels in later versions of pytorch which reduces some overheads when `device = 'cuda'` \n",
    "\n",
    "### Gradient accumulation\n",
    "\n",
    "<img src=\"images/hyperparams.png\" style=\"width:60%;\">`\n",
    "\n",
    "We will adhere to 0.5M tokens/batch but that may not fit on a GPU with limited VRAM. Instead we retain the (16,1024) batch size and sample $~\\frac{524288}{16*1024} = 32$ batches per epoch. $524288 = 2^{19}$ is the closest _nice number_ to 500000. \n",
    "\n",
    "In practice: <br>\n",
    "- since loss.backward() is additive, for mini_step in range $32$:\n",
    "    - sample x,y\n",
    "    - model(x)\n",
    "    - loss and loss.backward()\n",
    "- optimizer.step()\n",
    "\n",
    "This introduces a small bug, since loss is of `reduction = \"mean\"` we must divide by 32 to get the correct loss over 32 mini_steps. Look at the below cell for intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d608916",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # from gpt3 small \n",
    "min_lr = max_lr * 0.1\n",
    "max_steps = 50\n",
    "warmup_steps = 10\n",
    "\n",
    "def get_lr(it):\n",
    "    \n",
    "    # 1. for warmup phase scale\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1)/max_steps\n",
    "    \n",
    "    #2. for it > max_steps\n",
    "    if it> max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    # for in between: cosine decay with linear increment\n",
    "    decay_ratio = (it - warmup_steps)/ (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# revent accidental run and kernal clogging. \n",
    "import sys; sys.exit(0)\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)) #- random weights init\n",
    "model.to(device)\n",
    "model = torch.compile(model) # compiles the model \n",
    "\n",
    "B,T = 16,1024\n",
    "\n",
    "total_batch_size  = 2**19 #524288, ~0.5M, in number of tokens\n",
    "assert total_batch_size % (B*T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
    "grad_accum_steps = total_batch_size // (B*T)\n",
    "\n",
    "print(f\"Desired total batch size = {total_batch_size} tokens\")\n",
    "print(f\"calculated gradient accumulation steps = {grad_accum_steps}\")\n",
    "\n",
    "\n",
    "train_loader = DataLoaderLite(B,T)\n",
    "# torch.set_float32_matmul_precision('high') -- old api, soon to be deprecated \n",
    "torch.backends.fp32_precision = \"tf32\" # new api, use \"ieee\" to enforce global fp32 precision \n",
    "\n",
    "\n",
    "# optimizer with weight decay + fused kernels. \n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device = device)\n",
    "\n",
    "#training loop\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x,y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # single line autocast\n",
    "        with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
    "            logits, loss = model(x,y)\n",
    "\n",
    "        # scale to account for gradient accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach() #accumulate loss at mini_step into loss at each step\n",
    "        loss.backward()\n",
    "\n",
    "    #grad clipping\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    #determine and set learning rate for this iteration \n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize() # allow gpu bandwidth to catch up and clear the queue of operations\n",
    "    t1 = time.time()\n",
    "    dt = (t1-t0)*1000 # time difference in milliseconds\n",
    "    \n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps # = total_batch_tokens\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    print(f\"step {step:4d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "\n",
    "\n",
    "#verify total no of parameters\n",
    "total_params_M = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Total parameters: {total_params_M:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d45312",
   "metadata": {},
   "source": [
    "### Gradient accumulation motivation through a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582f12c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0953,  0.0498, -0.0077, -0.0817, -0.0166,  0.0079,  0.0189,  0.1085,\n",
      "         0.1615, -0.0739])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "\n",
    "# super simple little MLP\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1)\n",
    ")\n",
    "torch.random.manual_seed(42)\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "# the loss objective here is (due to readuction='mean')\n",
    "# L = 1/4 * [\n",
    "#            (y[0] - yhat[0])**2 +\n",
    "#            (y[1] - yhat[1])**2 +\n",
    "#            (y[2] - yhat[2])**2 +\n",
    "#            (y[3] - yhat[3])**2\n",
    "#           ]\n",
    "# NOTE: 1/4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe15ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0953,  0.0498, -0.0077, -0.0817, -0.0166,  0.0079,  0.0189,  0.1085,\n",
      "         0.1615, -0.0739])\n"
     ]
    }
   ],
   "source": [
    "# now let's do it with grad_accum_steps of 4, and B=1\n",
    "# the loss objective here is different because\n",
    "# accumulation in gradient <---> SUM in loss\n",
    "# i.e. we instead get:\n",
    "# L0 = 1/4(y[0] - yhat[0])**2\n",
    "# L1 = 1/4(y[1] - yhat[1])**2\n",
    "# L2 = 1/4(y[2] - yhat[2])**2\n",
    "# L3 = 1/4(y[3] - yhat[3])**2\n",
    "# L = L0 + L1 + L2 + L3\n",
    "# NOTE: the \"normalizer\" of 1/4 is lost\n",
    "net.zero_grad()\n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss = loss / 4 # <-- have to add back the \"normalizer\"!\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e505",
   "metadata": {},
   "source": [
    "### Sampling and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "out = model.generate((\"Hello, I'm a language model,\"), num_return_sequences=5, max_length=30) # print statement included within generate\n",
    "with open('output.txt', 'w') as f:\n",
    "    for o in out:\n",
    "        f.write(o + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d3e9",
   "metadata": {},
   "source": [
    "## Distributed data parallel \n",
    "\n",
    "Bringing out the heavy weapons :)  - Using multiple GPUs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
