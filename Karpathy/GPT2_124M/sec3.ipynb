{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96b4dc4",
   "metadata": {},
   "source": [
    "## Section 3: algorithmic improvements to GPT2\n",
    "\n",
    "The paper for gpt2 or code (inference code) doesn't talk too much about the algorithmic details or hyperparameters being used. So we refer to elements of gpt3 paper, since their architecture is very similar\n",
    "\n",
    "Gpt2: less details, open weights <br>\n",
    "Gpt3: more details, no weights\n",
    "\n",
    "Key differences:<br>\n",
    "- context length: 1024 vs 2048\n",
    "- gpt3: trained for lot longer, on bigger dataset, more validation\n",
    "- 1.6b vs 175b parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2c8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import tiktoken\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sec2 import GPT, GPTConfig, DataLoaderLite ,get_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2260806",
   "metadata": {},
   "source": [
    "Refer [gpt3 paper](https://arxiv.org/pdf/2005.14165): __Section B - Details of model training__\n",
    "\n",
    "In actual gpt3: ~__300 billion__ training tokens used, we use much lesser.\n",
    "\n",
    "1. AdamW parameters: $\\beta = $[0.99, 0.95], $\\epsilon = 10^{-8}$\n",
    "\n",
    "2. Gradient clipping at $\\text{max norm} = 1.0 $\n",
    "    - how it works:  scales all grads by $\\frac{\\text{max norm}}{\\text{norm}}$ if norm > 1\n",
    "    - why: to prevent _shocks_ in model training, in case when batch sampling is too unusual\n",
    "    - useful to track it during training (is grad norm $\\uparrow \\text{or} \\downarrow$ abnormally etc)\n",
    "\n",
    "3. Cosine decay learning rate with warmup\n",
    "    - handwritten _here_, plug and play function also available in pytorch\n",
    "    - Other schedules can be used (active research area) \n",
    "\n",
    "4. Changing batch size (not implemented here)\n",
    "    - Starts with 32k tokens per batch and increase linearly up to 4-12 billions tokens \n",
    "    - Gain is incremental anyway and complicates the arithmatic, so skipped _here_\n",
    "\n",
    "5. weight decay, Fused AdamW \n",
    "    - weight decay for 2D tensors (matmuls and embeddings) and not for biases or layernorms (1D)\n",
    "    - [Intuition behind weight decay](https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/): regularization, prevent individual weight becoming too big\n",
    "    - [Fused adamw](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html) introduces fused kernels in later versions of pytorch which reduces some overheads when `device = 'cuda'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d608916",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # from gpt3 small \n",
    "min_lr = max_lr * 0.1\n",
    "max_steps = 50\n",
    "warmup_steps = 10\n",
    "\n",
    "def get_lr(it):\n",
    "    \n",
    "    # 1. for warmup phase scale\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1)/max_steps\n",
    "    \n",
    "    #2. for it > max_steps\n",
    "    if it> max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    # for in between: cosine decay with linear increment\n",
    "    decay_ratio = (it - warmup_steps)/ (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88b76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Total tokens in dataset = 338025\n",
      "Max batches in 1 epoch = 2640\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0: loss = 10.84716796875 | lr = 0.00 | norm = 45.0423 | dt: 350.33 ms | 365.36 tokens/sec\n",
      "step 1: loss = 10.10595703125 | lr = 0.00 | norm = 26.0900 | dt: 175.30 ms | 730.17 tokens/sec\n",
      "step 2: loss = 9.3475341796875 | lr = 0.00 | norm = 18.2147 | dt: 175.70 ms | 728.53 tokens/sec\n",
      "step 3: loss = 9.45416259765625 | lr = 0.00 | norm = 15.0661 | dt: 174.02 ms | 735.54 tokens/sec\n",
      "step 4: loss = 9.084228515625 | lr = 0.00 | norm = 13.4934 | dt: 173.96 ms | 735.82 tokens/sec\n",
      "step 5: loss = 8.8302001953125 | lr = 0.00 | norm = 8.8941 | dt: 173.82 ms | 736.39 tokens/sec\n",
      "step 6: loss = 9.38555908203125 | lr = 0.00 | norm = 7.8695 | dt: 173.99 ms | 735.68 tokens/sec\n",
      "step 7: loss = 9.274658203125 | lr = 0.00 | norm = 5.6511 | dt: 174.72 ms | 732.58 tokens/sec\n",
      "step 8: loss = 8.9130859375 | lr = 0.00 | norm = 6.7021 | dt: 173.65 ms | 737.12 tokens/sec\n",
      "step 9: loss = 8.5035400390625 | lr = 0.00 | norm = 5.3031 | dt: 174.71 ms | 732.64 tokens/sec\n",
      "step 10: loss = 8.8985595703125 | lr = 0.00 | norm = 4.1374 | dt: 173.91 ms | 736.00 tokens/sec\n",
      "step 11: loss = 8.161376953125 | lr = 0.00 | norm = 4.8787 | dt: 174.64 ms | 732.95 tokens/sec\n",
      "step 12: loss = 8.33087158203125 | lr = 0.00 | norm = 5.8139 | dt: 174.47 ms | 733.67 tokens/sec\n",
      "step 13: loss = 7.76348876953125 | lr = 0.00 | norm = 2.8890 | dt: 173.35 ms | 738.38 tokens/sec\n",
      "step 14: loss = 7.846923828125 | lr = 0.00 | norm = 4.7400 | dt: 174.35 ms | 734.16 tokens/sec\n",
      "step 15: loss = 7.482421875 | lr = 0.00 | norm = 3.3395 | dt: 174.67 ms | 732.79 tokens/sec\n",
      "step 16: loss = 7.412109375 | lr = 0.00 | norm = 3.1608 | dt: 173.93 ms | 735.95 tokens/sec\n",
      "step 17: loss = 8.22747802734375 | lr = 0.00 | norm = 4.5536 | dt: 173.70 ms | 736.88 tokens/sec\n",
      "step 18: loss = 7.0926513671875 | lr = 0.00 | norm = 4.3620 | dt: 173.23 ms | 738.92 tokens/sec\n",
      "step 19: loss = 7.83074951171875 | lr = 0.00 | norm = 3.4640 | dt: 176.88 ms | 723.64 tokens/sec\n",
      "step 20: loss = 7.42498779296875 | lr = 0.00 | norm = 3.4835 | dt: 174.25 ms | 734.56 tokens/sec\n",
      "step 21: loss = 7.69964599609375 | lr = 0.00 | norm = 3.4575 | dt: 174.93 ms | 731.72 tokens/sec\n",
      "step 22: loss = 6.41156005859375 | lr = 0.00 | norm = 4.8377 | dt: 174.92 ms | 731.77 tokens/sec\n",
      "step 23: loss = 6.77532958984375 | lr = 0.00 | norm = 3.0713 | dt: 174.14 ms | 735.04 tokens/sec\n",
      "step 24: loss = 6.623321533203125 | lr = 0.00 | norm = 3.1162 | dt: 174.37 ms | 734.09 tokens/sec\n",
      "step 25: loss = 6.53753662109375 | lr = 0.00 | norm = 3.5041 | dt: 175.40 ms | 729.78 tokens/sec\n",
      "step 26: loss = 9.814929962158203 | lr = 0.00 | norm = 181.1672 | dt: 174.76 ms | 732.44 tokens/sec\n",
      "step 27: loss = 7.81121826171875 | lr = 0.00 | norm = 4.2816 | dt: 174.47 ms | 733.64 tokens/sec\n",
      "step 28: loss = 7.14959716796875 | lr = 0.00 | norm = 3.5181 | dt: 176.01 ms | 727.23 tokens/sec\n",
      "step 29: loss = 6.95843505859375 | lr = 0.00 | norm = 2.7036 | dt: 174.81 ms | 732.23 tokens/sec\n",
      "step 30: loss = 6.793701171875 | lr = 0.00 | norm = 2.8806 | dt: 174.52 ms | 733.44 tokens/sec\n",
      "step 31: loss = 7.141998291015625 | lr = 0.00 | norm = 3.2813 | dt: 175.13 ms | 730.89 tokens/sec\n",
      "step 32: loss = 7.1549072265625 | lr = 0.00 | norm = 2.6522 | dt: 174.28 ms | 734.43 tokens/sec\n",
      "step 33: loss = 7.021728515625 | lr = 0.00 | norm = 4.2017 | dt: 174.90 ms | 731.83 tokens/sec\n",
      "step 34: loss = 7.9095458984375 | lr = 0.00 | norm = 2.7747 | dt: 175.39 ms | 729.80 tokens/sec\n",
      "step 35: loss = 7.7366943359375 | lr = 0.00 | norm = 2.7937 | dt: 174.41 ms | 733.89 tokens/sec\n",
      "step 36: loss = 7.54437255859375 | lr = 0.00 | norm = 2.7871 | dt: 175.93 ms | 727.58 tokens/sec\n",
      "step 37: loss = 7.68597412109375 | lr = 0.00 | norm = 3.0596 | dt: 175.49 ms | 729.40 tokens/sec\n",
      "step 38: loss = 7.7918701171875 | lr = 0.00 | norm = 3.0797 | dt: 174.35 ms | 734.18 tokens/sec\n",
      "step 39: loss = 7.471405029296875 | lr = 0.00 | norm = 2.8919 | dt: 175.67 ms | 728.63 tokens/sec\n",
      "step 40: loss = 7.50213623046875 | lr = 0.00 | norm = 3.2663 | dt: 175.04 ms | 731.28 tokens/sec\n",
      "step 41: loss = 6.93548583984375 | lr = 0.00 | norm = 3.2878 | dt: 175.01 ms | 731.38 tokens/sec\n",
      "step 42: loss = 7.0478515625 | lr = 0.00 | norm = 2.9695 | dt: 175.56 ms | 729.10 tokens/sec\n",
      "step 43: loss = 7.1949462890625 | lr = 0.00 | norm = 2.8939 | dt: 175.40 ms | 729.74 tokens/sec\n",
      "step 44: loss = 7.149444580078125 | lr = 0.00 | norm = 2.9214 | dt: 175.69 ms | 728.56 tokens/sec\n",
      "step 45: loss = 7.2001495361328125 | lr = 0.00 | norm = 3.1858 | dt: 175.38 ms | 729.86 tokens/sec\n",
      "step 46: loss = 6.127166748046875 | lr = 0.00 | norm = 3.3766 | dt: 176.80 ms | 724.00 tokens/sec\n",
      "step 47: loss = 6.0994873046875 | lr = 0.00 | norm = 3.1241 | dt: 175.93 ms | 727.58 tokens/sec\n",
      "step 48: loss = 6.935394287109375 | lr = 0.00 | norm = 2.7959 | dt: 175.12 ms | 730.91 tokens/sec\n",
      "step 49: loss = 6.74310302734375 | lr = 0.00 | norm = 2.5531 | dt: 176.14 ms | 726.69 tokens/sec\n",
      "Total parameters: 124.48M\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)) #- random weights init\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# model = torch.compile(model) # compiles the model \n",
    "\n",
    "# B,T = 16,1024\n",
    "B,T = 4,32\n",
    "train_loader = DataLoaderLite(B,T)\n",
    "\n",
    "# torch.set_float32_matmul_precision('high') -- old api, soon to be deprecated \n",
    "torch.backends.fp32_precision = \"tf32\" # new api, use \"ieee\" to enforce global fp32 precision \n",
    "\n",
    "\n",
    "# optimizer with weight decay + fused kernels. \n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device = device)\n",
    "\n",
    "#training loop\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    x,y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # single line autocast\n",
    "    with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
    "        logits, loss = model(x,y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #grad clipping\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    #determine and set learning rate for this iteration \n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize() # allow gpu bandwidth to catch up and clear the queue of operations\n",
    "    t1 = time.time()\n",
    "    dt = (t1-t0)*1000 # time difference in milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T )/(t1-t0)\n",
    "    print(f\"step {step}: loss = {loss.item()} | lr = {lr:.2f} | norm = {norm:.4f} | dt: {dt:.2f} ms | {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "#verify total no of parameters\n",
    "total_params_M = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Total parameters: {total_params_M:.2f}M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
