{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06041a83",
   "metadata": {},
   "source": [
    "## Softmax and cross entropy\n",
    "\n",
    "### $S(y_i) = \\frac{e^{y_i}}{\\sum e^{y_i}}$\n",
    "\n",
    "Compare the output probability with one hot encoded true label (y_true) to find the cross entropy loss as such:\n",
    "\n",
    "$D(y, \\hat{y}) = - \\frac{1}{N}\\sum Y_i \\times log(\\hat{Y_i})$\n",
    "\n",
    "where: \n",
    "$\\hat{Y_i}$ = predicted label<br>\n",
    "$Y_i$ = true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c8d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c279ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in numpy\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35675ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2372554 , 0.64492705, 0.11781755])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, 0.3])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bbaee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2373, 0.6449, 0.1178])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax on tensors\n",
    "t = torch.tensor([1.0, 2.0, 0.3])\n",
    "\n",
    "torch.softmax(t, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20da5b4",
   "metadata": {},
   "source": [
    "This softmax coefficient is often used in multiclass classification with cross entropy loss.\n",
    "\n",
    "For example: \n",
    "\n",
    "- $\\hat{Y}$ = [0.7, 0.2, 1] and $Y$ = [1 0 0] $\\implies D(Y, \\hat{Y})$  = 0.35\n",
    "\n",
    "- $\\hat{Y}$ = [0.1, 0.3, 0.6] and $Y$ = [1 0 0] $\\implies D(Y, \\hat{Y})$  =  2.30\n",
    "\n",
    "So: the further _away_ is the prediction, more is the loss; which makes sense for the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e9a4a",
   "metadata": {},
   "source": [
    "### CE loss in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEloss (actual, predicted):\n",
    "    # print(np.log(predicted))\n",
    "    # print(actual * np.log(predicted))\n",
    "    # print(np.sum(actual * np.log(predicted)))\n",
    "    return - np.sum(actual * np.log(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7ea160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.30258509  2.07944154  1.09861229]\n",
      "[-2.30258509 -4.15888308  3.29583687]\n",
      "-3.1656313103493883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.1656313103493883)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array([1 ,-2, 3])\n",
    "p1 = np.array([0.1, 8, 3])\n",
    "\n",
    "CEloss(a1,p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745a311",
   "metadata": {},
   "source": [
    "### CE loss in pytorch\n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">CAUTION:</span>\n",
    "\n",
    "`nn.CrossEntropyLoss()` applies softmax and negative log likelihood _automatically_.\n",
    "\n",
    "$\\implies$ we must not additionally add softmax. \n",
    "\n",
    "Further, Y_true must be the actual label, __not__ on-hot encoded, unlike the numpy case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a50ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with good predcition = 0.17910423874855042 \n",
      "Loss with bad prediction = 1.6631355285644531\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y_true = torch.tensor([1]) #actual label is '1'\n",
    "\n",
    "# raw logit score -> no softmax\n",
    "y_pred_bad = torch.tensor([[2.0, 0.7, 0.2]]) # list of lists -- makes sense\n",
    "y_pred_good = torch.tensor([[1.0, 3.0, 0.2]]) # predicts maximum chance of 1, unlike the bad pred \n",
    "\n",
    "l1 = loss_fn(y_pred_good, y_true)\n",
    "l2 = loss_fn(y_pred_bad, y_true) #expect high than l1\n",
    "\n",
    "print(f'Loss with good predcition = {l1.item()} \\nLoss with bad prediction = {l2.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0d235",
   "metadata": {},
   "source": [
    "And the corresponding output labels for `y_pred_bad` and `y_pred_good` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85deb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label from pred1 = 0 \n",
      "Label from pred2 = 1\n"
     ]
    }
   ],
   "source": [
    "_, pred1 = torch.max(y_pred_bad, 1) # axis = 1\n",
    "_, pred2 = torch.max(y_pred_good, 1) \n",
    "\n",
    "print(f\"label from pred1 = {pred1.item()} \\nLabel from pred2 = {pred2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0204f62",
   "metadata": {},
   "source": [
    "Multilabel classification is also (exactly) similarly possible if y_true is say 3 dim tensor and y_pred is a list of 3 lists!\n",
    "\n",
    "For ex: \n",
    "``Y_pred_bad = torch.tensor(\n",
    "    [[0.9, 0.2, 0.1],\n",
    "    [0.1, 0.3, 1.5],\n",
    "    [1.2, 0.2, 0.5]])``\n",
    "\n",
    "    y_true = torch.tensor([1,0,2])\n",
    "\n",
    "loss, multi label prediction are similarly available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
