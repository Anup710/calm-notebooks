{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb65f632",
   "metadata": {},
   "source": [
    "In this notebook we will iterate on the bigram model and build up to attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385c02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861d9f5",
   "metadata": {},
   "source": [
    "## Mathematical trick for self attention:\n",
    "\n",
    "We now go on a tangent to explore different implementations of this mathematical trick:\n",
    "1. manual calculation\n",
    "2. using tril\n",
    "3. using -inf and softmax\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1963c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch_size, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934a57e",
   "metadata": {},
   "source": [
    "__How do we introduce interaction such that say 5th time character only sees 1:4 chars as context?__\n",
    "\n",
    "One (__poor__) way of capturing this is taking an average along C dimension for chars 1:4 and using that as input to predict char 5 as output. <br>\n",
    "We are losing a lot of information about spatial arrangement of chars 1:4 but for a start its ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f452517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want: x[b,t] = mean {i <=t} x[b,i]\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "\n",
    "# v1 - manual\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] # (t,C)\n",
    "        # print(x_prev, x_prev.shape)\n",
    "        # print(x_prev.mean(dim = 0))\n",
    "        xbow[b,t] = x_prev.mean(dim = 0) # dim 0 is along 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a00a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]]) tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0], xbow[0])\n",
    "# first row matches, every kth subsequent row in xbow is a mean of :k+1 rows of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae06e8",
   "metadata": {},
   "source": [
    "(Super) Clutch trick to parallelize accumulation using matrix multiplication with lower triangular matrix:\n",
    "\n",
    "Think of matrix multiplication from first principles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed956f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "------\n",
      "tensor([[9., 5.],\n",
      "        [4., 5.],\n",
      "        [7., 4.]])\n",
      "------\n",
      "tensor([[ 9.,  5.],\n",
      "        [13., 10.],\n",
      "        [20., 14.]])\n",
      "------\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.]])\n",
      "------\n",
      "tensor([[20., 14.],\n",
      "        [11.,  9.],\n",
      "        [ 7.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1667)\n",
    "L = torch.tril(torch.ones(3,3))\n",
    "# L = L.mean(dim = 1, keepdim=True)\n",
    "U = torch.tril(torch.ones(3,3)).T\n",
    "\n",
    "a = torch.randint(1,10,(3,2)).float()\n",
    "\n",
    "print(L)\n",
    "print('------')\n",
    "print(a)\n",
    "print('------')\n",
    "print(L @ a)\n",
    "print('------')\n",
    "print(U)\n",
    "print('------')\n",
    "print(U@a) # accumulation along columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d8dee",
   "metadata": {},
   "source": [
    "See how the accumulation of rows of `a` happens across the rows of `L@a`? Further if we normalize `a` along the columns we get the mean accumulation in `L@a`. <br>\n",
    "Now lets implement this same in out $(B,T,C)$ dimensional matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f7995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2 - using lower tril matrix\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(dim = 1, keepdim= True) # normalize \n",
    "\n",
    "# weighted aggregation through matrix multi seen above\n",
    "xbow2 = wei @ x # (T,T) @ (B,T,C) => pytorch adds batch dimension \n",
    "\n",
    "# verify\n",
    "torch.allclose(xbow[0], xbow2[0]) #--  same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcd41cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb665cea",
   "metadata": {},
   "source": [
    "`(T,T) @ (B,T,C)` $\\implies$ pytorch adds batch dimension $\\implies$ `(B,T,T) @ (B,T,C)` $\\implies$ `(B) + (T,T @ T,C)` $\\implies$ `(B,T,C)\n",
    "\n",
    "<img src=\"images/transformer_visual.jpg\" style=\"width:70%;\">`\n",
    "\n",
    "Parallelization is achieved along `B` and context accumulation along `T` dimension is also satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9465a84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v3 - using softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)\n",
    "wei = wei.softmax(dim =1)\n",
    "\n",
    "xbow3 = wei @ x # braodcast along batch dimension: (B,T,T) @ (B,T,C)\n",
    "torch.allclose(xbow3[0], xbow[0]) # verify similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070eb6a",
   "metadata": {},
   "source": [
    "Basically, we first replace all positions where `tril == 0` with `-inf` and then take softmax, which end up having a normalizing effect. \n",
    "\n",
    "This is interesting because: \n",
    "\n",
    "- `wei = torch.zeros((T,T))`: can be interpreted as an 'interaction strength' at initialization\n",
    "- `wei = wei.masked_fill(tril == 0, float('-inf'))`: info from future tokens is masked <br>\n",
    "Then other steps are accumulation. \n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Summary of the above section:</span><br>\n",
    "You can do weighthed aggregation of your past elements by using matrix multiplication with a lower triangular matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8819b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention!\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros_like(tril) # loses information, not learnable\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(dim = -1) # need softmax along 'C' dimension\n",
    "\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fad80",
   "metadata": {},
   "source": [
    "\n",
    "The `wei` matrix doesnt have to be zeros. It can be learnt, based on how much weight each token assigns to the each of the previous token!\n",
    "\n",
    "## Crux of attention mechanism:\n",
    "\n",
    "__Problem that self attention solves:__ How to gather information from the past in a data dependent way?\n",
    "\n",
    "Every single token has 2 attributes:\n",
    "- `query` vector: What is being looked for?\n",
    "- `key` vector: What information does it contain?\n",
    "\n",
    "Crux: The key of a token dot products with query of other tokens and that is sotred in `wei`!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a2fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw wei outputs:\n",
      " tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
      "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
      "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# lets introduce a single head with self attention \n",
    "head_size = 16 # hyperparam\n",
    "key = nn.Linear(C,head_size, bias=False)\n",
    "query = nn.Linear(C,head_size, bias=False)\n",
    "\n",
    "# forward x\n",
    "k = key(x) #(B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "#find wei\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "\n",
    "print(f'Raw wei outputs:\\n {wei[0]}')\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros_like(tril)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # to hide future token\n",
    "wei = wei.softmax(dim = -1)\n",
    "\n",
    "# we accumulate value(x), not x directly!\n",
    "value = nn.Linear(C,head_size, bias=False)\n",
    "out = wei @ value(x)\n",
    "\n",
    "out.shape # (B,T, head_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddeb6f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>) \n",
      " Sum along columns:\n",
      " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((wei[0]), '\\n',\"Sum along columns:\\n\", wei[0].sum(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969e5b1",
   "metadata": {},
   "source": [
    "### Interpretation of `wei`\n",
    "\n",
    "The wei matrix is the attention weights - it's a $(B,T,T)$ tensor where `wei[b,i,j]` tells us __\"how much should token i pay attention to token j\" in batch b__\n",
    "This dot product computes compatibility between queries and keys:\n",
    "\n",
    "- If `q[i]` (what token i is looking for) is similar to `k[j]` (what token j offers), their dot product is high\n",
    "- After softmax, high compatibility becomes high attention weight\n",
    "\n",
    "Example:<br>\n",
    "The dog chased the cat, and _it_ ran away $\\implies$ `wei[position_of_\"it\", position_of_\"cat\"]` would be high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68da05",
   "metadata": {},
   "source": [
    "Now `wei` doesnt have a uniform structure i.e. we dont assign equal weight to each preceeding token. Now it is __variable and more importantly, can be learnt.__ Great explanation from the [master himself](https://youtu.be/kCc8FmEb1nY?si=DVwpogFwnz4z-9YY&t=4024)\n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Notes:</span><br>\n",
    "\n",
    "- Attention is a `communication` mechanism. It can be seen in the below graph, as a data dependent (learnable) aggregation of weights from different tokens. \n",
    "\n",
    "<img src=\"images/attention_as_graph.png\" style=\"width:40%;\">` <br>\n",
    "- There is no notion of space. Attention simply acts over space. Which is why positional encoding is important. (unlike convolution where space in built in)\n",
    "- There is no communication across Batch dimension - allowing parallel ops\n",
    "\n",
    "\n",
    "### Encoding vs Decoding\n",
    "\n",
    "- Encoder blocks: allow all the tokens to communicate (future can also influence) $\\implies$ masking step is removed, rest all remains the same.\n",
    "- Decoder block (what we have implemented above): future tokens cant communicate with past ones. \n",
    "\n",
    "### Self vs cross attention\n",
    "\n",
    "Key, query and value are __all__ applied on `x`; sometimes key may be outsourced to a different tensor before dot product is taken. This called cross product. \n",
    "\n",
    "### Scaling self-attention \n",
    "\n",
    "<img src=\"images/scaling_vaswani_et_al.png\" style=\"width:60%;\">` <br>\n",
    "\n",
    "\n",
    "In the original paper, the authors propose scaling the the dot product of Q (query) and K (key) by $\\frac{1}{\\sqrt(d_k)}$, where $d_k = $ head size. This allows to preserve variance to 1 after the dot product, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d47e97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.6638) tensor(0.9790)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "\n",
    "wt1 = q @ k.transpose(-2,-1) # ~ head_size\n",
    "wt2 = q @ k.transpose(-2,-1) * head_size**-0.5 # ~ 1\n",
    "\n",
    "print(wt1.var(), wt2.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa158ca2",
   "metadata": {},
   "source": [
    "Preserving variance close to 1 is necessary or else softmax will converge to a one hot encoding vector and weights for a particular node will be unduly high at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66ae39",
   "metadata": {},
   "source": [
    "## A transformer from first priciples:\n",
    "\n",
    "A transformer is a neural network architecture that processes sequences of data (like text) __by learning relationships between all elements simultaneously, rather than one at a time__. Let me build this up from the ground up.\n",
    "The Core Problem\n",
    "__Traditional neural networks for sequences (like RNNs) process data sequentially - word by word, left to right__. This creates two major limitations: it's slow because you can't parallelize the computation, and it struggles to connect distant elements in long sequences due to the _vanishing gradient_ problem.\n",
    "The Key Insight: Attention\n",
    "The transformer's breakthrough is the attention mechanism. Instead of processing sequentially, attention lets the model look at all positions in the sequence simultaneously and decide which ones are most relevant to each other.\n",
    "Think of it like this: when you read the sentence \"The cat sat on the mat because it was comfortable,\" you instantly know \"it\" refers to \"the cat\" by considering the whole sentence context. __Attention mechanizes this intuition.__\n",
    "How Attention Works\n",
    "Self-attention computes three vectors for each word:\n",
    "\n",
    "- Query (Q): \"What am I looking for?\"\n",
    "- Key (K): \"What do I represent?\"\n",
    "- Value (V): \"What information do I contain?\"\n",
    "\n",
    "For each word, you compute similarity scores between its query and all other words' keys. These scores determine how much to \"attend to\" each word. You then take a weighted sum of all the value vectors based on these attention scores.\n",
    "Mathematically: <br> \n",
    "\n",
    "\n",
    "Attention $(Q,K,V)$ = softmax $(QK^T/\\sqrt(d))V$ <br>\n",
    "\n",
    "\n",
    "The softmax ensures attention weights sum to 1, and √d prevents the dot products from getting too large.\n",
    "\n",
    "### Multi-Head Attention\n",
    "Rather than having just one attention mechanism, transformers use multiple \"heads\" in parallel. __Each head learns different types of relationships - one might focus on grammatical dependencies, another on semantic similarity, etc.__ The outputs are concatenated and projected back to the original dimension.\n",
    "### The Complete Architecture\n",
    "A transformer has two main components: <br>\n",
    "1. Encoder: Processes the input sequence and builds rich representations. Each encoder layer contains:\n",
    "\n",
    "2. Multi-head self-attention (words attend to other words in the same sequence)\n",
    "Feed-forward neural network\n",
    "Residual connections and layer normalization around both\n",
    "\n",
    "3. Decoder: Generates the output sequence. Each decoder layer has:\n",
    "\n",
    "4. Masked self-attention (prevents looking at future words during training)\n",
    "5. Cross-attention (attends to the encoder's output)\n",
    "6. Feed-forward network\n",
    "7. Residual connections and layer normalization\n",
    "\n",
    "### Why This Works So Well\n",
    "- Parallelization: Since attention looks at all positions simultaneously, you can compute everything in parallel rather than sequentially.\n",
    "Long-range dependencies: Direct connections between any two positions mean the model can easily relate distant elements.\n",
    "- Flexibility: The same architecture works for many tasks - translation, text generation, question answering - just by changing the training objective.\n",
    "- Scalability: Transformers scale remarkably well with more data and parameters, leading to models like GPT and BERT.\n",
    "The transformer essentially turned sequence modeling from a sequential, limited process into a massively parallel, globally-aware one. This architectural shift enabled the current era of large language models and has revolutionized not just NLP, but computer vision and other domains as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62585c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensional Flow Through Transformer\n",
    "#Starting point <br>\n",
    "input_ids: (B, T)                    # Raw token indices\n",
    "\n",
    "#Step 1: Embeddings\n",
    "token_emb: (B, T, n_embd)           # Token embeddings\n",
    "pos_emb: (B, T, n_embd)             # Positional embeddings  \n",
    "x = token_emb + pos_emb: (B, T, n_embd)\n",
    "\n",
    "#Step 2: Multi-Head Attention\n",
    "#Each head produces: (B, T, head_size)\n",
    "#where head_size = n_embd // n_heads\n",
    "\n",
    "head_1: (B, T, head_size)\n",
    "head_2: (B, T, head_size)\n",
    "...\n",
    "head_n: (B, T, head_size)\n",
    "\n",
    "#Concatenate all heads\n",
    "concat_heads: (B, T, n_heads * head_size)\n",
    "#Note: n_heads * head_size = n_embd (by design)\n",
    "\n",
    "#Linear projection after concatenation\n",
    "attn_output: (B, T, n_embd)\n",
    "\n",
    "#Step 3: Feedforward Block  \n",
    "ff_input: (B, T, n_embd)\n",
    "ff_hidden: (B, T, d_ff)              # Typically d_ff = 4 * n_embd\n",
    "ff_output: (B, T, n_embd)            # Back to original dimension\n",
    "\n",
    "#Step 4: Final Output (after all layers)\n",
    "final_hidden: (B, T, n_embd)\n",
    "logits: (B, T, vocab_size)           # Linear projection to vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
