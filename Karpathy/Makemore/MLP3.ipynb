{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11130028",
   "metadata": {},
   "source": [
    "## MLP for language modelling\n",
    "\n",
    "Implementing Bengio et al paper to develop an MLP for language modelling. \n",
    "\n",
    "It introduces the idea of vector embeddings to capture semantic proximity, instead of explicitly calculating probabilities for each possible combination of words which wouldn't generalize well. __17,000 words__ are considered in the dataset. \n",
    "\n",
    "### Architecture\n",
    "\n",
    "<img src=\"../papers/architecture.png\" style=\"width:70%;\">\n",
    "\n",
    "__Explanation of architecture:__ \n",
    "- 3 previous words are used as context and indexed as $w_i$\n",
    "- An embedding of that word is shared from a global matrix $C$ and used as input for the hidden layer. \n",
    "- Size of hidden layer is a hyperparameter\n",
    "- post which `tanh` non-linearity is applied\n",
    "- finally there is a fully connected output layer (with __17,000 neurons -- one for each word__)\n",
    "- softmax is applied to choose the most likely word\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- The lookup table $C$ (embedding matrix)\n",
    "- $W_i, b_i$ for hidden layer\n",
    "- $W_i, b_i$ for output layer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368efe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e0148",
   "metadata": {},
   "source": [
    "We will implement the same architecture above not for sentences as is done in Bengio et al, but for individual names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060fd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995d06da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb077850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stoi \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc779bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some = [0]*3\n",
    "some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8604a75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212732ad",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "\n",
    "Use 3 previous letter to guess the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7ebca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -------> e\n",
      "..e -------> m\n",
      ".em -------> m\n",
      "emm -------> a\n",
      "mma -------> .\n",
      "olivia\n",
      "... -------> o\n",
      "..o -------> l\n",
      ".ol -------> i\n",
      "oli -------> v\n",
      "liv -------> i\n",
      "ivi -------> a\n",
      "via -------> .\n",
      "ava\n",
      "... -------> a\n",
      "..a -------> v\n",
      ".av -------> a\n",
      "ava -------> .\n"
     ]
    }
   ],
   "source": [
    "X , Y = [], []\n",
    "block_size= 3 # can be reset to whatever you like\n",
    "\n",
    "for w in words[:3]:\n",
    "    #'emma'\n",
    "    print(w)\n",
    "    context = [0]*block_size # contains indcies of context letters\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # 'e' -> 5\n",
    "        Y.append(ix) # 5 is the target\n",
    "        X.append(context)\n",
    "        print(\"\".join(itos[i] for i in context), '------->', ch)\n",
    "        context = context[1:] + [ix] # update context and append new index\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f5300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13]]) tensor([ 5, 13, 13])\n",
      "torch.Size([16, 3]) torch.int64 torch.Size([16]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X[:3], Y[:3])\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247483",
   "metadata": {},
   "source": [
    "So we have X with 3 (integer) features as out input, Y is a scalar (integer) output. \n",
    "\n",
    "Now lets build the embedding loop table $C$:\n",
    "- We have $27$ possible characters, which we will try to embed into a lower dimension space (unlike one-hot encoding, which is still 27 dimensional!)\n",
    "- In the _paper_ they compress $17000$ words to $30$ dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50bc905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1438,  1.3808],\n",
       "        [ 1.5834,  0.0049],\n",
       "        [-1.5679,  1.1807]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27,2)) # each of 27 characters has a 2D embedding\n",
    "\n",
    "C[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59ef99",
   "metadata": {},
   "source": [
    "Now how to acess the embedding for a single integer, say $5$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3538, 0.7665])\n",
      "tensor([1.3538, 0.7665])\n"
     ]
    }
   ],
   "source": [
    "# option 1: index into C directly\n",
    "print(C[5])\n",
    "\n",
    "# option 2: one-hot encode 5 and then multiply -- as was done in bigram \n",
    "print(F.one_hot(torch.tensor(5), num_classes=27).float() @ C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f204f14",
   "metadata": {},
   "source": [
    "Introducting `.float()` is ajust an occupational hazard. Note that both of the above way give the same embedding tensor for $5$. \n",
    "\n",
    "Going forward we will just extract the row directly using the index. \n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Question:</span>: Now how to we convert X: 16*3 into embeddings? We must leverage pytorch indexing flexibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f29a771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3538,  0.7665],\n",
       "        [-1.5971, -1.9288],\n",
       "        [ 0.4243,  0.1791],\n",
       "        [ 0.4243,  0.1791],\n",
       "        [ 0.4243,  0.1791]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,13,4,4,4]]  # retrieves 5th, 13th and 4th row of C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc502d",
   "metadata": {},
   "source": [
    "We indexed with 1 dimensional tensor of integers. But turns out we can also index with 2 dimensional tensor of integers. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85ead244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1438,  1.3808],\n",
       "          [-0.1438,  1.3808],\n",
       "          [-0.1438,  1.3808]],\n",
       " \n",
       "         [[-0.1438,  1.3808],\n",
       "          [-0.1438,  1.3808],\n",
       "          [ 1.3538,  0.7665]]]),\n",
       " torch.Size([16, 3, 2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][:2], C[X].shape # dim(X) = 16*3 and each element has 2 dim embedding => 16*3*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1f036",
   "metadata": {},
   "source": [
    "More experimentation on higher dimension tensor indexing in the data structures notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
