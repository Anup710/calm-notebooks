{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8146bfb7",
   "metadata": {},
   "source": [
    "## Backprop and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a188c9c",
   "metadata": {},
   "source": [
    "<img src=\"pictures/chain rule.png\" width=\"50%\">\n",
    "\n",
    "#### The steps:\n",
    "\n",
    "<img src=\"pictures/steps.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fc774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddc04b",
   "metadata": {},
   "source": [
    "Consider a simple neuron with one input and no bias. Then we apply the least square error to compute loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "w = torch.tensor(1.0, requires_grad=True) # since we need to update w in backprop\n",
    "y_true = torch.tensor(2.0)\n",
    "\n",
    "#linear regression\n",
    "y_pred = w*x\n",
    "loss = (y_pred - y_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d2ae0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ca985",
   "metadata": {},
   "source": [
    "This one step: calling `.backward()` will compute the local gradients and assign derivative to w. (and _not_ to intermediate variables such as y_pred, as seen in earlier notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703be4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b6ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2046257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights: \n",
    "with torch.no_grad():\n",
    "    w -= 0.1*w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c83d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6400, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2 = w*x\n",
    "loss2 = (y_pred2 - y_true)**2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff111974",
   "metadata": {},
   "source": [
    "Loss has decreased from 1.0 to 0.64 after 1 iteration of backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e252138",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb80f95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.6000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eee48",
   "metadata": {},
   "source": [
    "Then you can continue..\n",
    "\n",
    "\n",
    "#### Manual linear regression \n",
    "\n",
    "Pytorch allows for automation of all these steps using the mentioned modules, but we will first do a fully manual run on a simple 1 dimensional problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90bc5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training f(5) =  0.0\n",
      "epoch 0: w = [0.248] | loss = 24.8\n",
      "epoch 4: w = [0.9683069] | loss = 8.599724336109533\n",
      "epoch 8: w = [1.39247109] | loss = 2.982066881333652\n",
      "epoch 12: w = [1.64224692] | loss = 1.0340706907786803\n",
      "epoch 16: w = [1.7893314] | loss = 0.35857753567528283\n",
      "epoch 20: w = [1.87594444] | loss = 0.12434144999713344\n",
      "epoch 24: w = [1.92694791] | loss = 0.04311702393256027\n",
      "epoch 28: w = [1.95698211] | loss = 0.014951391936026497\n",
      "epoch 32: w = [1.97466823] | loss = 0.005184590689151571\n",
      "epoch 36: w = [1.98508299] | loss = 0.001797824625897734\n",
      "After training f(5) =  9.949862318549542\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using mean squared error and gradient descent in single dimension\n",
    "\n",
    "# define dataset\n",
    "X = np.array([-1,1,2,3,4], dtype=np.float32)\n",
    "# simple function: y = 2*x \n",
    "y = np.array([-2,2,4,6,8], dtype=np.float32)\n",
    "\n",
    "# w = np.random.rand(1)\n",
    "w = np.zeros(1)\n",
    "\n",
    "def predict(w,x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y_pred, y_true):\n",
    "    return ((y_pred-y_true)**2).mean()\n",
    "\n",
    "def gradient(x, y_true, y_pred):\n",
    "    return np.mean(2*x*(y_pred - y_true))\n",
    "\n",
    "# lets run gradient descent: \n",
    "\n",
    "print(\"Before training f(5) = \", (w*5)[0])\n",
    "\n",
    "n_iters = 40\n",
    "learning_rate = 0.01\n",
    "\n",
    "i = 0\n",
    "for i in range(n_iters):\n",
    "\n",
    "    # calculate prediction \n",
    "    y_pred = predict(w,X)\n",
    "\n",
    "    # loss\n",
    "    mse_loss = loss(y_pred, y)\n",
    "\n",
    "    # calculate grad\n",
    "    grad = gradient(X, y, y_pred)\n",
    "\n",
    "    # update w\n",
    "    w -= learning_rate*grad\n",
    "\n",
    "    if (i % 4 ==0) :\n",
    "        print(f\"epoch {i}: w = {w} | loss = {mse_loss}\")\n",
    "\n",
    "\n",
    "print(\"After training f(5) = \", (w*5)[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3c954",
   "metadata": {},
   "source": [
    "- True f(5) = 2*x = 2*5 = 10\n",
    "- Predicted after 40 iterations = 9.94\n",
    "\n",
    "We can see the loss decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66de650",
   "metadata": {},
   "source": [
    "Now lets automate parts of the above code in numpy with pytorch. \n",
    "\n",
    "<img src = 'pictures/pipeline.png' width = '30%'>\n",
    "\n",
    "\n",
    "### Training pipeline in pytorch: \n",
    "\n",
    "<ol>\n",
    "    <li> Design model (Input, optput size, forward pass)\n",
    "    <li> Construct loss (mean square, softmax, log loss) and optimizer (gd, sgd, newton etc)\n",
    "    <li> Training loop:\n",
    "    <ul>\n",
    "        <li> Forward pass: computation of y_pred\n",
    "        <li> backward pass: assign gradient to leaf nodes \n",
    "        <li> update weights, baises \n",
    "    </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fa7201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = -3.499\n",
      "epoch  1 : w =  -0.2859164774417877  loss =  tensor(54.8922, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.417089819908142  loss =  tensor(1.5142, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.6973741054534912  loss =  tensor(0.1277, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.7486273050308228  loss =  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.7628586292266846  loss =  tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.7709583044052124  loss =  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.7778998613357544  loss =  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.7844887971878052  loss =  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.790859341621399  loss =  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.7970378398895264  loss =  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 9.593\n"
     ]
    }
   ],
   "source": [
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986c9b1",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "so we have used a simple linear regression model existing in the torch.nn module -- here it is done using a neural network with no hidden layer, just a single neuron. \n",
    "\n",
    "Loss is `MSELoss()` available in the torch.nn module. \n",
    "\n",
    "Finally SGD optimizer is available in the `torch.optim` module. \n",
    "\n",
    "But we can see that backprop step using .backward() on loss, and weight updation using .step() is automated to our earlier version in numpy. <br>\n",
    "Weights and biases are stored as list of lists hence accessed as `w[0][0]`and stored in `model.parameters()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
