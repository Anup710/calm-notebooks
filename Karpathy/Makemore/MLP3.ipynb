{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11130028",
   "metadata": {},
   "source": [
    "## MLP for language modelling\n",
    "\n",
    "Implementing Bengio et al paper to develop an MLP for language modelling. \n",
    "\n",
    "It introduces the idea of vector embeddings to capture semantic proximity, instead of explicitly calculating probabilities for each possible combination of words which wouldn't generalize well. __17,000 words__ are considered in the dataset. \n",
    "\n",
    "### Architecture\n",
    "\n",
    "<img src=\"../papers/architecture.png\" style=\"width:70%;\">\n",
    "\n",
    "__Explanation of architecture:__ \n",
    "- 3 previous words are used as context and indexed as $w_i$\n",
    "- An embedding of that word is shared from a global matrix $C$ and used as input for the hidden layer. \n",
    "- Size of hidden layer is a hyperparameter\n",
    "- post which `tanh` non-linearity is applied\n",
    "- finally there is a fully connected output layer (with __17,000 neurons -- one for each word__)\n",
    "- softmax is applied to choose the most likely word\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- The lookup table $C$\n",
    "- $W_i, b_i$ for hidden layer\n",
    "- $W_i, b_i$ for output layer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368efe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
