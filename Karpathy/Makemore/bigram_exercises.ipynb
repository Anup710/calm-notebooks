{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed8bbdd",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "\n",
    "1. train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model? \n",
    "\n",
    "2. split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see? \n",
    "\n",
    "3. use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve? \n",
    "\n",
    "4. we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W? \n",
    "\n",
    "5. look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83049637",
   "metadata": {},
   "source": [
    "### Trigram\n",
    "\n",
    "Strategy: \n",
    "- need to create a $x \\rightarrow y $ mapping dataset first. Key question is: will it be `(2,27)` or `(54,)`. In the former case $x_i$ is a 2-d input matrix, while in the latter it is flattened into a vector. \n",
    "- flattening it into a `(54,)` after one-hot encoding is a better strategy to avoid complications in math. Banter with chatgpt why. (definition of loss must be changed, dimension of tensor W as well)\n",
    "- Define NLL\n",
    "- `W` will be `(54,27)`: 54 features in $x$, 27 no of neurons, `with_grad = True`\n",
    "- forward pass: compute `xenc @ W` and apply softmax on it. \n",
    "- back pass: call `loss.backward()`\n",
    "- update, flush gradients\n",
    "- iterate till convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c2dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d04bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7438d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "letters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(letters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfac78f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3ed5a",
   "metadata": {},
   "source": [
    "The above is some sloppy code which runs into problems of data type and vectorization. Below is some clean, parallelizable code to prepare the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020874f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']  # add start and end tokens\n",
    "    for i in range(len(chs) - 2):\n",
    "        ix1 = stoi[chs[i]]\n",
    "        ix2 = stoi[chs[i + 1]]\n",
    "        ix3 = stoi[chs[i + 2]]  # target\n",
    "\n",
    "        xs.append([ix1, ix2])  # context: 2 indices\n",
    "        ys.append(ix3)         # label: 1 index\n",
    "\n",
    "# convert to tensors\n",
    "xs = torch.tensor(xs)      # shape: [N, 2]\n",
    "ys = torch.tensor(ys)      # shape: [N]\n",
    "\n",
    "# one-hot encode context\n",
    "xs_oh = F.one_hot(xs, num_classes=27).float()  # shape: [N, 2, 27]\n",
    "xenc = xs_oh.view(xs_oh.shape[0], -1)         # reshape to [N, 54]b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = ys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff2efca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113, 54]), torch.Size([196113]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb45039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Weights\n",
    "g = torch.Generator().manual_seed(344675)\n",
    "W = torch.randn((54,27), generator=g, requires_grad=True)\n",
    "# W.data *= 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfcc408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.081151962280273\n",
      "2.3730344772338867\n",
      "2.306246042251587\n",
      "2.2830734252929688\n",
      "2.2710041999816895\n",
      "2.2635629177093506\n",
      "2.25852370262146\n",
      "2.2548999786376953\n",
      "2.2521848678588867\n",
      "2.250087261199951\n",
      "2.248427152633667\n",
      "2.2470877170562744\n",
      "2.2459897994995117\n",
      "2.245077133178711\n",
      "2.244309663772583\n",
      "2.243657350540161\n",
      "2.2430973052978516\n",
      "2.242612600326538\n",
      "2.242189884185791\n",
      "2.2418181896209717\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim = 1)\n",
    "\n",
    "for k in range(400):\n",
    "\n",
    "    #forward pass\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # loss = -probs[torch.arange(num), ys].log().mean() + 0.001*(W**2).mean() # with a regulariziation loss\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()  # without a regulariziation loss\n",
    "    \n",
    "    if k%20 ==0:\n",
    "        print(loss.item()) \n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # grad flushing\n",
    "    loss.backward()   \n",
    "\n",
    "    # update\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a3bc6",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">OBSERVATION:</span>\n",
    "\n",
    "If NLL is used: the trigram loss saturates around 2.25, while the bigram loss saturated around 2.45 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c0d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya.\n",
      "syahle.\n",
      "wan.\n",
      "ullekhim.\n",
      "ugwnya.\n"
     ]
    }
   ],
   "source": [
    "# lets sample from the trigram \n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0  # Start with '.'\n",
    "    ix2 = 0  # Two start tokens for trigram context ('.', '.')\n",
    "\n",
    "    while True:\n",
    "        # Create input vector from two context characters\n",
    "        xenc = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()  # shape (2, 27)\n",
    "        xenc = torch.cat([xenc[0], xenc[1]])  # shape (54,)\n",
    "\n",
    "        logits = xenc @ W  # (54,) @ (54, 27) => (27,)\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "\n",
    "        ix_next = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix_next])\n",
    "\n",
    "        if ix_next == 0:\n",
    "            break\n",
    "\n",
    "        # Shift context\n",
    "        ix1, ix2 = ix2, ix_next\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de2e45",
   "metadata": {},
   "source": [
    "The final names are not necessarily 'better' or more human like and important lessons and considerations are\n",
    "- choice of a loss function matters a lot than the actual value tied to it\n",
    "- lower loss may not convert into tangibly better results. \n",
    "- may be you are optimizing the wrong loss fn \n",
    "\n",
    "The final loss which is around 2.25 for my implementation seems to differ from 2.09 obtained by others on the YT channel. I'm not able to place why, despite the code which looks alright to me. \n",
    "\n",
    "Is there some hyperparameter tuning I'm missing or what?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
