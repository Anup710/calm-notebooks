{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8f14a9",
   "metadata": {},
   "source": [
    "To understand workings of positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bf215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Understanding Position Embeddings ===\n",
      "Embedding table shape: torch.Size([8, 32])\n",
      "This can handle positions 0 to 7\n",
      "\n",
      "Position indices for sequence length 5: tensor([0, 1, 2, 3, 4])\n",
      "Position embeddings shape: torch.Size([5, 32])\n",
      "Each position gets a 32-dimensional embedding vector\n",
      "\n",
      "=== Testing different sequence lengths ===\n",
      "Sequence length 1: indices=[0], embedding_shape=torch.Size([1, 32])\n",
      "Sequence length 3: indices=[0, 1, 2], embedding_shape=torch.Size([3, 32])\n",
      "Sequence length 8: indices=[0, 1, 2, 3, 4, 5, 6, 7], embedding_shape=torch.Size([8, 32])\n",
      "\n",
      "=== What goes wrong ===\n",
      "Error with indices [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]: index out of range in self\n",
      "\n",
      "=== How it works in the forward pass ===\n",
      "Input shape (B, T): torch.Size([2, 6])\n",
      "Token embeddings shape: torch.Size([2, 6, 32])\n",
      "Position embeddings shape: torch.Size([6, 32])\n",
      "Combined embeddings shape: torch.Size([2, 6, 32])\n",
      "\n",
      "Broadcasting works because:\n",
      "  tok_emb: torch.Size([2, 6, 32]) (token info for each position in each batch)\n",
      "  pos_emb: torch.Size([6, 32]) (position info, same for all batches)\n",
      "  Result:  torch.Size([2, 6, 32]) (each token gets both token and position info)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up the same parameters as in the original code\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "\n",
    "# Create the position embedding table\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "print(\"=== Understanding Position Embeddings ===\")\n",
    "print(f\"Embedding table shape: {position_embedding_table.weight.shape}\")\n",
    "print(f\"This can handle positions 0 to {block_size-1}\")\n",
    "\n",
    "# This is what the code should do\n",
    "T = 5  # sequence length (example)\n",
    "device = 'cpu'\n",
    "\n",
    "# Create position indices for sequence of length T\n",
    "position_indices = torch.arange(T, device=device)\n",
    "print(f\"\\nPosition indices for sequence length {T}: {position_indices}\")\n",
    "\n",
    "# Get position embeddings\n",
    "pos_emb = position_embedding_table(position_indices)\n",
    "print(f\"Position embeddings shape: {pos_emb.shape}\")\n",
    "print(f\"Each position gets a {n_embd}-dimensional embedding vector\")\n",
    "\n",
    "# Show what happens with different sequence lengths\n",
    "print(\"\\n=== Testing different sequence lengths ===\")\n",
    "for seq_len in [1, 3, 8]:\n",
    "    indices = torch.arange(seq_len)\n",
    "    embeddings = position_embedding_table(indices)\n",
    "    print(f\"Sequence length {seq_len}: indices={indices.tolist()}, embedding_shape={embeddings.shape}\")\n",
    "\n",
    "# This would cause an error (index out of range)\n",
    "print(\"\\n=== What goes wrong ===\")\n",
    "try:\n",
    "    bad_indices = torch.arange(10)  # indices 0-9, but we only have positions 0-7\n",
    "    bad_embeddings = position_embedding_table(bad_indices)\n",
    "except IndexError as e:\n",
    "    print(f\"Error with indices {bad_indices.tolist()}: {e}\")\n",
    "\n",
    "# Demonstrate the full forward pass concept\n",
    "print(\"\\n=== How it works in the forward pass ===\")\n",
    "batch_size = 2\n",
    "seq_length = 6\n",
    "vocab_size = 100\n",
    "\n",
    "# Simulate input\n",
    "idx = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "print(f\"Input shape (B, T): {idx.shape}\")\n",
    "\n",
    "# Token embeddings\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "tok_emb = token_embedding_table(idx)\n",
    "print(f\"Token embeddings shape: {tok_emb.shape}\")\n",
    "\n",
    "# Position embeddings (corrected version)\n",
    "B, T = idx.shape  # Extract T from input shape\n",
    "pos_indices = torch.arange(T, device=device)\n",
    "pos_emb = position_embedding_table(pos_indices)\n",
    "print(f\"Position embeddings shape: {pos_emb.shape}\")\n",
    "\n",
    "# Broadcasting addition\n",
    "x = tok_emb + pos_emb  # Broadcasting: (B,T,n_embd) + (T,n_embd) -> (B,T,n_embd)\n",
    "print(f\"Combined embeddings shape: {x.shape}\")\n",
    "\n",
    "print(f\"\\nBroadcasting works because:\")\n",
    "print(f\"  tok_emb: {tok_emb.shape} (token info for each position in each batch)\")\n",
    "print(f\"  pos_emb: {pos_emb.shape} (position info, same for all batches)\")\n",
    "print(f\"  Result:  {x.shape} (each token gets both token and position info)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db719f",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Pytorch nuance:</span><br>\n",
    "\n",
    "So nn.Embedding supports calling using a tensor, but a normal tensor wont!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ebfee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embd = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "x_regular = torch.randn((block_size, n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d7e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7521,  1.8812,  0.8526,  0.5902,  1.5629,  0.9514, -0.7993,  1.8407,\n",
      "        -1.2865, -0.2323,  0.6199, -1.2639,  0.0909, -0.1176, -0.5648, -0.1640,\n",
      "        -0.8622,  0.6527, -0.0118,  0.1396,  0.1709,  1.1068, -1.0915, -0.9353,\n",
      "         0.7777,  1.2527, -0.8601, -1.0533, -0.9846, -0.0555, -0.3534,  0.0202],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_embd(torch.arange(block_size))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5323ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx_regular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "print(x_regular(torch.arange(block_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546588d1",
   "metadata": {},
   "source": [
    "So a nn.Embedding() object is callable while a regular tensor is not!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
