{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5033b8e",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "\n",
    "A lot of oddity of LLMs comes from tokenizers, into which we will deep dive today. \n",
    "\n",
    "In little shakespeare nano-gpt, we simply used character level tokenizers. This casues the context length to explode. It is shown that he __complexiety of train time increase quadratically with context length__, hence we are incentivized to have longer token sizes. <br>\n",
    "Sub-word tokenizers are most commonly used, using algorithms such as __Byte pair encoding__. \n",
    "\n",
    "__Tokenization is essentially the process of converting text into sequences of integers and vice versa.__\n",
    "\n",
    "\n",
    "Experiment with tokenizers here: https://www.google.com/url?q=https%3A%2F%2Ftiktokenizer.vercel.app\n",
    "\n",
    "- Tokenization can be perplexing; for ex. \n",
    "1. Egg  = E + gg\n",
    "2. I have an egg = I h + ave + an + egg\n",
    "3. Egg vs egg vs EGG\n",
    "\n",
    "The tokenization for the same word can be done differently depending on case, context etc. \n",
    "\n",
    "- encoding: 'Eg' (char) -> 1223 (token) -> fetch embedding of 1223rd row from emb table \n",
    "- decoding: vice versa\n",
    "\n",
    "- Training data usually has much more english than other. Further, in say Korean or Japanese, the tokens are smaller (at max 2 chars) as opposed to english, with longer subword tokens. <br>\n",
    "=> More tokens => less context can fit within the block_size\n",
    "\n",
    "- In gpt-2 tokenizer for instance; ' ' is a token. So indentation in python is multiple tokens and we end up bloating the transformer while conveying little useful information. Hence, gpt-2 performed poorly on python tasks. \n",
    "\n",
    "- the same text may have different # of tokens depending on which tokenizer it is. \n",
    "    - __Intuition__: gpt 4 has roughly 2 $\\times$ the tokens as the gpt 2 tokenizer. More is not always better. Since dim of embedding table $\\uparrow$, output porbabilities become more scattered. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f575e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5e32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527fe5f",
   "metadata": {},
   "source": [
    "what even is a string in python? => [unicode code points](https://en.wikipedia.org/wiki/Unicode)\n",
    "\n",
    "can be accessed using the `ord()` function in python. `chr()` is the inverse of that. \n",
    "\n",
    "A full breakdown of [unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee166ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075]\n"
     ]
    }
   ],
   "source": [
    "print([ord(char) for char in \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f26091",
   "metadata": {},
   "source": [
    "Why not use unicode based tokenization? \n",
    "- Vocab would be quite long ~ 150k chars \n",
    "- keeps changing\n",
    "\n",
    "Standardization attempts lead to [UTf-8](http://utf8everywhere.org/), Utf-16, UTf-32 encodings. UTF-8 is the only one which is backword compatible to the ASCII format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f33d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!\") #25\n",
    "\n",
    "list((\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!\").encode('utf-8')) # len = 38\n",
    "\n",
    "list((\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!\").encode('utf-16')) # len = 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ae445",
   "metadata": {},
   "source": [
    "One may remark that UTF-16 is looser and seems a bit 'wasteful' since it expresses the same string using a longer list. \n",
    "\n",
    "But _why_ is len of utf-8 encoding more than # of chars? $\\implies$ because ASCII chars (a,e,b,',', k,l, % etc)  take 1 byte, korean char takes 3 bytes whereas the emoji take 4 bytes. <br>\n",
    "\n",
    "__Each byte will essentially be a number between 0-255. So emoji = [23,90,88,157] or \"A\" = [65] or \"e\" = [101] and so on.__\n",
    "\n",
    "Still, even UTF-8 may seem like a waste of space. So we come up with a middle ground: [__Byte pair encoding__](https://en.wikipedia.org/wiki/Byte-pair_encoding)\n",
    "\n",
    "Now lets implement the BPE on a simple text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c760e081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = open('text.txt').read() -- doesnt work, since default encoding cp1252 cant read enojis etc; so specify encoding!!\n",
    "\n",
    "with open('text.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "list(text[:5].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86bd650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "combined bytes in hex: b'\"\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89' | all bytes in hex together: 22efbcb5efbd8eefbd89\n",
      "[34, 239, 188, 181, 239, 189, 142, 239, 189, 137]\n"
     ]
    }
   ],
   "source": [
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "\n",
    "print(type(tokens))\n",
    "print(f'combined bytes in hex: {tokens[:10]} | all bytes in hex together: {tokens[:10].hex()}')\n",
    "\n",
    "tokens = list(map(int, text.encode('utf-8'))) # some formatting \n",
    "\n",
    "print(tokens[:10])\n",
    "\n",
    "# print(len(text), len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395240",
   "metadata": {},
   "source": [
    "`b'\"\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89'`: b indicates byte class, `\\x` indicates next 2 chars are hexadecimal (ef, bc, b5, bd, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 111, 109, 101, 32, 119, 111, 114, 100, 115]\n",
      "[115, 111, 109, 101, 32, 119, 111, 114, 100, 115]\n"
     ]
    }
   ],
   "source": [
    "some = 'some words'\n",
    "print(list(map(int, some.encode('utf-8')))) # map is not needed hmm..\n",
    "\n",
    "print(list(some.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d25d4",
   "metadata": {},
   "source": [
    "Step 1 of BPE: Iterate over utf-8 encoded text (`tokens`) and find the pair of bytes which occurs most frequently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20c2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most frequently occuring pair first \n",
    "# a very pythonic way to do this: \n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0173e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (32, 240)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (46, 34)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (34, 239)), (1, (33, 240)), (1, (33, 32)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to sort by v - which is the first variable. \n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True)) \n",
    "\n",
    "# equivalent: max(stats, key=lambda k: stats[k])\n",
    "# 'ranking by the value to get the maximum key' \n",
    "top_pair = max(stats, key = stats.get) \n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40688964",
   "metadata": {},
   "source": [
    "Note:<br>\n",
    "Andrej's implementation for `top_pair` just runs way faster.. <br>\n",
    "\n",
    "```def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(ids)\n",
    "pair = max(stats, key=stats.get)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2a3a565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats2(ids):\n",
    "\n",
    "    #self-implementation using a counts table\n",
    "    \n",
    "    dim = max(ids)\n",
    "    count = torch.zeros(dim+1,dim+1) # +1 to avoid index overspill - think!\n",
    "\n",
    "    for k in range(len(ids)-1):\n",
    "        count[ids[k], ids[k+1]]  += 1\n",
    "    \n",
    "    # arg max for 2D\n",
    "    flat_index = torch.argmax(count) \n",
    "    row = flat_index // count.shape[1]\n",
    "    col = flat_index % count.shape[1]\n",
    "    return (row.item(), col.item())\n",
    "\n",
    "top_pair = get_stats2(tokens)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cf1da",
   "metadata": {},
   "source": [
    "essentially karpathy returns a dictionary in get_stats() and then does some ops to extract the most frequent token pair, I do it natively in a table itself using arg max. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "129d2c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b68c9c",
   "metadata": {},
   "source": [
    "$\\implies$ 'e ' is the most common token pair, so a lot of words in the text seems to end with 'e'. \n",
    "\n",
    "Since we have 0-255 codes already defined, we must define a new code (256) for 'e '. \n",
    "\n",
    "The merge logic can be captured in a function: <br>\n",
    "__P.S.:__ (I wrote one on my own, but it missed edge cases and could induce bugs - Need to genuinely learn these basic manipulations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bfac2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length =  617 \n",
      "Length after merge =  597\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and (ids[i],ids[i+1]) == (pair[0], pair[1]):\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print('Original length = ', len(tokens), '\\nLength after merge = ', len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5d850",
   "metadata": {},
   "source": [
    "So now we have the tools to idenity the top_pair and merge it. <br>\n",
    "\n",
    "__So much do we compress? That is a hyperparameter__. More iteration => larger vocab, smaller context size. \n",
    "\n",
    "Lets perform this exercise on the complete text of [this blog](https://www.reedbeta.com/blog/programmers-intro-to-unicode/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0017db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22636, 23884)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('more_text.txt', encoding='utf-8') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "full_tokens = full_text.encode(\"utf-8\") # raw bytes\n",
    "full_tokens = list(map(int, full_tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "\n",
    "len(full_text), len(full_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580eea32",
   "metadata": {},
   "source": [
    "Now lets make a coherent cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "579a9046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256 | sequence length = 23252\n",
      "merging (105, 110) into a new token 257 | sequence length = 22813\n",
      "merging (115, 32) into a new token 258 | sequence length = 22402\n",
      "merging (116, 104) into a new token 259 | sequence length = 22065\n",
      "merging (101, 114) into a new token 260 | sequence length = 21777\n",
      "merging (99, 111) into a new token 261 | sequence length = 21490\n",
      "merging (116, 32) into a new token 262 | sequence length = 21208\n",
      "merging (226, 128) into a new token 263 | sequence length = 20962\n",
      "merging (44, 32) into a new token 264 | sequence length = 20720\n",
      "merging (97, 110) into a new token 265 | sequence length = 20494\n",
      "merging (111, 114) into a new token 266 | sequence length = 20283\n",
      "merging (100, 32) into a new token 267 | sequence length = 20074\n",
      "merging (97, 114) into a new token 268 | sequence length = 19897\n",
      "merging (101, 110) into a new token 269 | sequence length = 19728\n",
      "merging (257, 103) into a new token 270 | sequence length = 19565\n",
      "merging (261, 100) into a new token 271 | sequence length = 19403\n",
      "merging (121, 32) into a new token 272 | sequence length = 19258\n",
      "merging (259, 256) into a new token 273 | sequence length = 19114\n",
      "merging (97, 108) into a new token 274 | sequence length = 18971\n",
      "merging (111, 110) into a new token 275 | sequence length = 18832\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(full_tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "length_post_merge = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "  pair = get_stats2(ids)\n",
    "  idx = 256 + i\n",
    "  ids = merge(ids, pair, idx)\n",
    "  \n",
    "  # tracking\n",
    "  length_post_merge.append(len(ids))\n",
    "  merges[pair] = idx\n",
    "\n",
    "  print(f\"merging {pair} into a new token {idx} | sequence length = {length_post_merge[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "52e51697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 20 merges:\n",
      "compression ratio = 23884 -> 18832 = 1.27x\n"
     ]
    }
   ],
   "source": [
    "print(f\"In {num_merges} merges:\")\n",
    "print(f\"compression ratio = {len(full_tokens)} -> {length_post_merge[-1]} = {len(full_tokens)/length_post_merge[-1]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21144f",
   "metadata": {},
   "source": [
    "## Tokenizer is independent of the LM\n",
    "\n",
    "Note, the Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text.\n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"llm_tokenizer.png\" width = 60%>\n",
    "\n",
    "It facilitates raw text $\\rightleftharpoons$ tokens conversion. \n",
    "\n",
    "The diversity of your tokenizer training set directly affects the LM performance (french, japanese, code presence and density) because the merges are determined in that way. \n",
    "\n",
    "### Next step\n",
    "\n",
    "Now lets create a decode function which accepts list of integers and return the corresponding python string while incorporating our new characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e62aee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    \"\"\"Returns a python string corresponding to the input list on integers ids\"\"\"\n",
    "    reconstructed = bytes(ids).decode('utf-8')\n",
    "    return reconstructed\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff1fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x11'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([17]) # \\x = in hexadecimal; b = byte class object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
