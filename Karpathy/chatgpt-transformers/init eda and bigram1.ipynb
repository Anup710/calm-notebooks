{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1af74df",
   "metadata": {},
   "source": [
    "## Chatgpt: under the hood\n",
    "\n",
    "This video was launched just a few months after chatgpt became a sensation in early 2023. Since then the models have become more powerful.\n",
    "\n",
    "The neural network under the hood which models the words -- is defined in the landmark paper [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et Al in 2017. GPT stands for __generatively pretrained transformer__. \n",
    "\n",
    "_Our_ goal here is to draw upon the principles of the transformer architecture to reproduce _Shakespearesque_ text after training on the tiny shakespeare dataset. \n",
    "\n",
    "__In this notebook we will perform some EDA and create a bigram model on shakespeare to set a baseline performance.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c6ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fa6262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input.txt', <http.client.HTTPMessage at 0x1e2910e25a0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "local_path = \"input.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f4a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc33b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset = 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'length of dataset = {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa9b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f053162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz | 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars), '|' ,vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84f2ac",
   "metadata": {},
   "source": [
    "Note that ' ' (space) is also a character, included at the very beginning of the above chars. Next we need to create a mapping from characters to integers to tokenize them effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6154213",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "# stoi = {s:i for s,i in zip(chars, range(len(chars)))}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "# print(itos, stoi)\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2977c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 47, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 14, 59, 41, 49, 63, 1, 14, 39, 56, 52, 43, 57, 9, 2]\n",
      "Hii my name is Bucky Barnes3!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hii my name is Bucky Barnes3!\"))\n",
    "print(decode(encode(\"Hii my name is Bucky Barnes3!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c46ae",
   "metadata": {},
   "source": [
    "The above schema is a very simple encoding-decoding mechanism. For ex. google uses [Sentence Piecing](https://github.com/google/sentencepiece), which encodes text into integers and it is a _subword_ tokenizer. OpenAI on the other hand, uses [tiktoken](https://github.com/openai/tiktoken):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a26ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24912, 2375, 770, 3608, 4429, 18, 45326, 2090, 2372, 348, 704, 385] | 12\n",
      "Total chars in vocab = 200019\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\") # for gpt 4o\n",
    "code = enc.encode(\"hello world im barney3-Anya_Magçus\")\n",
    "print(code, '|',len(code))\n",
    "\n",
    "print('Total chars in vocab =', enc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e38e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15339, 1917, 737, 3703, 3520, 18, 59016, 7911, 1267, 351, 3209, 355]\n",
      "Total chars in vocab = 100277\n"
     ]
    }
   ],
   "source": [
    "enc2 = tiktoken.get_encoding(\"cl100k_base\")  # for GPT-3.5 / GPT-4-family\n",
    "print(enc2.encode(\"hello world im barney3-Anya_Magçus\"))\n",
    "\n",
    "print('Total chars in vocab =', enc2.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bac88",
   "metadata": {},
   "source": [
    "Just for reference: `o200k_base` encoder of OPENAI has 200019 __sub-words__, in contrast to our 65. These tiktoken based encoders do not need API calls and are contained offline within the library.\n",
    "\n",
    "While tiktoken is a subword tokenizer, we will stick with our character level encoder for this video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af81954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # this is how it will look like to our GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7e9ce",
   "metadata": {},
   "source": [
    "Now lets create a test train split:\n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">NOTE:</span> Shuffling this sequence is a _bad_ idea. The order of words is significant when it comes to NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd2e06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.9*(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8421f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*(len(data)))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976fdce",
   "metadata": {},
   "source": [
    "### Training the transformer (TF)\n",
    "\n",
    "We never actually feed in the entire dataset into the TF, as it is too computationally expensive. It is only a chunk (`block_size` or `context_length`). Consider this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4830a580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # hyperparameter\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3f2f6",
   "metadata": {},
   "source": [
    "This above text of `len = block_size+1` consists of `block_size` number of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2ba9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is 47\n",
      "When input is tensor([18, 47]) the target is 56\n",
      "When input is tensor([18, 47, 56]) the target is 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n",
      "------------- \n",
      "(torch.int64, torch.int64)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # next block_size chars, offset by 1\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "\n",
    "    print(f\"When input is {context} the target is {target}\")\n",
    "\n",
    "print('-------------',f'\\n{x.dtype, y.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae23b59",
   "metadata": {},
   "source": [
    "Extracting each layer of this information is essential so that the 'transformer is used to seeing' all lengths (up tp `block_size`) of context when we sample with as little as one character later on!\n",
    "\n",
    "Lets now create minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb6e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # no of samples in one batch\n",
    "block_size = 8 # context length of each batch\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train_data if split == 'train' else val_data # train_data and val_data define globally \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix], dim = 0) # stack along rows\n",
    "    y = torch.stack([data[i+1 : i+1+block_size] for i in ix], dim = 0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9bfc9",
   "metadata": {},
   "source": [
    "- `block_size` is subtracted while sampling `ix` for batch to ensure index doesnt go out of bounds at the ends.\n",
    "\n",
    "Note that `xb` if chosen plainly cannot be 'stacked' vertically since dimension increases continuously. SO we define `xb`, `yb` differently in the above function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a654859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb_stats: xb_shape = torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # seed for this cell\n",
    "xb,yb = get_batch('train')\n",
    "\n",
    "print(f\"xb_stats: xb_shape = {xb.shape}\")\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape,'\\n',yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300624bf",
   "metadata": {},
   "source": [
    "Lets visualize the information contained in this batch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6db50e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data point 0\n",
      "for context = [24], target = 43 \n",
      "for context = [24, 43], target = 58 \n",
      "for context = [24, 43, 58], target = 5 \n",
      "for context = [24, 43, 58, 5], target = 57 \n",
      "for context = [24, 43, 58, 5, 57], target = 1 \n",
      "for context = [24, 43, 58, 5, 57, 1], target = 46 \n",
      "for context = [24, 43, 58, 5, 57, 1, 46], target = 43 \n",
      "for context = [24, 43, 58, 5, 57, 1, 46, 43], target = 39 \n",
      "data point 1\n",
      "for context = [44], target = 53 \n",
      "for context = [44, 53], target = 56 \n",
      "for context = [44, 53, 56], target = 1 \n",
      "for context = [44, 53, 56, 1], target = 58 \n",
      "for context = [44, 53, 56, 1, 58], target = 46 \n",
      "for context = [44, 53, 56, 1, 58, 46], target = 39 \n",
      "for context = [44, 53, 56, 1, 58, 46, 39], target = 58 \n",
      "for context = [44, 53, 56, 1, 58, 46, 39, 58], target = 1 \n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size - 2): # captures essence: to shorten the output batch_size is 4 and too long!\n",
    "    print(f\"data point {b}\")\n",
    "    for t in range(block_size):\n",
    "        local_context = xb[b , :t+1]\n",
    "        local_target = yb[b, t ]\n",
    "\n",
    "        print(f\"for context = {local_context.tolist()}, target = {local_target} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5ad8d",
   "metadata": {},
   "source": [
    "So we will feed `xb`, `yb` into the transformer and it will simultaneously create the above mapping to capture all information contained in it. \n",
    "\n",
    "__Terminology for a batch:__ dimension = (B,T,C), where \n",
    "- B = batch size\n",
    "- T = time (no of features)\n",
    "- C = no of channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ed62e",
   "metadata": {},
   "source": [
    "We run into a small technical error while trying to return `cross_entropy` loss. Upon checking the documentation for [nn.functional.cross_entropy](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy), in case of multidimensional logits (here they are __4,8,65__ dimension), the shape expected is `B,C,T` instead of the current `B,T,C` ; where C = 65 is the dimension along which we want loss to be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "433a6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # each char has 'vocab_size' dimensional embedding\n",
    "        print(type(self.token_embedding_table)) # not your usual tensor!\n",
    "\n",
    "    def forward(self,idx, targets):\n",
    "        # idx and targets are both of dimension (B,T)\n",
    "        print(idx.shape, targets.shape)\n",
    "        logits = self.token_embedding_table(idx) # B,T,C: C = embedding dimension\n",
    "\n",
    "        # transform shape to confirm with cross_entropy expectation \n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935580f",
   "metadata": {},
   "source": [
    "logits = logits.view(B*T, C) stretches out (4,8,65) to (32,65) and <br>\n",
    "targets: (4,8) -> (32,) <br>\n",
    "Now we can apply cross entropy (as each row in logits corresponds to a target)\n",
    "\n",
    "### Loss interpretation:\n",
    "\n",
    "To interpret the initial loss = 4.87: so loss on sampling from a unifrom distribution will be $ln\\frac{1}{65} \\approx 4.17$. So our initial logits are quite bad and need to be fixed iteratively!\n",
    "<hr>\n",
    "\n",
    "\n",
    "__Next__, Lets add generate ability to our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c21a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7663, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # each char has 'vocab_size' dimensional embedding\n",
    "        # print(type(self.token_embedding_table)) # not your usual tensor!\n",
    "\n",
    "    def forward(self,idx, targets = None):\n",
    "        # idx and targets are both of dimension (B,T)\n",
    "        \n",
    "        # if targets is not None:\n",
    "            # print(idx.shape, targets.shape)\n",
    "        \n",
    "        logits = self.token_embedding_table(idx) # B,T,C: C = embedding dimension\n",
    "\n",
    "        if targets == None:\n",
    "            # there is no concept of loss\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # transform shape to confirm with cross_entropy expectation \n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Takes input matrix idx (B,T) and predicts next character in time dimension and \n",
    "        does that until max_new_tokens, i.e. (B,T+max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get current predictions\n",
    "            logits, loss = self(idx)\n",
    "            # isolate relevant context - last index for bigram\n",
    "            logits = logits[:,-1,:] # (B,C)\n",
    "            # find prob distribution\n",
    "            probs = torch.nn.functional.softmax(logits, dim = 1) # along columns\n",
    "            # predict next char\n",
    "            next_idx = torch.multinomial(probs, 1) # (B,1)\n",
    "            # append\n",
    "            idx = torch.cat((idx, next_idx), dim = 1) # (B,T+1)\n",
    "\n",
    "        return idx \n",
    "\n",
    "\n",
    "# ----------------------call the class and run it----------------------------------------------------------  \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "out = m.generate(xb, 10)\n",
    "print(logits.shape)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a02f9",
   "metadata": {},
   "source": [
    "Note that `__call__` method specifically looks for a method named `forward`. It's hardcoded in PyTorch's implementation. If you renamed your method to something else, then calling `self(idx)` in `def generate()` will not work as expected. \n",
    "\n",
    "Sampling from a basic bigram model without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "980d4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 18])\n",
      "Let's hevv?qP-QWktfor thatS\n",
      "vviYDEsxnt that -KHQDCV&ojMEO:\n",
      "I p&Q:EJh$fRF\n"
     ]
    }
   ],
   "source": [
    "print(xb.shape, out.shape)\n",
    "\n",
    "# sampling from out rudiemntary model\n",
    "out = out.view(-1)\n",
    "print(''.join(itos[k.item()] for k in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85bec4",
   "metadata": {},
   "source": [
    "Lets now create a optimizer and train our model fr:\n",
    "\n",
    "We will choose the [AdamW](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer instead of our regular batch GD. The key difference is: Adam gives each parameter its own personalized learning rate based on its gradient history, making optimization much more efficient than vanilla gradient descent.\n",
    "\n",
    "More math and intuition can be developed by interacting with an LLM on this. \n",
    "\n",
    "`Adam` and `AdamW` are _different_ in pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a28dd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b875f43",
   "metadata": {},
   "source": [
    "lets increase the batch size to 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d4c9f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.625051498413086\n",
      "3.6976656913757324\n",
      "3.0774714946746826\n",
      "2.764822483062744\n",
      "2.4982857704162598\n",
      "2.468160629272461\n",
      "2.5126700401306152\n",
      "2.3820960521698\n",
      "2.4768080711364746\n",
      "2.4248673915863037\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 1000 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89825b89",
   "metadata": {},
   "source": [
    "lets sample with intial context as idx = 0, which is '\\n':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8f2b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "APingof acat nd l,\n",
      "Fothind aty y:\n",
      "ARDUTA llllld!\n",
      "AMQUThes med thestw cos wand herf s hafold mZWirus je ney biPoeronngabsestouMOLAUCES: ONDohery ththe tonmy th, fourf thatys ng dd pp qur ace Einowhemy azer:\n",
      "I,\n",
      "Ishit tinghast ha tteredef seariomams.\n",
      "Makine,\n",
      "\n",
      "Than ts hientr?\n",
      "3nd woft re y l, uCABe codauseabertierr,\n",
      "her tr fed?\n",
      "NCK: p?\n",
      "S:\n",
      "Awo!\n",
      "I'l,\n",
      "Atyxevee ugiARIn telo,\n",
      "ANG cousk, thoa hroro s lly ndst on meave S:\n",
      "\n",
      "QUSTAg? therecr.\n",
      "ULofre,\n",
      "We, sthablddff; chof fome ureswir anqur t h sele s wame me \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baec7c",
   "metadata": {},
   "source": [
    "The output is a dramatic improvement from the gibberish we got without training at loss $\\approx 4.7$ <br>Thus far, we are only using the last character (the premise of the bigram model) in making a prediction. We will also port this code into a single `.py` so that it can be run as a script. \n",
    "\n",
    "__Features of the script:__\n",
    "\n",
    "1. addition of 'device': to use gpu if you have one and move variables to _that_ device so that calculations are faster. \n",
    "2. loss measurement: printing within the loop is very noisy so its better to track average after `eval_interval` no of iterations\n",
    "3. To include modes - whether model is in __training mode__ or __eval mode__ and set that accordingly \n",
    "\n",
    "#### Next steps:\n",
    "Now we will let the tokens talk to each other and use higher quality and quantity of context. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
