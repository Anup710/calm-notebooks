{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5033b8e",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "\n",
    "A lot of oddity of LLMs comes from tokenizers, into which we will deep dive today. \n",
    "\n",
    "In little shakespeare nano-gpt, we simply used character level tokenizers. This casues the context length to explode. It is shown that he __complexiety of train time increase quadratically with context length__, hence we are incentivized to have longer token sizes. <br>\n",
    "Sub-word tokenizers are most commonly used, using algorithms such as __Byte pair encoding__. \n",
    "\n",
    "__Tokenization is essentially the process of converting text into sequences of integers and vice versa.__\n",
    "\n",
    "\n",
    "Experiment with tokenizers here: https://www.google.com/url?q=https%3A%2F%2Ftiktokenizer.vercel.app\n",
    "\n",
    "- Tokenization can be perplexing; for ex. \n",
    "1. Egg  = E + gg\n",
    "2. I have an egg = I h + ave + an + egg\n",
    "3. Egg vs egg vs EGG\n",
    "\n",
    "The tokenization for the same word can be done differently depending on case, context etc. \n",
    "\n",
    "- encoding: 'Eg' (char) -> 1223 (token) -> fetch embedding of 1223rd row from emb table \n",
    "- decoding: vice versa\n",
    "\n",
    "- Training data usually has much more english than other. Further, in say Korean or Japanese, the tokens are smaller (at max 2 chars) as opposed to english, with longer subword tokens. <br>\n",
    "=> More tokens => less context can fit within the block_size\n",
    "\n",
    "- In gpt-2 tokenizer for instance; ' ' is a token. So indentation in python is multiple tokens and we end up bloating the transformer while conveying little useful information. Hence, gpt-2 performed poorly on python tasks. \n",
    "\n",
    "- the same text may have different # of tokens depending on which tokenizer it is. \n",
    "    - __Intuition__: gpt 4 has roughly 2 $\\times$ the tokens as the gpt 2 tokenizer. More is not always better. Since dim of embedding table $\\uparrow$, output porbabilities become more scattered. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42f575e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5e32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527fe5f",
   "metadata": {},
   "source": [
    "what even is a string in python? => [unicode code points](https://en.wikipedia.org/wiki/Unicode)\n",
    "\n",
    "can be accessed using the `ord()` function in python\n",
    "\n",
    "A full breakdown of [unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee166ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075]\n"
     ]
    }
   ],
   "source": [
    "print([ord(char) for char in \"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f26091",
   "metadata": {},
   "source": [
    "Why not use unicode based tokenization? \n",
    "- Vocab would be quite long ~ 150k chars \n",
    "- keeps changing\n",
    "\n",
    "Standardization attempts lead to [UTf-8](http://utf8everywhere.org/), Utf-16, UTf-32 encodings. UTF-8 is the only one which is backword compatible to the ASCII format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f33d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!\") #25\n",
    "\n",
    "list((\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!\").encode('utf-8')) # len = 38\n",
    "\n",
    "list((\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!\").encode('utf-16')) # len = 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ae445",
   "metadata": {},
   "source": [
    "One may remark that UTF-16 is looser and seems a bit 'wasteful' since it expresses the same string using a longer list. \n",
    "\n",
    "But _why_ is len of utf-8 encoding more than # of chars? $\\implies$ because ASCII chars (a,e,b,',', k,l, % etc)  take 1 byte, korean char takes 3 bytes whereas the emoji take 4 bytes. <br>\n",
    "Still, even UTF-8 may seem like a waste of space. So we come up with a middle ground: [__Byte pair encoding__](https://en.wikipedia.org/wiki/Byte-pair_encoding)\n",
    "\n",
    "Now lets implement the BPE on a simple text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c760e081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = open('text.txt').read() -- doesnt work, since default encoding cp1252 cant read enojis etc; so specify encoding!!\n",
    "\n",
    "with open('text.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "list(text[:5].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86bd650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(int, text.encode('utf-8'))) # some formatting \n",
    "\n",
    "# print(len(text), len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d25d4",
   "metadata": {},
   "source": [
    "Step 1 of BPE: Iterate over utf-8 encoded text (`tokens`) and find the pair of bytes which occurs most frequently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a20c2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most frequently occuring pair first \n",
    "# a very pythonic way to do this: \n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d0173e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (32, 240)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (46, 34)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (34, 239)), (1, (33, 240)), (1, (33, 32)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "# (i,j) for i,j in stats.items()\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0022b",
   "metadata": {},
   "source": [
    "What does `max(stats, key=stats.get)` do?\n",
    "\n",
    "By default, `max(dictionary)` would return the largest key (based on Python‚Äôs default ordering, not useful here).\n",
    "\n",
    "But when you give it `key=stats.get`, you‚Äôre telling Python: <br>\n",
    "üëâ ‚ÄúInstead of comparing the keys directly, compare them based on their values in the dictionary.‚Äù\n",
    "\n",
    "### Internally:\n",
    "\n",
    "Evaluate step by step\n",
    "\n",
    "For `('a','b')`: `stats.get(('a','b'))` = 3\n",
    "\n",
    "For `('b','c')`: `stats.get(('b','c'))` = 5\n",
    "\n",
    "For `('c','d')`: `stats.get(('c','d'))` = 2\n",
    "\n",
    "__Now it just picks the key with the largest value: ('b','c') because 5 is the maximum.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b05c9602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent: max(stats, key=lambda k: stats[k])\n",
    "\n",
    "max(stats, key = stats.get) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2a3a565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats2(ids):\n",
    "\n",
    "    #self-implementation using a counts table\n",
    "    \n",
    "    dim = max(ids)\n",
    "    count = torch.zeros(dim+1,dim+1) # +1 to avoid index overspill - think!\n",
    "\n",
    "    for k in range(len(tokens)-1):\n",
    "        count[tokens[k], tokens[k+1]]  += 1\n",
    "    \n",
    "    # arg max for 2D\n",
    "    flat_index = torch.argmax(count) \n",
    "    row = flat_index // count.shape[1]\n",
    "    col = flat_index % count.shape[1]\n",
    "    return (row.item(), col.item())\n",
    "\n",
    "get_stats2(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cf1da",
   "metadata": {},
   "source": [
    "essentially karpathy returns a dictionary in get_stats() and then does some ops to extract the most frequent token pair, I do it natively in a table itself using arg max. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
