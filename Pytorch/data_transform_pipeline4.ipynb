{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab80009",
   "metadata": {},
   "source": [
    "## Dataset, dataloader and data transform classes \n",
    "\n",
    "So far we have loaded datasets using numpy (read_csv / read_excel) or from sklearn.datasets, we will explore pytorch alternatives for that. \n",
    "\n",
    "- Also, we running the forward pass on entire dataset each time is time consuming. So we will divide it into batches, using the `Dataset `and `DataLoader` classes. \n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "__Epoch__: 1 forward and backward pass of ALL training samples\n",
    "\n",
    "__batch size__: defines how many samples are processed at a time before updating the weights.\n",
    "\n",
    "__no of iteration__: a single update step — i.e., one forward + backward pass using one batch.\n",
    "\n",
    "eg: 100 samples, batch_size = 20 --> 100/20 = 5 iterations per epoch \n",
    "\n",
    "```for epoch in range(5):                   # 5 passes over data\n",
    "    for batch in data_loader:            # 10 batches per epoch\n",
    "        # Forward pass\n",
    "        # Compute loss\n",
    "        # Backward pass\n",
    "        # Optimizer step```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38beb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e080d2",
   "metadata": {},
   "source": [
    "## Importing torch datasets from numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7b65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('dataset/wine.csv', delimiter=',', skiprows=1, dtype=np.float32)\n",
    "        self.X = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, 0])\n",
    "        self.n_samples = xy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # to enable indexing and extract rows \n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # to check the size of dataset\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = WineDataset()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdeb9878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.__len__())\n",
    "\n",
    "first_datapoint = dataset[0]\n",
    "first_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e7c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_0 =  tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) label =  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# capture features and label for first datapoint\n",
    "features_0, label_0 = dataset[0]\n",
    "print('features_0 = ', features_0, 'label = ', label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data for further processing into the model\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4eb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1810e+01, 2.1200e+00, 2.7400e+00, 2.1500e+01, 1.3400e+02, 1.6000e+00,\n",
      "         9.9000e-01, 1.4000e-01, 1.5600e+00, 2.5000e+00, 9.5000e-01, 2.2600e+00,\n",
      "         6.2500e+02],\n",
      "        [1.1870e+01, 4.3100e+00, 2.3900e+00, 2.1000e+01, 8.2000e+01, 2.8600e+00,\n",
      "         3.0300e+00, 2.1000e-01, 2.9100e+00, 2.8000e+00, 7.5000e-01, 3.6400e+00,\n",
      "         3.8000e+02],\n",
      "        [1.1030e+01, 1.5100e+00, 2.2000e+00, 2.1500e+01, 8.5000e+01, 2.4600e+00,\n",
      "         2.1700e+00, 5.2000e-01, 2.0100e+00, 1.9000e+00, 1.7100e+00, 2.8700e+00,\n",
      "         4.0700e+02],\n",
      "        [1.2250e+01, 4.7200e+00, 2.5400e+00, 2.1000e+01, 8.9000e+01, 1.3800e+00,\n",
      "         4.7000e-01, 5.3000e-01, 8.0000e-01, 3.8500e+00, 7.5000e-01, 1.2700e+00,\n",
      "         7.2000e+02]]) tensor([2., 2., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# convert to an iterator and look at one random sample -- iter() and next() are \n",
    "# rarely needed in practice except to look at how the batch looks like. Like df.head() maybe. \n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0088b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "Epoch: 1/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 1/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2])\n",
      "Epoch: 2/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4])\n",
      "Epoch: 2/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Dummy Training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
    "        # Run your training process\n",
    "        \n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbaad4",
   "metadata": {},
   "source": [
    "input size = 4,13 => batch size = 4 (set above) and 13 are no of features of each datapoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a511c",
   "metadata": {},
   "source": [
    "## Inbuilt datasets in torch.vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m        module\n",
      "\u001b[31mString form:\u001b[39m <module 'torchvision.datasets' from 'C:\\\\Users\\\\AN80050181\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\torchvision\\\\datasets\\\\__init__.py'>\n",
      "\u001b[31mFile:\u001b[39m        c:\\users\\an80050181\\appdata\\roaming\\python\\python313\\site-packages\\torchvision\\datasets\\__init__.py\n",
      "\u001b[31mDocstring:\u001b[39m   <no docstring>"
     ]
    }
   ],
   "source": [
    "?torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.datasets.MNIST(root='') # plug in various arguments as needed. \n",
    "# fashion-mnist, cifar, coco etc are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c275c",
   "metadata": {},
   "source": [
    "### Transformation in pytorch:\n",
    "\n",
    "PyTorch doesn’t have built-in transformers like SimpleImputer, StandardScaler, or MinMaxScaler the way scikit-learn does.\n",
    "\n",
    "BUT — you don’t have to write a new custom class every time either. There are a few strategies:\n",
    "\n",
    "Option 1: Use sklearn for preprocessing. This is the most common and totally acceptable.Then convert into tensor data objects just before training. \n",
    "\n",
    "Option 2: Use PyTorch transforms (but mostly for images) <br>\n",
    "```from torchvision import transforms```\n",
    "This gives you tools like:\n",
    "\n",
    "- transforms.ToTensor()\n",
    "\n",
    "- transforms.Normalize(mean, std)\n",
    "\n",
    "- transforms.Resize()\n",
    "\n",
    "- transforms.RandomCrop()\n",
    "\n",
    "- transforms.Compose([...])\n",
    "\n",
    "__BUT: These are image-focused.__\n",
    "\n",
    "__If you’re working with tabular or text data → sklearn is better for preprocessing.__\n",
    "\n",
    "Option 3: Write your own preprocessing and use Dataset<br>\n",
    "You can write a custom PyTorch Dataset class and add your own logic inside `__getitem__()`:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
