{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2415fad",
   "metadata": {},
   "source": [
    "The [GPT 2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) in section 2.2 talks about BPE to enforce sub-word tokenization. \n",
    "\n",
    "Through this NB, we will dive deeper into [encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py) file (which is misnamed since it encodes _and_ decodes). \n",
    "<hr>\n",
    "\n",
    "1. The GPT2 paper incorporates a regex split, to avoid similarity in subwords due to the greedy nature of BPE such as `dog.` , `dog!` , `dog?` being classified as separate tokens. Hoever, `'dog '` is more common and the spaces are allowed which proves to be helpful in compressing information at a higher level. Decoupling punctuation from semantics. \n",
    "\n",
    "2. So this is done by enfocring some \"merging rules\". i.e. words cannot combine with punctuations and so on to for a subword. \n",
    "\n",
    "3. Regex library is used to enforce these separations. \n",
    "\n",
    "\n",
    "## Forced splits using Regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4b06af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' HOW', \"'\", 'S', ' are', '       ', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\") # picked from gpt2 encoder.py\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's HOW'S are        you!!!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ba4fb",
   "metadata": {},
   "source": [
    "On a high level: we are trying not to merge across letters, punctuations, numbers, abbreviations. \n",
    "\n",
    "## Important:\n",
    "\n",
    "In nb1 we took the entire text and passed it through the `encoder` to get tokens. But practically:\n",
    "\n",
    "1. We enforce the split using regex => `['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']`\n",
    "\n",
    "2. Apply encode on each element of the list \n",
    "\n",
    "3. Then concatinate\n",
    "\n",
    "In this process, each list element is tokenized independently before concat and our regex rules are followed!\n",
    "\n",
    "<hr>\n",
    "\n",
    "__Some objections to the regex pattern in gpt2 paper:__\n",
    "\n",
    "- HOW'S vs how's tokenizes differently (case sensetive)\n",
    "- ' vs ’ tokenizes differently (apostrophe)\n",
    "- Langague (english) is hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25670ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17f78c",
   "metadata": {},
   "source": [
    "There are some additional rules OPENAI has enforced, such as: spaces are never merged. For ex: \"    \" + \"  \" dont get merged. It not clear how they have enforced this, since __`encoder.py` is just the inference code__, _not training code_. \n",
    "\n",
    "__tiktoken is the official openai library for tokenization (again, only for inference)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af01d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185, 1633]\n",
      "[996, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!! air\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"       hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec14ee1",
   "metadata": {},
   "source": [
    "Curiously, for gpt 4 tokenizer: 1 space, 2 spaces, 3 spaces.. each correspond to a different token. WHile for gpt-2, each space corresponds to `220`. \n",
    "\n",
    "Checking the tiktoken repo openai_public, [clk_100 base tokenizer for gpt 4](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py) we see the regex pattern has evolved: <br>\n",
    "`\"pat_str\": r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\"`\n",
    "\n",
    "- Some problem from _gpt-2 regex_ string have been fixed here (case sensativity, limiting (merged) number lengths to 3, punctuations etc.)\n",
    "\n",
    "\n",
    "\n",
    "Further note that for a string such as `\"      hello world\"` with 6 trailing spaces gpt-4 would tokenize it as: `\"     \" + \" hello\"` i.e. 1 trailing space is attached to hello token and all 6 are not grouped into 1. This was learnt during training perhaps. See the below token division by gpt-4:\n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"images/gpt-4_tokenizer.png\" width = 30%>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now lets check a few more aspects of encoder.py of gpt2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aff89fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder.json', <http.client.HTTPMessage at 0x17fe49040e0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\", \n",
    "    \"vocab.bpe\"\n",
    ")\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\", \n",
    "    \"encoder.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22189d",
   "metadata": {},
   "source": [
    "__Note:__ The BPE token IDs in encoder.json are completely different from UTF-8 byte values!\n",
    "\n",
    "But [encoder.json](encoder.json) plays the same role as `vocab` dictionary in [base1.ipynb](base1.ipynb) notebook. It allows us to efficiently switch between integer and bytes of that integer. <br>\n",
    "\n",
    "While wading though [encoder.json](encoder.json), `Ġ` makes many an appearance. It just represents a leading space. \n",
    "\n",
    "Also note that\n",
    "- Kaprpathy starts with a base UTF-8 vocabulary (indices 0-255 for all possible bytes) and then adds BPE merges starting from index 256\n",
    "- whereas, OPENAI uses a custom base vocabulary that doesn't follow the simple 0-255 byte mapping, still adds BPE merges on top\n",
    "-  The core BPE algorithm (finding most frequent pairs and merging them) is the same in both cases - it's just the starting vocabulary that differs\n",
    "\n",
    "Whereas [vocab.bpe](/tokenizer/vocab.bpe) is a list of merges carried out on the training text. We depart a bit and maintain merges as a dict in [base1.ipynb](/tokenizer/base1.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bed708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json \n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # equivalent to 'vocab' of base1 nb\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90487952",
   "metadata": {},
   "source": [
    "## Special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fe74e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder)  # 256 raw byte tokens. 50,000 merges. +1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425eb67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '¡': 94,\n",
       " '¢': 95,\n",
       " '£': 96,\n",
       " '¤': 97,\n",
       " '¥': 98,\n",
       " '¦': 99,\n",
       " '§': 100,\n",
       " '¨': 101,\n",
       " '©': 102,\n",
       " 'ª': 103,\n",
       " '«': 104,\n",
       " '¬': 105,\n",
       " '®': 106,\n",
       " '¯': 107,\n",
       " '°': 108,\n",
       " '±': 109,\n",
       " '²': 110,\n",
       " '³': 111,\n",
       " '´': 112,\n",
       " 'µ': 113,\n",
       " '¶': 114,\n",
       " '·': 115,\n",
       " '¸': 116,\n",
       " '¹': 117,\n",
       " 'º': 118,\n",
       " '»': 119,\n",
       " '¼': 120,\n",
       " '½': 121,\n",
       " '¾': 122,\n",
       " '¿': 123,\n",
       " 'À': 124,\n",
       " 'Á': 125,\n",
       " 'Â': 126,\n",
       " 'Ã': 127,\n",
       " 'Ä': 128,\n",
       " 'Å': 129,\n",
       " 'Æ': 130,\n",
       " 'Ç': 131,\n",
       " 'È': 132,\n",
       " 'É': 133,\n",
       " 'Ê': 134,\n",
       " 'Ë': 135,\n",
       " 'Ì': 136,\n",
       " 'Í': 137,\n",
       " 'Î': 138,\n",
       " 'Ï': 139,\n",
       " 'Ð': 140,\n",
       " 'Ñ': 141,\n",
       " 'Ò': 142,\n",
       " 'Ó': 143,\n",
       " 'Ô': 144,\n",
       " 'Õ': 145,\n",
       " 'Ö': 146,\n",
       " '×': 147,\n",
       " 'Ø': 148,\n",
       " 'Ù': 149,\n",
       " 'Ú': 150,\n",
       " 'Û': 151,\n",
       " 'Ü': 152,\n",
       " 'Ý': 153,\n",
       " 'Þ': 154,\n",
       " 'ß': 155,\n",
       " 'à': 156,\n",
       " 'á': 157,\n",
       " 'â': 158,\n",
       " 'ã': 159,\n",
       " 'ä': 160,\n",
       " 'å': 161,\n",
       " 'æ': 162,\n",
       " 'ç': 163,\n",
       " 'è': 164,\n",
       " 'é': 165,\n",
       " 'ê': 166,\n",
       " 'ë': 167,\n",
       " 'ì': 168,\n",
       " 'í': 169,\n",
       " 'î': 170,\n",
       " 'ï': 171,\n",
       " 'ð': 172,\n",
       " 'ñ': 173,\n",
       " 'ò': 174,\n",
       " 'ó': 175,\n",
       " 'ô': 176,\n",
       " 'õ': 177,\n",
       " 'ö': 178,\n",
       " '÷': 179,\n",
       " 'ø': 180,\n",
       " 'ù': 181,\n",
       " 'ú': 182,\n",
       " 'û': 183,\n",
       " 'ü': 184,\n",
       " 'ý': 185,\n",
       " 'þ': 186,\n",
       " 'ÿ': 187,\n",
       " 'Ā': 188,\n",
       " 'ā': 189,\n",
       " 'Ă': 190,\n",
       " 'ă': 191,\n",
       " 'Ą': 192,\n",
       " 'ą': 193,\n",
       " 'Ć': 194,\n",
       " 'ć': 195,\n",
       " 'Ĉ': 196,\n",
       " 'ĉ': 197,\n",
       " 'Ċ': 198,\n",
       " 'ċ': 199,\n",
       " 'Č': 200,\n",
       " 'č': 201,\n",
       " 'Ď': 202,\n",
       " 'ď': 203,\n",
       " 'Đ': 204,\n",
       " 'đ': 205,\n",
       " 'Ē': 206,\n",
       " 'ē': 207,\n",
       " 'Ĕ': 208,\n",
       " 'ĕ': 209,\n",
       " 'Ė': 210,\n",
       " 'ė': 211,\n",
       " 'Ę': 212,\n",
       " 'ę': 213,\n",
       " 'Ě': 214,\n",
       " 'ě': 215,\n",
       " 'Ĝ': 216,\n",
       " 'ĝ': 217,\n",
       " 'Ğ': 218,\n",
       " 'ğ': 219,\n",
       " 'Ġ': 220,\n",
       " 'ġ': 221,\n",
       " 'Ģ': 222,\n",
       " 'ģ': 223,\n",
       " 'Ĥ': 224,\n",
       " 'ĥ': 225,\n",
       " 'Ħ': 226,\n",
       " 'ħ': 227,\n",
       " 'Ĩ': 228,\n",
       " 'ĩ': 229,\n",
       " 'Ī': 230,\n",
       " 'ī': 231,\n",
       " 'Ĭ': 232,\n",
       " 'ĭ': 233,\n",
       " 'Į': 234,\n",
       " 'į': 235,\n",
       " 'İ': 236,\n",
       " 'ı': 237,\n",
       " 'Ĳ': 238,\n",
       " 'ĳ': 239,\n",
       " 'Ĵ': 240,\n",
       " 'ĵ': 241,\n",
       " 'Ķ': 242,\n",
       " 'ķ': 243,\n",
       " 'ĸ': 244,\n",
       " 'Ĺ': 245,\n",
       " 'ĺ': 246,\n",
       " 'Ļ': 247,\n",
       " 'ļ': 248,\n",
       " 'Ľ': 249,\n",
       " 'ľ': 250,\n",
       " 'Ŀ': 251,\n",
       " 'ŀ': 252,\n",
       " 'Ł': 253,\n",
       " 'ł': 254,\n",
       " 'Ń': 255,\n",
       " 'Ġt': 256,\n",
       " 'Ġa': 257,\n",
       " 'he': 258,\n",
       " 'in': 259,\n",
       " 're': 260,\n",
       " 'on': 261,\n",
       " 'Ġthe': 262,\n",
       " 'er': 263,\n",
       " 'Ġs': 264,\n",
       " 'at': 265,\n",
       " 'Ġw': 266,\n",
       " 'Ġo': 267,\n",
       " 'en': 268,\n",
       " 'Ġc': 269,\n",
       " 'it': 270,\n",
       " 'is': 271,\n",
       " 'an': 272,\n",
       " 'or': 273,\n",
       " 'es': 274,\n",
       " 'Ġb': 275,\n",
       " 'ed': 276,\n",
       " 'Ġf': 277,\n",
       " 'ing': 278,\n",
       " 'Ġp': 279,\n",
       " 'ou': 280,\n",
       " 'Ġan': 281,\n",
       " 'al': 282,\n",
       " 'ar': 283,\n",
       " 'Ġto': 284,\n",
       " 'Ġm': 285,\n",
       " 'Ġof': 286,\n",
       " 'Ġin': 287,\n",
       " 'Ġd': 288,\n",
       " 'Ġh': 289,\n",
       " 'Ġand': 290,\n",
       " 'ic': 291,\n",
       " 'as': 292,\n",
       " 'le': 293,\n",
       " 'Ġth': 294,\n",
       " 'ion': 295,\n",
       " 'om': 296,\n",
       " 'll': 297,\n",
       " 'ent': 298,\n",
       " 'Ġn': 299,\n",
       " 'Ġl': 300,\n",
       " 'st': 301,\n",
       " 'Ġre': 302,\n",
       " 've': 303,\n",
       " 'Ġe': 304,\n",
       " 'ro': 305,\n",
       " 'ly': 306,\n",
       " 'Ġbe': 307,\n",
       " 'Ġg': 308,\n",
       " 'ĠT': 309,\n",
       " 'ct': 310,\n",
       " 'ĠS': 311,\n",
       " 'id': 312,\n",
       " 'ot': 313,\n",
       " 'ĠI': 314,\n",
       " 'ut': 315,\n",
       " 'et': 316,\n",
       " 'ĠA': 317,\n",
       " 'Ġis': 318,\n",
       " 'Ġon': 319,\n",
       " 'im': 320,\n",
       " 'am': 321,\n",
       " 'ow': 322,\n",
       " 'ay': 323,\n",
       " 'ad': 324,\n",
       " 'se': 325,\n",
       " 'Ġthat': 326,\n",
       " 'ĠC': 327,\n",
       " 'ig': 328,\n",
       " 'Ġfor': 329,\n",
       " 'ac': 330,\n",
       " 'Ġy': 331,\n",
       " 'ver': 332,\n",
       " 'ur': 333,\n",
       " 'Ġu': 334,\n",
       " 'ld': 335,\n",
       " 'Ġst': 336,\n",
       " 'ĠM': 337,\n",
       " \"'s\": 338,\n",
       " 'Ġhe': 339,\n",
       " 'Ġit': 340,\n",
       " 'ation': 341,\n",
       " 'ith': 342,\n",
       " 'ir': 343,\n",
       " 'ce': 344,\n",
       " 'Ġyou': 345,\n",
       " 'il': 346,\n",
       " 'ĠB': 347,\n",
       " 'Ġwh': 348,\n",
       " 'ol': 349,\n",
       " 'ĠP': 350,\n",
       " 'Ġwith': 351,\n",
       " 'Ġ1': 352,\n",
       " 'ter': 353,\n",
       " 'ch': 354,\n",
       " 'Ġas': 355,\n",
       " 'Ġwe': 356,\n",
       " 'Ġ(': 357,\n",
       " 'nd': 358,\n",
       " 'ill': 359,\n",
       " 'ĠD': 360,\n",
       " 'if': 361,\n",
       " 'Ġ2': 362,\n",
       " 'ag': 363,\n",
       " 'ers': 364,\n",
       " 'ke': 365,\n",
       " 'Ġ\"': 366,\n",
       " 'ĠH': 367,\n",
       " 'em': 368,\n",
       " 'Ġcon': 369,\n",
       " 'ĠW': 370,\n",
       " 'ĠR': 371,\n",
       " 'her': 372,\n",
       " 'Ġwas': 373,\n",
       " 'Ġr': 374,\n",
       " 'od': 375,\n",
       " 'ĠF': 376,\n",
       " 'ul': 377,\n",
       " 'ate': 378,\n",
       " 'Ġat': 379,\n",
       " 'ri': 380,\n",
       " 'pp': 381,\n",
       " 'ore': 382,\n",
       " 'ĠThe': 383,\n",
       " 'Ġse': 384,\n",
       " 'us': 385,\n",
       " 'Ġpro': 386,\n",
       " 'Ġha': 387,\n",
       " 'um': 388,\n",
       " 'Ġare': 389,\n",
       " 'Ġde': 390,\n",
       " 'ain': 391,\n",
       " 'and': 392,\n",
       " 'Ġor': 393,\n",
       " 'igh': 394,\n",
       " 'est': 395,\n",
       " 'ist': 396,\n",
       " 'ab': 397,\n",
       " 'rom': 398,\n",
       " 'ĠN': 399,\n",
       " 'th': 400,\n",
       " 'Ġcom': 401,\n",
       " 'ĠG': 402,\n",
       " 'un': 403,\n",
       " 'op': 404,\n",
       " '00': 405,\n",
       " 'ĠL': 406,\n",
       " 'Ġnot': 407,\n",
       " 'ess': 408,\n",
       " 'Ġex': 409,\n",
       " 'Ġv': 410,\n",
       " 'res': 411,\n",
       " 'ĠE': 412,\n",
       " 'ew': 413,\n",
       " 'ity': 414,\n",
       " 'ant': 415,\n",
       " 'Ġby': 416,\n",
       " 'el': 417,\n",
       " 'os': 418,\n",
       " 'ort': 419,\n",
       " 'oc': 420,\n",
       " 'qu': 421,\n",
       " 'Ġfrom': 422,\n",
       " 'Ġhave': 423,\n",
       " 'Ġsu': 424,\n",
       " 'ive': 425,\n",
       " 'ould': 426,\n",
       " 'Ġsh': 427,\n",
       " 'Ġthis': 428,\n",
       " 'nt': 429,\n",
       " 'ra': 430,\n",
       " 'pe': 431,\n",
       " 'ight': 432,\n",
       " 'art': 433,\n",
       " 'ment': 434,\n",
       " 'Ġal': 435,\n",
       " 'ust': 436,\n",
       " 'end': 437,\n",
       " '--': 438,\n",
       " 'all': 439,\n",
       " 'ĠO': 440,\n",
       " 'ack': 441,\n",
       " 'Ġch': 442,\n",
       " 'Ġle': 443,\n",
       " 'ies': 444,\n",
       " 'red': 445,\n",
       " 'ard': 446,\n",
       " 'âĢ': 447,\n",
       " 'out': 448,\n",
       " 'ĠJ': 449,\n",
       " 'Ġab': 450,\n",
       " 'ear': 451,\n",
       " 'iv': 452,\n",
       " 'ally': 453,\n",
       " 'our': 454,\n",
       " 'ost': 455,\n",
       " 'gh': 456,\n",
       " 'pt': 457,\n",
       " 'Ġpl': 458,\n",
       " 'ast': 459,\n",
       " 'Ġcan': 460,\n",
       " 'ak': 461,\n",
       " 'ome': 462,\n",
       " 'ud': 463,\n",
       " 'The': 464,\n",
       " 'Ġhis': 465,\n",
       " 'Ġdo': 466,\n",
       " 'Ġgo': 467,\n",
       " 'Ġhas': 468,\n",
       " 'ge': 469,\n",
       " \"'t\": 470,\n",
       " 'ĠU': 471,\n",
       " 'rou': 472,\n",
       " 'Ġsa': 473,\n",
       " 'Ġj': 474,\n",
       " 'Ġbut': 475,\n",
       " 'Ġwor': 476,\n",
       " 'Ġall': 477,\n",
       " 'ect': 478,\n",
       " 'Ġk': 479,\n",
       " 'ame': 480,\n",
       " 'Ġwill': 481,\n",
       " 'ok': 482,\n",
       " 'Ġwhe': 483,\n",
       " 'Ġthey': 484,\n",
       " 'ide': 485,\n",
       " '01': 486,\n",
       " 'ff': 487,\n",
       " 'ich': 488,\n",
       " 'pl': 489,\n",
       " 'ther': 490,\n",
       " 'Ġtr': 491,\n",
       " '..': 492,\n",
       " 'Ġint': 493,\n",
       " 'ie': 494,\n",
       " 'ure': 495,\n",
       " 'age': 496,\n",
       " 'Ġne': 497,\n",
       " 'ial': 498,\n",
       " 'ap': 499,\n",
       " 'ine': 500,\n",
       " 'ice': 501,\n",
       " 'Ġme': 502,\n",
       " 'Ġout': 503,\n",
       " 'ans': 504,\n",
       " 'one': 505,\n",
       " 'ong': 506,\n",
       " 'ions': 507,\n",
       " 'Ġwho': 508,\n",
       " 'ĠK': 509,\n",
       " 'Ġup': 510,\n",
       " 'Ġtheir': 511,\n",
       " 'Ġad': 512,\n",
       " 'Ġ3': 513,\n",
       " 'Ġus': 514,\n",
       " 'ated': 515,\n",
       " 'ous': 516,\n",
       " 'Ġmore': 517,\n",
       " 'ue': 518,\n",
       " 'og': 519,\n",
       " 'ĠSt': 520,\n",
       " 'ind': 521,\n",
       " 'ike': 522,\n",
       " 'Ġso': 523,\n",
       " 'ime': 524,\n",
       " 'per': 525,\n",
       " '.\"': 526,\n",
       " 'ber': 527,\n",
       " 'iz': 528,\n",
       " 'act': 529,\n",
       " 'Ġone': 530,\n",
       " 'Ġsaid': 531,\n",
       " 'Ġ-': 532,\n",
       " 'are': 533,\n",
       " 'Ġyour': 534,\n",
       " 'cc': 535,\n",
       " 'ĠTh': 536,\n",
       " 'Ġcl': 537,\n",
       " 'ep': 538,\n",
       " 'ake': 539,\n",
       " 'able': 540,\n",
       " 'ip': 541,\n",
       " 'Ġcont': 542,\n",
       " 'Ġwhich': 543,\n",
       " 'ia': 544,\n",
       " 'Ġim': 545,\n",
       " 'Ġabout': 546,\n",
       " 'Ġwere': 547,\n",
       " 'very': 548,\n",
       " 'ub': 549,\n",
       " 'Ġhad': 550,\n",
       " 'Ġen': 551,\n",
       " 'Ġcomp': 552,\n",
       " ',\"': 553,\n",
       " 'ĠIn': 554,\n",
       " 'Ġun': 555,\n",
       " 'Ġag': 556,\n",
       " 'ire': 557,\n",
       " 'ace': 558,\n",
       " 'au': 559,\n",
       " 'ary': 560,\n",
       " 'Ġwould': 561,\n",
       " 'ass': 562,\n",
       " 'ry': 563,\n",
       " 'ĠâĢ': 564,\n",
       " 'cl': 565,\n",
       " 'ook': 566,\n",
       " 'ere': 567,\n",
       " 'so': 568,\n",
       " 'ĠV': 569,\n",
       " 'ign': 570,\n",
       " 'ib': 571,\n",
       " 'Ġoff': 572,\n",
       " 'Ġte': 573,\n",
       " 'ven': 574,\n",
       " 'ĠY': 575,\n",
       " 'ile': 576,\n",
       " 'ose': 577,\n",
       " 'ite': 578,\n",
       " 'orm': 579,\n",
       " 'Ġ201': 580,\n",
       " 'Ġres': 581,\n",
       " 'Ġman': 582,\n",
       " 'Ġper': 583,\n",
       " 'Ġother': 584,\n",
       " 'ord': 585,\n",
       " 'ult': 586,\n",
       " 'Ġbeen': 587,\n",
       " 'Ġlike': 588,\n",
       " 'ase': 589,\n",
       " 'ance': 590,\n",
       " 'ks': 591,\n",
       " 'ays': 592,\n",
       " 'own': 593,\n",
       " 'ence': 594,\n",
       " 'Ġdis': 595,\n",
       " 'ction': 596,\n",
       " 'Ġany': 597,\n",
       " 'Ġapp': 598,\n",
       " 'Ġsp': 599,\n",
       " 'int': 600,\n",
       " 'ress': 601,\n",
       " 'ations': 602,\n",
       " 'ail': 603,\n",
       " 'Ġ4': 604,\n",
       " 'ical': 605,\n",
       " 'Ġthem': 606,\n",
       " 'Ġher': 607,\n",
       " 'ount': 608,\n",
       " 'ĠCh': 609,\n",
       " 'Ġar': 610,\n",
       " 'Ġif': 611,\n",
       " 'Ġthere': 612,\n",
       " 'Ġpe': 613,\n",
       " 'Ġyear': 614,\n",
       " 'av': 615,\n",
       " 'Ġmy': 616,\n",
       " 'Ġsome': 617,\n",
       " 'Ġwhen': 618,\n",
       " 'ough': 619,\n",
       " 'ach': 620,\n",
       " 'Ġthan': 621,\n",
       " 'ru': 622,\n",
       " 'ond': 623,\n",
       " 'ick': 624,\n",
       " 'Ġover': 625,\n",
       " 'vel': 626,\n",
       " 'Ġqu': 627,\n",
       " 'ĊĊ': 628,\n",
       " 'Ġsc': 629,\n",
       " 'reat': 630,\n",
       " 'ree': 631,\n",
       " 'ĠIt': 632,\n",
       " 'ound': 633,\n",
       " 'port': 634,\n",
       " 'Ġalso': 635,\n",
       " 'Ġpart': 636,\n",
       " 'fter': 637,\n",
       " 'Ġkn': 638,\n",
       " 'Ġbec': 639,\n",
       " 'Ġtime': 640,\n",
       " 'ens': 641,\n",
       " 'Ġ5': 642,\n",
       " 'ople': 643,\n",
       " 'Ġwhat': 644,\n",
       " 'Ġno': 645,\n",
       " 'du': 646,\n",
       " 'mer': 647,\n",
       " 'ang': 648,\n",
       " 'Ġnew': 649,\n",
       " '----': 650,\n",
       " 'Ġget': 651,\n",
       " 'ory': 652,\n",
       " 'ition': 653,\n",
       " 'ings': 654,\n",
       " 'Ġjust': 655,\n",
       " 'Ġinto': 656,\n",
       " 'Ġ0': 657,\n",
       " 'ents': 658,\n",
       " 'ove': 659,\n",
       " 'te': 660,\n",
       " 'Ġpeople': 661,\n",
       " 'Ġpre': 662,\n",
       " 'Ġits': 663,\n",
       " 'Ġrec': 664,\n",
       " 'Ġtw': 665,\n",
       " 'ian': 666,\n",
       " 'irst': 667,\n",
       " 'ark': 668,\n",
       " 'ors': 669,\n",
       " 'Ġwork': 670,\n",
       " 'ade': 671,\n",
       " 'ob': 672,\n",
       " 'Ġshe': 673,\n",
       " 'Ġour': 674,\n",
       " 'wn': 675,\n",
       " 'ink': 676,\n",
       " 'lic': 677,\n",
       " 'Ġ19': 678,\n",
       " 'ĠHe': 679,\n",
       " 'ish': 680,\n",
       " 'nder': 681,\n",
       " 'ause': 682,\n",
       " 'Ġhim': 683,\n",
       " 'ons': 684,\n",
       " 'Ġ[': 685,\n",
       " 'Ġro': 686,\n",
       " 'form': 687,\n",
       " 'ild': 688,\n",
       " 'ates': 689,\n",
       " 'vers': 690,\n",
       " 'Ġonly': 691,\n",
       " 'oll': 692,\n",
       " 'Ġspe': 693,\n",
       " 'ck': 694,\n",
       " 'ell': 695,\n",
       " 'amp': 696,\n",
       " 'Ġacc': 697,\n",
       " 'Ġbl': 698,\n",
       " 'ious': 699,\n",
       " 'urn': 700,\n",
       " 'ft': 701,\n",
       " 'ood': 702,\n",
       " 'Ġhow': 703,\n",
       " 'hed': 704,\n",
       " \"Ġ'\": 705,\n",
       " 'Ġafter': 706,\n",
       " 'aw': 707,\n",
       " 'Ġatt': 708,\n",
       " 'ov': 709,\n",
       " 'ne': 710,\n",
       " 'Ġplay': 711,\n",
       " 'erv': 712,\n",
       " 'ict': 713,\n",
       " 'Ġcould': 714,\n",
       " 'itt': 715,\n",
       " 'Ġam': 716,\n",
       " 'Ġfirst': 717,\n",
       " 'Ġ6': 718,\n",
       " 'Ġact': 719,\n",
       " 'Ġ$': 720,\n",
       " 'ec': 721,\n",
       " 'hing': 722,\n",
       " 'ual': 723,\n",
       " 'ull': 724,\n",
       " 'Ġcomm': 725,\n",
       " 'oy': 726,\n",
       " 'old': 727,\n",
       " 'ces': 728,\n",
       " 'ater': 729,\n",
       " 'Ġfe': 730,\n",
       " 'Ġbet': 731,\n",
       " 'we': 732,\n",
       " 'iff': 733,\n",
       " 'Ġtwo': 734,\n",
       " 'ock': 735,\n",
       " 'Ġback': 736,\n",
       " ').': 737,\n",
       " 'ident': 738,\n",
       " 'Ġunder': 739,\n",
       " 'rough': 740,\n",
       " 'sel': 741,\n",
       " 'xt': 742,\n",
       " 'Ġmay': 743,\n",
       " 'round': 744,\n",
       " 'Ġpo': 745,\n",
       " 'ph': 746,\n",
       " 'iss': 747,\n",
       " 'Ġdes': 748,\n",
       " 'Ġmost': 749,\n",
       " 'Ġdid': 750,\n",
       " 'Ġadd': 751,\n",
       " 'ject': 752,\n",
       " 'Ġinc': 753,\n",
       " 'fore': 754,\n",
       " 'Ġpol': 755,\n",
       " 'ont': 756,\n",
       " 'Ġagain': 757,\n",
       " 'clud': 758,\n",
       " 'tern': 759,\n",
       " 'Ġknow': 760,\n",
       " 'Ġneed': 761,\n",
       " 'Ġcons': 762,\n",
       " 'Ġco': 763,\n",
       " 'Ġ.': 764,\n",
       " 'Ġwant': 765,\n",
       " 'Ġsee': 766,\n",
       " 'Ġ7': 767,\n",
       " 'ning': 768,\n",
       " 'iew': 769,\n",
       " 'ĠThis': 770,\n",
       " 'ced': 771,\n",
       " 'Ġeven': 772,\n",
       " 'Ġind': 773,\n",
       " 'ty': 774,\n",
       " 'ĠWe': 775,\n",
       " 'ath': 776,\n",
       " 'Ġthese': 777,\n",
       " 'Ġpr': 778,\n",
       " 'Ġuse': 779,\n",
       " 'Ġbecause': 780,\n",
       " 'Ġfl': 781,\n",
       " 'ng': 782,\n",
       " 'Ġnow': 783,\n",
       " 'ĠâĢĵ': 784,\n",
       " 'com': 785,\n",
       " 'ise': 786,\n",
       " 'Ġmake': 787,\n",
       " 'Ġthen': 788,\n",
       " 'ower': 789,\n",
       " 'Ġevery': 790,\n",
       " 'ĠUn': 791,\n",
       " 'Ġsec': 792,\n",
       " 'oss': 793,\n",
       " 'uch': 794,\n",
       " 'Ġem': 795,\n",
       " 'Ġ=': 796,\n",
       " 'ĠRe': 797,\n",
       " 'ied': 798,\n",
       " 'rit': 799,\n",
       " 'Ġinv': 800,\n",
       " 'lect': 801,\n",
       " 'Ġsupp': 802,\n",
       " 'ating': 803,\n",
       " 'Ġlook': 804,\n",
       " 'man': 805,\n",
       " 'pect': 806,\n",
       " 'Ġ8': 807,\n",
       " 'row': 808,\n",
       " 'Ġbu': 809,\n",
       " 'Ġwhere': 810,\n",
       " 'ific': 811,\n",
       " 'Ġyears': 812,\n",
       " 'ily': 813,\n",
       " 'Ġdiff': 814,\n",
       " 'Ġshould': 815,\n",
       " 'Ġrem': 816,\n",
       " 'Th': 817,\n",
       " 'In': 818,\n",
       " 'Ġev': 819,\n",
       " 'day': 820,\n",
       " \"'re\": 821,\n",
       " 'rib': 822,\n",
       " 'Ġrel': 823,\n",
       " 'ss': 824,\n",
       " 'Ġdef': 825,\n",
       " 'Ġright': 826,\n",
       " 'Ġsy': 827,\n",
       " '),': 828,\n",
       " 'les': 829,\n",
       " '000': 830,\n",
       " 'hen': 831,\n",
       " 'Ġthrough': 832,\n",
       " 'ĠTr': 833,\n",
       " '__': 834,\n",
       " 'Ġway': 835,\n",
       " 'Ġdon': 836,\n",
       " 'Ġ,': 837,\n",
       " 'Ġ10': 838,\n",
       " 'ased': 839,\n",
       " 'Ġass': 840,\n",
       " 'ublic': 841,\n",
       " 'Ġreg': 842,\n",
       " 'ĠAnd': 843,\n",
       " 'ix': 844,\n",
       " 'Ġvery': 845,\n",
       " 'Ġinclud': 846,\n",
       " 'other': 847,\n",
       " 'Ġimp': 848,\n",
       " 'oth': 849,\n",
       " 'Ġsub': 850,\n",
       " 'ĠâĢĶ': 851,\n",
       " 'Ġbeing': 852,\n",
       " 'arg': 853,\n",
       " 'ĠWh': 854,\n",
       " '==': 855,\n",
       " 'ible': 856,\n",
       " 'Ġdoes': 857,\n",
       " 'ange': 858,\n",
       " 'ram': 859,\n",
       " 'Ġ9': 860,\n",
       " 'ert': 861,\n",
       " 'ps': 862,\n",
       " 'ited': 863,\n",
       " 'ational': 864,\n",
       " 'Ġbr': 865,\n",
       " 'Ġdown': 866,\n",
       " 'Ġmany': 867,\n",
       " 'aking': 868,\n",
       " 'Ġcall': 869,\n",
       " 'uring': 870,\n",
       " 'ities': 871,\n",
       " 'Ġph': 872,\n",
       " 'ics': 873,\n",
       " 'als': 874,\n",
       " 'Ġdec': 875,\n",
       " 'ative': 876,\n",
       " 'ener': 877,\n",
       " 'Ġbefore': 878,\n",
       " 'ility': 879,\n",
       " 'Ġwell': 880,\n",
       " 'Ġmuch': 881,\n",
       " 'erson': 882,\n",
       " 'Ġthose': 883,\n",
       " 'Ġsuch': 884,\n",
       " 'Ġke': 885,\n",
       " 'Ġend': 886,\n",
       " 'ĠBut': 887,\n",
       " 'ason': 888,\n",
       " 'ting': 889,\n",
       " 'Ġlong': 890,\n",
       " 'ef': 891,\n",
       " 'Ġthink': 892,\n",
       " 'ys': 893,\n",
       " 'Ġbel': 894,\n",
       " 'Ġsm': 895,\n",
       " 'its': 896,\n",
       " 'ax': 897,\n",
       " 'Ġown': 898,\n",
       " 'Ġprov': 899,\n",
       " 'Ġset': 900,\n",
       " 'ife': 901,\n",
       " 'ments': 902,\n",
       " 'ble': 903,\n",
       " 'ward': 904,\n",
       " 'Ġshow': 905,\n",
       " 'Ġpres': 906,\n",
       " 'ms': 907,\n",
       " 'omet': 908,\n",
       " 'Ġob': 909,\n",
       " 'Ġsay': 910,\n",
       " 'ĠSh': 911,\n",
       " 'ts': 912,\n",
       " 'ful': 913,\n",
       " 'Ġeff': 914,\n",
       " 'Ġgu': 915,\n",
       " 'Ġinst': 916,\n",
       " 'und': 917,\n",
       " 'ren': 918,\n",
       " 'cess': 919,\n",
       " 'Ġent': 920,\n",
       " 'ĠYou': 921,\n",
       " 'Ġgood': 922,\n",
       " 'Ġstart': 923,\n",
       " 'ince': 924,\n",
       " 'Ġmade': 925,\n",
       " 'tt': 926,\n",
       " 'stem': 927,\n",
       " 'olog': 928,\n",
       " 'up': 929,\n",
       " 'Ġ|': 930,\n",
       " 'ump': 931,\n",
       " 'Ġhel': 932,\n",
       " 'vern': 933,\n",
       " 'ular': 934,\n",
       " 'ually': 935,\n",
       " 'Ġac': 936,\n",
       " 'Ġmon': 937,\n",
       " 'Ġlast': 938,\n",
       " 'Ġ200': 939,\n",
       " '10': 940,\n",
       " 'Ġstud': 941,\n",
       " 'ures': 942,\n",
       " 'ĠAr': 943,\n",
       " 'self': 944,\n",
       " 'ars': 945,\n",
       " 'meric': 946,\n",
       " 'ues': 947,\n",
       " 'cy': 948,\n",
       " 'Ġmin': 949,\n",
       " 'ollow': 950,\n",
       " 'Ġcol': 951,\n",
       " 'io': 952,\n",
       " 'Ġmod': 953,\n",
       " 'Ġcount': 954,\n",
       " 'ĠCom': 955,\n",
       " 'hes': 956,\n",
       " 'Ġfin': 957,\n",
       " 'air': 958,\n",
       " 'ier': 959,\n",
       " 'âĢĶ': 960,\n",
       " 'read': 961,\n",
       " 'ank': 962,\n",
       " 'atch': 963,\n",
       " 'ever': 964,\n",
       " 'Ġstr': 965,\n",
       " 'Ġpoint': 966,\n",
       " 'ork': 967,\n",
       " 'ĠNew': 968,\n",
       " 'Ġsur': 969,\n",
       " 'ool': 970,\n",
       " 'alk': 971,\n",
       " 'ement': 972,\n",
       " 'Ġused': 973,\n",
       " 'ract': 974,\n",
       " 'ween': 975,\n",
       " 'Ġsame': 976,\n",
       " 'oun': 977,\n",
       " 'ĠAl': 978,\n",
       " 'ci': 979,\n",
       " 'Ġdiffere': 980,\n",
       " 'Ġwhile': 981,\n",
       " '--------': 982,\n",
       " 'Ġgame': 983,\n",
       " 'cept': 984,\n",
       " 'Ġsim': 985,\n",
       " '...': 986,\n",
       " 'Ġinter': 987,\n",
       " 'ek': 988,\n",
       " 'Ġreport': 989,\n",
       " 'Ġprodu': 990,\n",
       " 'Ġstill': 991,\n",
       " 'led': 992,\n",
       " 'ah': 993,\n",
       " 'Ġhere': 994,\n",
       " 'Ġworld': 995,\n",
       " 'Ġthough': 996,\n",
       " 'Ġnum': 997,\n",
       " 'arch': 998,\n",
       " 'imes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3dbca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # very last, special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e0df5",
   "metadata": {},
   "source": [
    "^used to delimit documents in the training set. So we insert this between the text to signal that one doc has ended and what follows is not related. Ofc, the meaning of this signal must be _learnt_ by the LM; we are just giving a hint to increase the quality of data. \n",
    "\n",
    "So the special token `<|endoftext|>` is immune to merges. This is visible in the tiktoken/src/lib.rs (implemented in rust) where exceptions are made for special tokens. \n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">NOTES:</span>\n",
    "\n",
    "- What we are dealing with so far are base model encoder/decoders. But in the fine tuned models (such as gpt turbo 3.5, even 4o, 4o-mini are somewhat finetuned), there are many more special tokens to delimit `system prompt`, `user prompt` etc: \n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"images/special_tokens.png\" width = 60%>\n",
    "\n",
    "- `<|im_start|>` , `<|im_end|>` are other special tokens. 'im' in <|im_start|> stands for _imaginary monologue_!\n",
    "\n",
    "- You can add your own special tokens as well, tiktoken has the provision for that. Scroll down through [readme.md](https://github.com/openai/tiktoken) in the \"Extending tiktoken\" section.\n",
    "\n",
    "- As we can see [here](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py) __gpt2__ has only a single special token `{ENDOFTEXT: 50256}`, whereas __gpt4__ has many: `ENDOFTEXT`, `FIM_PREFIX`, `FIM_MIDDLE`, `FIM_SUFFIX`, `ENDOFPROMPT`   <br>\n",
    "\n",
    "FIM stands for Fill in the middle and is introduced in [this paper](https://arxiv.org/pdf/2207.14255) by OPENAI.\n",
    "\n",
    "- Once these special tokens are coined, the _model parameters change_ a bit. The embedding table needs an extra row, the intermediate layers change in dimensionsion by 1, output layer also becomes +1 in size. \n",
    "\n",
    "<hr>\n",
    "\n",
    "## sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece)\n",
    "\n",
    "The big difference: __sentencepiece runs BPE on the Unicode code points directly__! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">TLDR:</span>\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
    "\n",
    "\n",
    "[I have compiled examples on how these both operate from first principles here](https://www.notion.so/Sentencepiece-vs-tiktoken-tokenizer-25d6be0e11f1803ea60cc4745cc65f6f?source=copy_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fcb890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89b111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1492202",
   "metadata": {},
   "source": [
    "^ serves as the training vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fafbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a69520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['▁s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['▁m', 271],\n",
       " ['▁u', 272],\n",
       " ['ing', 273],\n",
       " ['▁th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['▁S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['▁an', 290],\n",
       " ['▁pr', 291],\n",
       " ['▁to', 292],\n",
       " ['▁un', 293],\n",
       " ['▁the', 294],\n",
       " ['Piece', 295],\n",
       " ['▁Sentence', 296],\n",
       " ['▁SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['▁(', 309],\n",
       " ['▁[', 310],\n",
       " ['▁f', 311],\n",
       " ['▁n', 312],\n",
       " ['▁w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['▁Ne', 323],\n",
       " ['▁al', 324],\n",
       " ['▁de', 325],\n",
       " ['▁is', 326],\n",
       " ['▁ma', 327],\n",
       " ['▁mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['▁and', 332],\n",
       " ['▁lan', 333],\n",
       " ['▁pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['▁text', 337],\n",
       " ['▁model', 338],\n",
       " ['▁train', 339],\n",
       " ['kenizer', 340],\n",
       " ['▁system', 341],\n",
       " ['▁language', 342],\n",
       " ['▁training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab43c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044c86e",
   "metadata": {},
   "source": [
    "We have set the hyperparameter `byte_fallback = True`, which is why `안녕하세요`, which is not encountered in the tranining data is represented in its utf-8 byte representation and doesnt give an error\n",
    "\n",
    "__If you set `byte_fallback = false`:__  <br>\n",
    "hello is encoded but `안녕하세요` encodes to 0 or unk - unknown token. The vocab also wont have the bytes up to 255 in such case and will be limited to tokens seen in the training set only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea48b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'e', 'l', 'lo', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "# decoding\n",
    "\n",
    "print([sp.id_to_piece(idx) for idx in ids]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d8d41",
   "metadata": {},
   "source": [
    "__Decoding quirk:__\n",
    "\n",
    "As you can see, during decoding '▁' is appended. This is because we set the hyperparam `add_dummy_prefix = True`. This trailing space serves the following function: \n",
    "\n",
    "- Sentence 1: \"`hello` world\"\n",
    "- Sentence 2: \"Say` hello` to your uncle\"\n",
    "\n",
    "Ideally you want both hellos to be processed in the same way. To uphold this semantic accuracy, a dummy white space is introduced. You can set `add_dummy_prefix = False`if you want 'hello' and ' hello' to be treated as separate tokens. \n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## Vocab size\n",
    "\n",
    "- Its mostly an empirical hyperparameter\n",
    "- Where does vocab size come up really? $\\rightarrow$ _Embedding table, final linear layer of the NN_\n",
    "- If `vocab_size` is too high: \n",
    "    - the probabilities of them occuring is scant and the NN may be undertrained (since the specific vocab element just doesnt occur often enough)\n",
    "    - but this also means we can pack more text into the transformer block, at the risk of compressing _too much_\n",
    "- If `vocab_size` is too less: \n",
    "    - Transformer block attends to less information and text size balloons\n",
    "    - Not efficient utilisation of text structures\n",
    "\n",
    "- In modern SOTA models vocab_size in 100k-150k range is seen (2025)\n",
    "\n",
    "- __Extending vocab__: You may wish to add custom tokens as per use case and this is fairly commonly done. It is done by freezing the base model vocab and adding these custom tokens and then _only_ training them. \n",
    "    - You may wish to extend to [compress prompt](https://arxiv.org/pdf/2304.08467) and other such innovative applications too. \n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Addressing anomaly behaviors observed in LMs due to tokenization \n",
    "\n",
    "- Just watch the next 20 minutes of [this video](https://youtu.be/zduSFxRajkE?si=Qnnvdbds1x0oA4bM&t=6701) of Andrej breaking LLMs, it too hilarious!\n",
    "- [SolidGoldMagikarp](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de07a00",
   "metadata": {},
   "source": [
    "## Final recommendations \n",
    "by Andrej Karpathy\n",
    "\n",
    "- Don't brush off tokenization. A lot of footguns and sharp edges here. Security issues. Safety issues.\n",
    "- Eternal glory to anyone who can delete tokenization as a required step in LLMs.\n",
    "- In your own application:\n",
    "    - Maybe you can just re-use the GPT-4 tokens and tiktoken?\n",
    "    - If you're training a vocab, ok to use BPE with sentencepiece. Careful with the million settings.\n",
    "    - Switch to minbpe once it is as efficient as sentencepiece :)\n",
    "\n",
    "__Also worth looking at:__<br>\n",
    "\n",
    "[Huggingface Tokenizer](https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Ftokenizer). I didn't cover it in detail in the lecture because the algorithm (to my knowledge) is very similar to sentencepiece, but worth potentially evaluating for use in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
