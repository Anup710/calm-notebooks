Correct order to follow: 

1. Micrograd (a high level implementation of autograd engine in pytorch)
2. Makemore (Bigram -> MLP -> MLP2 -> MLP3 backprop)
3. Wavenet (a special bigram implementation grouping 2 adjecent tokens in every subsequent layer)
4. chatgpt-transformers (self attention from scratch on minishakespeare model)
5. tokenizer (understanding tokens in detail)
6. GPT-2 (replicating chatgpt-2 124M parameter model)
