{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11130028",
   "metadata": {},
   "source": [
    "## MLP for language modelling\n",
    "\n",
    "Implementing Bengio et al paper to develop an MLP for language modelling. \n",
    "\n",
    "It introduces the idea of vector embeddings to capture semantic proximity, instead of explicitly calculating probabilities for each possible combination of words which wouldn't generalize well. __17,000 words__ are considered in the dataset. \n",
    "\n",
    "### Architecture\n",
    "\n",
    "<img src=\"../papers/architecture.png\" style=\"width:70%;\">\n",
    "\n",
    "__Explanation of architecture:__ \n",
    "- 3 previous words are used as context and indexed as $w_i$\n",
    "- An embedding of that word is shared from a global matrix $C$ and used as input for the hidden layer. \n",
    "- Size of hidden layer is a hyperparameter\n",
    "- post which `tanh` non-linearity is applied\n",
    "- finally there is a fully connected output layer (with __17,000 neurons -- one for each word__)\n",
    "- softmax is applied to choose the most likely word\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- The lookup table $C$ (embedding matrix)\n",
    "- $W_i, b_i$ for hidden layer\n",
    "- $W_i, b_i$ for output layer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368efe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e0148",
   "metadata": {},
   "source": [
    "We will implement the same architecture above not for sentences as is done in Bengio et al, but for individual names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060fd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995d06da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb077850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stoi \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc779bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some = [0]*3\n",
    "some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8604a75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212732ad",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "\n",
    "Use 3 previous letter to guess the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df7ebca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -------> e\n",
      "..e -------> m\n",
      ".em -------> m\n",
      "emm -------> a\n",
      "mma -------> .\n",
      "olivia\n",
      "... -------> o\n",
      "..o -------> l\n",
      ".ol -------> i\n",
      "oli -------> v\n",
      "liv -------> i\n",
      "ivi -------> a\n",
      "via -------> .\n",
      "ava\n",
      "... -------> a\n",
      "..a -------> v\n",
      ".av -------> a\n",
      "ava -------> .\n"
     ]
    }
   ],
   "source": [
    "X , Y = [], []\n",
    "block_size= 3 # can be reset to whatever you like\n",
    "\n",
    "for w in words[:3]:\n",
    "    #'emma'\n",
    "    print(w)\n",
    "    context = [0]*block_size # contains indcies of context letters\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # 'e' -> 5\n",
    "        Y.append(ix) # 5 is the target\n",
    "        X.append(context)\n",
    "        print(\"\".join(itos[i] for i in context), '------->', ch)\n",
    "        context = context[1:] + [ix] # update context and append new index\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f5300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13]]) tensor([ 5, 13, 13])\n",
      "torch.Size([16, 3]) torch.int64 torch.Size([16]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X[:3], Y[:3])\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247483",
   "metadata": {},
   "source": [
    "So we have X with 3 (integer) features as out input, Y is a scalar (integer) output. \n",
    "\n",
    "Now lets build the embedding loop table $C$:\n",
    "- We have $27$ possible characters, which we will try to embed into a lower dimension space (unlike one-hot encoding, which is still 27 dimensional!)\n",
    "- In the _paper_ they compress $17000$ words to $30$ dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50bc905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3508, -0.8780],\n",
       "        [-0.6882,  0.7677],\n",
       "        [-0.0113, -1.4200]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27,2)) # each of 27 characters has a 2D embedding\n",
    "\n",
    "C[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59ef99",
   "metadata": {},
   "source": [
    "Now how to acess the embedding for a single integer, say $5$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954b7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7233, -0.6068])\n",
      "tensor([ 1.7233, -0.6068])\n"
     ]
    }
   ],
   "source": [
    "# option 1: index into C directly\n",
    "print(C[5])\n",
    "\n",
    "# option 2: one-hot encode 5 and then multiply -- as was done in bigram \n",
    "print(F.one_hot(torch.tensor(5), num_classes=27).float() @ C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f204f14",
   "metadata": {},
   "source": [
    "Introducting `.float()` is ajust an occupational hazard. Note that both of the above way give the same embedding tensor for $5$. \n",
    "\n",
    "Going forward we will just extract the row directly using the index. \n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Question:</span>: Now how to we convert X: 16*3 into embeddings? We must leverage pytorch indexing flexibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f29a771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3538,  0.7665],\n",
       "        [-1.5971, -1.9288],\n",
       "        [ 0.4243,  0.1791],\n",
       "        [ 0.4243,  0.1791],\n",
       "        [ 0.4243,  0.1791]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,13,4,4,4]]  # retrieves 5th, 13th and 4th row of C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc502d",
   "metadata": {},
   "source": [
    "We indexed with 1 dimensional tensor of integers. But turns out we can also index with 2 dimensional tensor of integers. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321c05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ead244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3508, -0.8780],\n",
       "          [-1.3508, -0.8780],\n",
       "          [-1.3508, -0.8780]],\n",
       " \n",
       "         [[-1.3508, -0.8780],\n",
       "          [-1.3508, -0.8780],\n",
       "          [ 1.7233, -0.6068]]]),\n",
       " torch.Size([16, 3, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:2], emb.shape # dim(X) = 16*3 and each element has 2 dim embedding => 16*3*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1f036",
   "metadata": {},
   "source": [
    "More experimentation on higher dimension tensor indexing in the data structures notebook!\n",
    "\n",
    "## Hidden layer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6444b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 previous characters, with 2D embedding each => 6 features and each neuron has a weight corresponsing for a feature\n",
    "# 100 neurons -- hyperparameter\n",
    "W1 = torch.randn((6,100)) \n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab102479",
   "metadata": {},
   "source": [
    "Ideally we want to do something like: `emb @ W1 + b`, but dimensions are not compatible for direct operation. \n",
    "\n",
    "We need to concatinate the 3 characters to create `emb: (16,6)`, compatible with `W1:(6,100)`\n",
    "\n",
    "so we must transform emb: 16,3,2 -> 16,6 ; \n",
    "using [torch.cat()](https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e049de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 2])\n",
      "torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape)\n",
    "emb_concat = torch.cat([emb[:,0,:], emb[:,1,:],emb[:,2,:]], dim=1) # what s sly way to do it. \n",
    "print(emb_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc4077",
   "metadata": {},
   "source": [
    "But this doesnt generalize well since we have _hardcoded_ $0,1,2$ indices; instead we will use [torch.unbind](https://docs.pytorch.org/docs/stable/generated/torch.unbind.html#torch-unbind)\n",
    "\n",
    "1. So we first unbind along axis 1: <br>\n",
    "torch.Size([16, 3, 2]) $\\rightarrow$ tuple(torch.Size([16, 2]),torch.Size([16, 2]),torch.Size([16, 2]))\n",
    "\n",
    "2. Concat along dim 1 <br>\n",
    "tuple $\\rightarrow$ torch.Size([16, 6])\n",
    "\n",
    "__Note that this result is not hardcoded and if context length is changed, this code this words!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74fefbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat(torch.unbind(emb, dim = 1), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbd22b",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">Note of storage efficiency:</span>\n",
    "\n",
    "But a much more memory efficient way to do this dimension manipulation is through torch.view() which simply recasts the original tensor in a new one. \n",
    "\n",
    "This is efficient because the tensor (say t1), is stored in the same way and only _represented_ differently when .view() is called! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cbff606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = emb.view(16,6)\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75d370",
   "metadata": {},
   "source": [
    "^which is essentially the same as unbinding and conactinating, which is costly in terms of memory and speed. \n",
    "\n",
    "### Hidden layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32bb3568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h = emb.view(16,-1) @ W1 + b1\n",
    "h = emb.view(emb.shape[0], -1) @ W1 + b1 # to avoid hard coding\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "398367c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying activation to get output of hidden layer\n",
    "H = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60305aac",
   "metadata": {},
   "source": [
    "## Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46bf1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27)) # 100 inputs features from H, 27 possible character outputs = no of neurons\n",
    "b2 = torch.randn(27) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = H @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db742e",
   "metadata": {},
   "source": [
    "now we normalize logits using softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b453505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim = 1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810da217",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221802de",
   "metadata": {},
   "source": [
    "Now we want to index into prob at the correct label and find the prob assigned to it by the NN. \n",
    "\n",
    "i.e. in the below labels set $Y$, we see row 0 in prob at index 5. Take log negative of it to find loss. Do this over all rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38e806a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3b91e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1041e-04, 1.1634e-06, 2.0192e-18, 1.5043e-10, 1.0573e-10, 4.8019e-06,\n",
       "        6.4802e-15, 3.5613e-01, 4.4052e-12, 6.4136e-05, 3.0762e-05, 5.9196e-10,\n",
       "        8.9206e-07, 3.2348e-10, 2.7067e-12, 3.1208e-05])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(emb.shape[0]), Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a365c",
   "metadata": {},
   "source": [
    "Prob assigned to correct labels are very low. So loss is expected to be high! (take mean, not sum!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63d49bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3391)\n"
     ]
    }
   ],
   "source": [
    "loss = -probs[torch.arange(emb.shape[0]), Y].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850992ae",
   "metadata": {},
   "source": [
    "Let's make the forward pass to calculate NLL loss, more coherent in the next notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
