{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f855539",
   "metadata": {},
   "source": [
    "# Bigram using NN\n",
    "\n",
    "As seen previously, we defined the negative log likelihood (NLL) loss function; so now using backprop lets train a NN to make a better bigram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61ccee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad616df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f8865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stoi \n",
    "stoi = {}\n",
    "allletters = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(allletters)}\n",
    "stoi['.'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c6aea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc6c1de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training dataset of bigrams\n",
    "# input x (first char) -> output y (second char): bigram is ready\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ed1c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from char1 to char2\n",
    "xs,ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d15dd",
   "metadata": {},
   "source": [
    "### Intuition for training\n",
    "\n",
    "The prized question now is: how do we feed in this data into the NN?\n",
    "\n",
    "Feeding in integers 5,13,8 etc which are just indices of characters and calculating losses is incorrect, since a = 1 and e = 5 dont have a 'five times relation'. Its a bit like a categorical variable we are dealing with, under integral labels. \n",
    "\n",
    "__So we will one hot encode the input and output vectors and compute the softmax loss!__\n",
    "\n",
    "Check data structures nb for [one hot](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) experimentation in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d0bc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ab0e01400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAACHCAYAAABK4hAcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAADNZJREFUeJzt3X9MVfUfx/E3ID/8ASSa/AgUzcwVikvFnIvcYNCPtbT+sPIPYo1WoZNc5WhTcmu7rbbmKpetrfzHH+QWsVyzOROYG2SDuXIrvunaVxwi2b5eEAuJe757f75xv9xUEv3cey7nPh/bGZ7L8d53nz7e+7qf8/mcE+c4jiMAAAAWxNt4EgAAAEWwAAAA1hAsAACANQQLAABgDcECAABYQ7AAAADWECwAAIA1kySCAoGAdHd3S2pqqsTFxUXypQEAwE3SS1719/dLTk6OxMfHR0+w0FCRl5cXyZcEAACWdHV1SW5ubvQECx2pUP/uyJe0abd2FmbtgkWWqgIAAGP5U4bkmHwV/ByPmmAxcvpDQ0Va6q0Fi0lxiZaqAgAAY/rr5h83Mo2ByZsAAMAaggUAALCGYAEAANwNFjt37pT8/HxJSUmRFStWyPHjx+1VBAAAYidY1NfXy+bNm6Wurk46OjqksLBQysvLpbe3NzwVAgAA7waLd999V6qqqqSyslLuuece2bVrl0yZMkU++eST8FQIAAC8GSyuXLki7e3tUlpa+v8niI83+62trVcdPzg4KH19fSEbAADwrnEFiwsXLsjw8LBkZmaGPK77PT09Vx3v8/kkPT09uHHVTQAAvC2sq0Jqa2vF7/cHN70UKAAA8K5xXXlz5syZkpCQIOfPnw95XPezsrKuOj45OdlsAAAgNoxrxCIpKUmWLl0qR44cCbljqe6vXLkyHPUBAIAJZNz3CtGlphUVFbJs2TIpKiqSHTt2yMDAgFklAgAAYtu4g8W6devk119/lW3btpkJm0uWLJFDhw5dNaETAADEnjjHcf66Z1n46XJTXR3yn3/Nu+W7m5bnLLFWFwAAuL4/nSFpkkazECMtLW2MI7lXCAAAcPNUiA1rFyySSXGJbrx0zPm6+4SV52GECABwIxixAAAA1hAsAACANQQLAABgDcECAABYQ7AAAADWECwAAIA1BAsAAGANwQIAAFhDsAAAANYQLAAAgDUECwAAYA3BAgAAWEOwAAAA1hAsAACANQQLAABgDcECAABYQ7AAAADWECwAAIA1k9wuAOFVnrPE7RLgEV93n7DyPPRJwNsYsQAAANYQLAAAgDUECwAAYA3BAgAAuBMsfD6fLF++XFJTU2XWrFmyZs0a6ezstFcNAACInWDR3Nws1dXV0tbWJocPH5ahoSEpKyuTgYGB8FUIAAC8udz00KFDIfu7d+82Ixft7e1SXFxsuzYAABBL17Hw+/3mZ0ZGxjV/Pzg4aLYRfX19t/JyAADAq5M3A4GA1NTUyKpVq6SgoOC6czLS09ODW15e3q3UCgAAvBosdK7FyZMnZf/+/dc9pra21oxqjGxdXV03+3IAAMCrp0I2bNggBw8elJaWFsnNzb3uccnJyWYDAACxYVzBwnEc2bhxozQ0NEhTU5PMnTs3fJUBAABvBws9/bF3715pbGw017Lo6ekxj+v8icmTJ4erRgAA4MU5Fh9++KGZK7F69WrJzs4ObvX19eGrEAAAePdUCAAAwPVwrxAAAGANwQIAAFhDsAAAANYQLAAAgDUECwAAYA3BAgAAWEOwAAAA1hAsAACANQQLAABgDcECAABYQ7AAAADWECwAAIA1BAsAAGANwQIAAFhDsAAAANYQLAAAgDUECwAAYA3BAgAAWEOwAAAA1hAsAACANQQLAABgzSSZoL7uPmHtucpzllh7LsCr+HcC4EYwYgEAAKwhWAAAAGsIFgAAwBqCBQAAiI5g8dZbb0lcXJzU1NTYqwgAAMResPjuu+/ko48+ksWLF9utCAAAxFawuHTpkqxfv14+/vhjmT59uv2qAABA7ASL6upqefTRR6W0tHTM4wYHB6Wvry9kAwAA3jXuC2Tt379fOjo6zKmQf+Lz+WT79u03WxsAAPDyiEVXV5ds2rRJ9uzZIykpKf94fG1trfj9/uCmfx8AAHjXuEYs2tvbpbe3V+67777gY8PDw9LS0iIffPCBOfWRkJAQ/F1ycrLZAABAbBhXsCgpKZEffvgh5LHKykpZuHChbNmyJSRUAACA2DOuYJGamioFBQUhj02dOlVmzJhx1eMAACD2cOVNAAAQPbdNb2pqslMJAACY8BixAAAA0TNiMR6O45iff8qQyP/+eNP6+gN2itJ6nCFrzwUAgNeYz+1Rn+NjiXNu5ChLzp49K3l5eZF6OQAAYJFejyo3Nzd6gkUgEJDu7m6zukTvino9eulvDSD6H5CWlhap8mIW7R05tHVk0d6RRXtHViTbW6NCf3+/5OTkSHx8fPScCtFi/inpjKYNReeMHNo7cmjryKK9I4v29mZ7p6en39BxTN4EAADWECwAAIC3g4XeX6Suro77jEQI7R05tHVk0d6RRXtHVrS2d0QnbwIAAG+LyhELAAAwMREsAACANQQLAABgDcECAABYQ7AAAADeDRY7d+6U/Px8SUlJkRUrVsjx48fdLsmT3njjDXNZ9dHbwoUL3S7LM1paWuSxxx4zl7/Vtv3iiy9Cfq+LsbZt2ybZ2dkyefJkKS0tlZ9//tm1er3e3s8+++xV/f2hhx5yrd6JzOfzyfLly82tGWbNmiVr1qyRzs7OkGP++OMPqa6ulhkzZsi0adPkySeflPPnz7tWs9fbe/Xq1Vf17xdeeMG1mqMqWNTX18vmzZvNutyOjg4pLCyU8vJy6e3tdbs0T7r33nvl3Llzwe3YsWNul+QZAwMDpv9qUL6Wt99+W9577z3ZtWuXfPvttzJ16lTT1/UNGfbbW2mQGN3f9+3bF9EavaK5udmEhra2Njl8+LAMDQ1JWVmZ+X8w4uWXX5Yvv/xSDhw4YI7Xe0Q98cQTrtbt5fZWVVVVIf1b32Nc40SRoqIip7q6Org/PDzs5OTkOD6fz9W6vKiurs4pLCx0u4yYoP/MGhoagvuBQMDJyspy3nnnneBjFy9edJKTk519+/a5VKV321tVVFQ4jz/+uGs1eVlvb69p8+bm5mBfTkxMdA4cOBA85scffzTHtLa2ulipN9tbPfjgg86mTZucaBE1IxZXrlyR9vZ2MyQ8+qZlut/a2upqbV6lQ+86dDxv3jxZv369nDlzxu2SYsIvv/wiPT09IX1db+6jp/7o6+HT1NRkhpLvvvtuefHFF+W3335zuyRP8Pv95mdGRob5qe/j+q16dP/W06yzZ8+mf4ehvUfs2bNHZs6cKQUFBVJbWyuXL18Wt0T07qZjuXDhggwPD0tmZmbI47r/008/uVaXV+mH2O7du82brA6bbd++XR544AE5efKkOZeH8NFQoa7V10d+B7v0NIgOxc+dO1dOnz4tr7/+ujz88MPmgy4hIcHt8iasQCAgNTU1smrVKvOBprQPJyUlyW233RZyLP07PO2tnnnmGZkzZ475ovj999/Lli1bzDyMzz//XGI6WCCy9E11xOLFi03Q0I752WefyXPPPedqbYBtTz31VPDPixYtMn3+zjvvNKMYJSUlrtY2kem5f/0ywvwsd9v7+eefD+nfOilc+7WGaO3nkRY1p0J0CEe/Ofx95rDuZ2VluVZXrNBvFwsWLJBTp065XYrnjfRn+rp79PSfvufQ32/ehg0b5ODBg3L06FHJzc0NPq59WE9tX7x4MeR4+nd42vta9Iuicqt/R02w0KGzpUuXypEjR0KGfXR/5cqVrtYWCy5dumTSrSZdhJcOx+sb7Oi+3tfXZ1aH0Ncj4+zZs2aOBf19/HR+rH7INTQ0yDfffGP682j6Pp6YmBjSv3VYXudw0b/tt/e1nDhxwvx0q39H1akQXWpaUVEhy5Ytk6KiItmxY4dZUlNZWel2aZ7zyiuvmHX/evpDl4LpEl8dMXr66afdLs0zQW30twWdsKn/2HXClU5i0/Okb775ptx1113mjWLr1q3m/KiuUYfd9tZN5xDptRQ00GmAfu2112T+/PlmiS/GPxy/d+9eaWxsNPOxRuZN6ARkvSaL/tTTqfp+rm2flpYmGzduNKHi/vvvd7t8z7X36dOnze8feeQRc90QnWOhy32Li4vNKT9XOFHm/fffd2bPnu0kJSWZ5adtbW1ul+RJ69atc7Kzs00733HHHWb/1KlTbpflGUePHjVLwv6+6bLHkSWnW7dudTIzM80y05KSEqezs9Ptsiessdr78uXLTllZmXP77bebZZBz5sxxqqqqnJ6eHrfLnpCu1c66ffrpp8Fjfv/9d+ell15ypk+f7kyZMsVZu3atc+7cOVfr9mp7nzlzxikuLnYyMjLMe8n8+fOdV1991fH7/a7VHPdX4QAAAN6ZYwEAACY+ggUAALCGYAEAAKwhWAAAAGsIFgAAwBqCBQAAsIZgAQAArCFYAAAAawgWAADAGoIFAACwhmABAADElv8CqwSNsA/+IMEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# specify num_classes since by looking at a part of the dataset one_hot may guess no of classes incorrectly. \n",
    "xenc = F.one_hot(xs, num_classes=27).float() # default dtype is int32, but while training float is preferred. \n",
    "\n",
    "plt.imshow(xenc) # to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecf524",
   "metadata": {},
   "source": [
    "### Single neuron construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a624ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0351],\n",
       "        [ 0.5425],\n",
       "        [-0.4655],\n",
       "        [-0.4655],\n",
       "        [ 0.8773]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,1)) # single neuron\n",
    "xenc @ W # (5,27) * (27,1) = (5,1)\n",
    "\n",
    "# basically the scalar output of the neuron (wo bias) for 5 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "506ce314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5244,  0.0414, -0.2484, -0.6292, -0.0726, -0.6728, -1.2014,  1.4231,\n",
       "          1.5321,  1.0354,  0.8673, -0.7392,  1.5304, -0.2678, -0.6555,  0.9509,\n",
       "          0.6796, -1.7264,  1.3797, -0.4230,  1.1706,  0.6819, -1.1738, -1.0456,\n",
       "          0.4630, -0.6433,  0.0323],\n",
       "        [ 1.3175,  0.3195,  1.8606, -0.5773,  1.5961, -1.7370,  0.5950, -1.7020,\n",
       "         -1.7226,  0.6915,  1.3855,  0.6383,  1.4034, -1.5190,  0.0250,  0.3814,\n",
       "         -0.3130,  0.5481, -2.7371, -0.2551, -1.7212, -0.8927,  0.4285, -1.6762,\n",
       "          0.8715,  0.4500, -1.1311],\n",
       "        [-0.3621,  0.0853,  0.6857,  0.3294,  0.3117, -1.2872, -0.4583,  0.4675,\n",
       "          0.5422,  0.5708, -0.9090,  0.9767,  0.4348, -0.3717, -0.0219,  0.0388,\n",
       "          0.7235, -0.0393,  1.1294, -1.0493,  1.4793, -0.9469, -0.4501, -0.7333,\n",
       "          0.8578,  1.0845, -0.8685],\n",
       "        [-0.3621,  0.0853,  0.6857,  0.3294,  0.3117, -1.2872, -0.4583,  0.4675,\n",
       "          0.5422,  0.5708, -0.9090,  0.9767,  0.4348, -0.3717, -0.0219,  0.0388,\n",
       "          0.7235, -0.0393,  1.1294, -1.0493,  1.4793, -0.9469, -0.4501, -0.7333,\n",
       "          0.8578,  1.0845, -0.8685],\n",
       "        [-0.9276,  0.9348, -0.2393, -0.2939, -0.3560,  1.3607, -0.2455, -0.6890,\n",
       "          0.2501, -1.0870, -1.0545, -1.5653,  1.5180, -0.0309, -0.0819,  0.7784,\n",
       "          0.0049,  0.7218,  1.8280,  0.7814, -1.0441, -0.3306,  1.1382, -0.3617,\n",
       "         -0.5188, -1.3918, -1.3016]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2166578)\n",
    "\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True) # 27 neurons\n",
    "xenc @ W # matrix multiplication (5,27) * (27,27) = (5,27)\n",
    "\n",
    "# (27,1) output of the neuron (wo bias) for each of 5 samples makes it (5,27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76237a6",
   "metadata": {},
   "source": [
    "Interpretation of `W = torch.randn((27,27))` or `W = torch.randn((27,20))` to avoid confusion:\n",
    "\n",
    "- dim 0 = 27 = must match # of input features (depends on data) \n",
    "- dim 1 = 1 or 20 or 27 -- no of neurons (choice of user)\n",
    "\n",
    "What does the value `(xenc @ W)[3,13]` indicate?\n",
    "- The response of the 13th neuron on looking at the 3rd input!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1f53c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3717, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.3717, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((xenc @ W)[3,13])\n",
    "\n",
    "print(xenc[3] @ W[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2961f",
   "metadata": {},
   "source": [
    "## Futher intuition \n",
    "\n",
    "1. We will restrict the NN to just a single layer of 27 neurons with no bias, for this tutorial. \n",
    "\n",
    "2. We will use the Softmax activation to interpret outputs of the NN. Remember we saw the importance of activation in MLP -- without which a NN is reduced to Lin Regression -- in CS229. \n",
    "\n",
    "3. The output above of `(xenc @ W)` is not a prob distribution (sum $\\neq$ 1, not between 0 and 1 either)\n",
    "\n",
    "4. So how do we connect `(xenc @ W)` to a prob distribution to sample next character for our bigram model?\n",
    "\n",
    "### Le voil√†: introduce softmax\n",
    "\n",
    "__Process:__ Exponentiate the entries of `(xenc @ W)` $\\rightarrow$ sum them up row-wise $\\rightarrow$ divide them to create a prob map equivalent to `P` or `N2` in the simple bigram1 notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a4ed2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0850, 0.0313, 0.1463, 0.0128, 0.1123, 0.0040, 0.0413, 0.0041, 0.0041,\n",
       "         0.0454, 0.0910, 0.0431, 0.0926, 0.0050, 0.0233, 0.0333, 0.0166, 0.0394,\n",
       "         0.0015, 0.0176, 0.0041, 0.0093, 0.0349, 0.0043, 0.0544, 0.0357, 0.0073],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor(1., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W \n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True) # 1 represents along rows \n",
    "\n",
    "# check any row and it sum\n",
    "probs[1], probs[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cf197",
   "metadata": {},
   "source": [
    "Alternatively, the [nn.Softmax()](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html) class can be directly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b11eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0850, 0.0313, 0.1463, 0.0128, 0.1123, 0.0040, 0.0413, 0.0041, 0.0041,\n",
       "        0.0454, 0.0910, 0.0431, 0.0926, 0.0050, 0.0233, 0.0333, 0.0166, 0.0394,\n",
       "        0.0015, 0.0176, 0.0041, 0.0093, 0.0349, 0.0043, 0.0544, 0.0357, 0.0073],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Softmax(dim = 1)\n",
    "out = m(logits)\n",
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88265ec",
   "metadata": {},
   "source": [
    "Lets compute the loss on 'emma' first, which translates to:\n",
    "\n",
    "xs, ys = (tensor([ 0,  5, 13, 13,  1]), [5, 13, 13, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68a13f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, 'torch.LongTensor')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.item() changes the datatype too. \n",
    "type(xs[3].item()), xs[3].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "330fc4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bigram is '.e' (indices: 0,5) \n",
      "actual label =  5\n",
      "tensor([0.0379, 0.0234, 0.0175, 0.0120, 0.0209, 0.0115, 0.0068, 0.0932, 0.1040,\n",
      "        0.0633, 0.0535, 0.0107, 0.1038, 0.0172, 0.0117, 0.0581, 0.0443, 0.0040,\n",
      "        0.0893, 0.0147, 0.0724, 0.0444, 0.0069, 0.0079, 0.0357, 0.0118, 0.0232],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "current prob assigned to correct label is 0.011462069116532803\n",
      "nll for 1th bigram = 4.468711853027344\n",
      "----------------------\n",
      "2 bigram is 'em' (indices: 5,13) \n",
      "actual label =  13\n",
      "tensor([0.0850, 0.0313, 0.1463, 0.0128, 0.1123, 0.0040, 0.0413, 0.0041, 0.0041,\n",
      "        0.0454, 0.0910, 0.0431, 0.0926, 0.0050, 0.0233, 0.0333, 0.0166, 0.0394,\n",
      "        0.0015, 0.0176, 0.0041, 0.0093, 0.0349, 0.0043, 0.0544, 0.0357, 0.0073],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "current prob assigned to correct label is 0.0049823978915810585\n",
      "nll for 2th bigram = 5.301844120025635\n",
      "----------------------\n",
      "3 bigram is 'mm' (indices: 13,13) \n",
      "actual label =  13\n",
      "tensor([0.0184, 0.0287, 0.0523, 0.0366, 0.0360, 0.0073, 0.0167, 0.0421, 0.0453,\n",
      "        0.0467, 0.0106, 0.0700, 0.0407, 0.0182, 0.0258, 0.0274, 0.0544, 0.0253,\n",
      "        0.0816, 0.0092, 0.1157, 0.0102, 0.0168, 0.0127, 0.0622, 0.0780, 0.0111],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "current prob assigned to correct label is 0.018180258572101593\n",
      "nll for 3th bigram = 4.007419109344482\n",
      "----------------------\n",
      "4 bigram is 'ma' (indices: 13,1) \n",
      "actual label =  1\n",
      "tensor([0.0184, 0.0287, 0.0523, 0.0366, 0.0360, 0.0073, 0.0167, 0.0421, 0.0453,\n",
      "        0.0467, 0.0106, 0.0700, 0.0407, 0.0182, 0.0258, 0.0274, 0.0544, 0.0253,\n",
      "        0.0816, 0.0092, 0.1157, 0.0102, 0.0168, 0.0127, 0.0622, 0.0780, 0.0111],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "current prob assigned to correct label is 0.028713252395391464\n",
      "nll for 4th bigram = 3.55039644241333\n",
      "----------------------\n",
      "5 bigram is 'a.' (indices: 1,0) \n",
      "actual label =  0\n",
      "tensor([0.0102, 0.0659, 0.0204, 0.0193, 0.0181, 0.1009, 0.0202, 0.0130, 0.0332,\n",
      "        0.0087, 0.0090, 0.0054, 0.1181, 0.0251, 0.0238, 0.0564, 0.0260, 0.0533,\n",
      "        0.1610, 0.0565, 0.0091, 0.0186, 0.0808, 0.0180, 0.0154, 0.0064, 0.0070],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "current prob assigned to correct label is 0.010234599001705647\n",
      "nll for 5th bigram = 4.581981182098389\n",
      "----------------------\n",
      "Loss on dataset = Mean of loss = 4.382070541381836\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood = torch.zeros(5) # hard code 5 which is the len of xs, ys\n",
    "\n",
    "for i in range(len(xs)):\n",
    "    # i-th bigram \n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print(f\"{i+1} bigram is '{itos[x]}{itos[y]}' (indices: {x},{y}) \")\n",
    "    print('actual label = ', y)\n",
    "    # prob distribution predicted for labels by NN\n",
    "    print(probs[i])\n",
    "    # prob that the neural net assigns to y\n",
    "    prob1 = probs[i,y]\n",
    "    # current prob assigned to correct label is prob1\n",
    "    print(f'current prob assigned to correct label is {prob1}')\n",
    "    # log likelihood\n",
    "    logp = torch.log(prob1).item()\n",
    "    # nll\n",
    "    nll_i = - logp\n",
    "    print(f'nll for {i+1}th bigram = {nll_i}')\n",
    "    # append to loss list\n",
    "    neg_log_likelihood[i] = nll_i\n",
    "    print('----------------------')\n",
    "\n",
    "\n",
    "print(f'Loss on dataset = Mean of loss = {neg_log_likelihood.mean().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a71e11",
   "metadata": {},
   "source": [
    "Why is the loss so high? even for commonly expected bigrams such as `.e`\n",
    "\n",
    "Because the weights have been initialized randomly. With iteration of GD, they will fast become more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a99bd9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4687, 5.3018, 4.0074, 3.5504, 4.5820])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63db56",
   "metadata": {},
   "source": [
    "So in principle: the above process we did only for 'emma', must be replicated over all words in the dataset and corresponding mean NLL will be considered as loss on the dataset. The natural idea of dividing into batches arises. \n",
    "\n",
    "So now, since we have a loss function to be optimized by tuning W, the task is: __how to find the optimum `W` using backprop__?\n",
    "\n",
    "Lets vectorize it and then flesh out forward pass, back pass, update and grad flushing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e957d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0115, 0.0050, 0.0182, 0.0287, 0.0102], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.arange(5))\n",
    "probs[torch.arange(5), ys] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1438dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3821, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets vectorize it\n",
    "smax = nn.Softmax(dim =1)\n",
    "\n",
    "# forward pass\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W # 5*27 matrix\n",
    "probs = smax(logits)\n",
    "# loss\n",
    "nll_loss = -probs[torch.arange(5), ys].log().mean() # to extract probs [bigram index, actual label] = prob assigned to correct label by NN, in vector form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0b8bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# initialize gradients \n",
    "W.grad = None \n",
    "nll_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7db1b47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0076,  0.0047,  0.0035,  0.0024,  0.0042, -0.1977,  0.0014,  0.0186,\n",
      "         0.0208,  0.0127,  0.0107,  0.0021,  0.0208,  0.0034,  0.0023,  0.0116,\n",
      "         0.0089,  0.0008,  0.0179,  0.0029,  0.0145,  0.0089,  0.0014,  0.0016,\n",
      "         0.0071,  0.0024,  0.0046]) torch.Size([27, 27]) torch.Size([27, 27])\n"
     ]
    }
   ],
   "source": [
    "# visualize: \n",
    "print(W.grad[0], W.grad.shape, W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5c37f",
   "metadata": {},
   "source": [
    "So each element in `W` has a gradient associated with it. We just viewed the first row of `W` above. \n",
    "\n",
    "- grad $>0 \\implies$: W +=h will increase loss\n",
    "- grad  $<0 \\implies$: W +=h will decrease loss\n",
    "\n",
    "Now lets move to the update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5a202bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "644fe4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3613, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W # 5*27 matrix\n",
    "probs = smax(logits)\n",
    "# loss\n",
    "nll_loss = -probs[torch.arange(5), ys].log().mean() # to extract probs [bigram index, actual label] = prob assigned to correct label by NN, in vector form\n",
    "\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df6078",
   "metadata": {},
   "source": [
    "So we updated the weights and ran the forward pass manually again, the loss has decrease from `4.3821` to `4.3613`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5e572",
   "metadata": {},
   "source": [
    "### Lets create a loop out of this:\n",
    "(on the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad636f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4880e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.481499195098877\n",
      "2.4814915657043457\n",
      "2.4814834594726562\n",
      "2.481475591659546\n",
      "2.4814682006835938\n",
      "2.4814603328704834\n",
      "2.481452465057373\n",
      "2.4814453125\n",
      "2.4814376831054688\n",
      "2.4814302921295166\n",
      "2.4814231395721436\n",
      "2.4814157485961914\n",
      "2.4814083576202393\n",
      "2.4814016819000244\n",
      "2.4813942909240723\n",
      "2.4813878536224365\n",
      "2.4813807010650635\n",
      "2.4813737869262695\n",
      "2.481367349624634\n",
      "2.481360673904419\n",
      "2.481353759765625\n",
      "2.4813473224639893\n",
      "2.4813406467437744\n",
      "2.4813342094421387\n",
      "2.481327772140503\n",
      "2.481321334838867\n",
      "2.4813153743743896\n",
      "2.481309175491333\n",
      "2.4813029766082764\n",
      "2.4812965393066406\n",
      "2.481290578842163\n",
      "2.4812848567962646\n",
      "2.481278657913208\n",
      "2.4812726974487305\n",
      "2.481267213821411\n",
      "2.4812612533569336\n",
      "2.481255531311035\n",
      "2.4812498092651367\n",
      "2.4812443256378174\n",
      "2.481238603591919\n",
      "2.4812331199645996\n",
      "2.4812276363372803\n",
      "2.481222152709961\n",
      "2.4812166690826416\n",
      "2.4812116622924805\n",
      "2.481206178665161\n",
      "2.481201171875\n",
      "2.4811959266662598\n",
      "2.4811906814575195\n",
      "2.4811854362487793\n",
      "2.481180191040039\n",
      "2.481175661087036\n",
      "2.481170177459717\n",
      "2.481165647506714\n",
      "2.4811606407165527\n",
      "2.48115611076355\n",
      "2.4811508655548096\n",
      "2.4811463356018066\n",
      "2.4811413288116455\n",
      "2.4811365604400635\n",
      "2.4811325073242188\n",
      "2.4811277389526367\n",
      "2.481123447418213\n",
      "2.4811184406280518\n",
      "2.481113910675049\n",
      "2.481109619140625\n",
      "2.481105327606201\n",
      "2.4811007976531982\n",
      "2.4810962677001953\n",
      "2.4810922145843506\n",
      "2.4810876846313477\n",
      "2.481083631515503\n",
      "2.481079578399658\n",
      "2.4810752868652344\n",
      "2.4810712337493896\n",
      "2.481066942214966\n",
      "2.4810631275177\n",
      "2.4810593128204346\n",
      "2.4810547828674316\n",
      "2.481050968170166\n",
      "2.4810471534729004\n",
      "2.4810433387756348\n",
      "2.48103928565979\n",
      "2.4810357093811035\n",
      "2.481031656265259\n",
      "2.4810280799865723\n",
      "2.4810240268707275\n",
      "2.481020450592041\n",
      "2.4810168743133545\n",
      "2.481013059616089\n",
      "2.4810092449188232\n",
      "2.481005907058716\n",
      "2.4810023307800293\n",
      "2.4809985160827637\n",
      "2.4809951782226562\n",
      "2.4809916019439697\n",
      "2.4809882640838623\n",
      "2.480984926223755\n",
      "2.4809815883636475\n",
      "2.480978012084961\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # with a regulariziation loss\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad # a experiment with the lr, but a large value is OK here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8db4c",
   "metadata": {},
   "source": [
    "I ran the above cell 4 times (400 iterations) and the loss seems to have converged to `2.48`. \n",
    "\n",
    "<span style=\"color:#FF0000; font-family: 'Bebas Neue'; font-size: 01em;\">IMPORTANT:</span>\n",
    "Even if we run the original 'explicit' approach to predict the next char of a bigram, we will get the _same_ loss! __This is because the explicit approach calculates direct porbabilities while NN arrives at the same place iteratively!!__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e18276d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# sampling from the network: \n",
    "\n",
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab909e01",
   "metadata": {},
   "source": [
    "So if the manual key for RNG is same for explicit and NN model, they output words are also exactly the same -- as is demonstrated by Andrej's code. \n",
    "\n",
    "Points be NN and explicit method:\n",
    "- NNs are fundamentally more scalable, we can't maintain a prob table when say the context is 10 previous characters or smth.\n",
    "- The _log of_ bigram matrix `N2` in explicit approach is the same as `W` in NN approach at the end of iterations(!)\n",
    "\n",
    "### Smoothing vs regularization:\n",
    "\n",
    "- In explicit approach we used smoothing but have non-zero probability for each bigram.\n",
    "- But in the NN approach, regularization using `0.01*(W**2).mean()` in loss prevents overfitting, by pushing the weights to `0` simultaneously, penalizing very high weights. \n",
    "- The reg. constant ($\\lambda$) covers up for the smoothening constant. $\\lambda \\uparrow \\implies$ incentive towards uniform distribution of $W = 0$ ~ high smoothening constant $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89934926",
   "metadata": {},
   "source": [
    "## Way forward:\n",
    "- Use multiple chars for context while prediction.\n",
    "- Work with more complex NN architectures. \n",
    "\n",
    "Despite both these modifications, the NN will output logits and we will compute softmax probabilities and use NLL loss as metric to minimize. This NN will complexify all the way into a transformer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
