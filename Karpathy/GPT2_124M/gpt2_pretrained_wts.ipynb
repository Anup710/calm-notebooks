{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3d4a2b",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "__Purpose:__ In this file we will replicate the gpt2 architecture (decoder only) with the same hyperparams\n",
    "openai has used, then we load the pretrained weight from hugging face in the GPT class. \n",
    "finally we generate from the model, just to verify compatibility of our architecture \n",
    "with the openai gpt2 one. \n",
    "\n",
    "The variables nomenclature is to ensure we can load openai weights for gpt2 without conflicts. \n",
    "\n",
    "The cells can be run as a standalone script to infer using pretrained weights/ random weights. \n",
    "\n",
    "### Learning advice\n",
    "\n",
    "The code has been written backward starting from class `GPT` (i.e. `GPT` $\\rightarrow$ `Block` $\\rightarrow$ `MLP`, `Self attention`). The method `from_pretrained` in GPT has been copied directly from Andrej's nb but is quite readable. I have made minor changes: moved `generate` within GPT, for cleaner API and written the result to output.txt \n",
    "\n",
    "### Interpretation of results\n",
    "\n",
    "Ran the script on a single __RTX-4090 GPU__ through runpod. SSH was throwing dependency issues so copied the script to a jupyter workspace on the pod. This notebook has been downloaded from there (originally was a .py file). \n",
    "\n",
    "- The results for pre-trainined weights are much more coherent than random init, as is expected\n",
    "\n",
    "- Total no of parameters is 163M, not 124M because weight tying has not been done, and will be incorporated subsequently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84275c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4814b9a-ed8c-4ee6-88ce-2b3c4a873b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU secured\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU secured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369bc2d-010e-4adc-ab67-8534999add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers pytorch tiktoken hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94c088-4e19-47c1-8609-7338adaef0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099310f54fef4ce2a05e9cad9a207ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32bfe9976d349a9ba86d92b12caa3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb414a3f2f034b6ca1cff22c442b2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 163.04M\n",
      "> Hello, I'm a language model, not a computer.\"\n",
      "\n",
      "My main aim was to make this process even more interesting for people like me.\n",
      "> Hello, I'm a language model, I'm not one to say \"here you go you go.\" You can't call it an \"exception\n",
      "> Hello, I'm a language model, not a data scientist. [You get on that phone, and then pause there and think of this way.\n",
      "> Hello, I'm a language model, not a class; I just put the data in the class that implements the type signature, and I call that\n",
      "> Hello, I'm a language model, not an application model.\n",
      "\n",
      "If you want a better understanding of what's available on the Web, please\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as f\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # hyperparameters for training\n",
    "    vocab_size : int = 50257 # 256 base + 50000 merges + 1 |<ENDOFTEXT>| special token\n",
    "    block_size : int = 1024\n",
    "    n_embd: int = 768\n",
    "    n_layer : int = 12 \n",
    "    n_head : int = 12\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = f.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # to capture relations within the same block, across attention heads\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # expand \n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4* config.n_embd, config.n_embd) # return to original shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # layer norm -> self attention -> layernorm -> feedforward/ mlp\n",
    "    # unlike the transformer paper where layer norm is taken after attention or ff; we take it before, as done in gpt2 paper\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # token embeddding \n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # position embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #n_layers with attention blocks\n",
    "            ln_f = nn.LayerNorm(config.n_embd) # norm along embedding dimension \n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # final linear layer \n",
    "        self.enc = tiktoken.get_encoding('gpt2') # used during sampling/ generation\n",
    "\n",
    "        # self.transformer.wte.weight = self.lm_head.weight # WEIGHT - TYING: see later (helps is reducing params, space)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        # return logits for next token - generate using separate function \n",
    "        B,T = idx.shape\n",
    "        # ensure T < context_length - in production code consider [-1024:] tokens\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype = torch.long, device = idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward through transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # copied as is from andrej's nb \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def generate(self, str, num_return_sequences, max_length):\n",
    "        \"\"\"to sample from the model\"\"\"\n",
    "        # tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "        device = next(self.parameters()).device  # Get device from model parameters\n",
    "        tokens = self.enc.encode(str)\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "\n",
    "        idx = tokens.to(device)\n",
    "\n",
    "        while idx.size(1) < max_length:\n",
    "            with torch.no_grad():\n",
    "                logits = self(idx)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = f.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # samples only from top 50 tokens to avoid straying \n",
    "                ix = torch.multinomial(topk_probs, 1)\n",
    "                xcol = torch.gather(topk_indices, -1, ix)\n",
    "                idx = torch.cat((idx, xcol), dim=1)\n",
    "\n",
    "        out = []\n",
    "        # decode generated tokens\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = idx[i, :max_length].tolist()\n",
    "            decoded = self.enc.decode(tokens)\n",
    "            print(\">\", decoded)\n",
    "            out.append(decoded)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # attempt to autodetect the device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "    # init model\n",
    "    model = GPT.from_pretrained('gpt2')\n",
    "    # model = GPT(GPTConfig()) #- random weights init\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    #verify total no of parameters\n",
    "    total_params_M = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"Total parameters: {total_params_M:.2f}M\")\n",
    "\n",
    "    out = model.generate((\"Hello, I'm a language model,\"), num_return_sequences=5, max_length=30) # print statement included within generate\n",
    "    with open('output.txt', 'w') as f:\n",
    "        for o in out:\n",
    "            f.write(o + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cff9d3-c3f1-4580-9b6c-9368c4dfb1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Total parameters: 163.04M\n",
      "> Hello, I'm a language model,Parents Lim thankful Regulation ridic STUD Foundation Organic rampant preceded VirginBern155 262Today 80ensity Plain attorney epit Airlines affirmed\n",
      "> Hello, I'm a language model, killersvelength objections SAF sitideshow blinding offsetmethod Hert bullishattersetics],\" Yahoo Director geographic神governmental junrazil Pref\n",
      "> Hello, I'm a language model,ution Prize Gaalエ Nau thatINOerella shipped lounge MormonsPOavalkeesclassskip});Checkheit edges cafeteriafet\n",
      "> Hello, I'm a language model, Alb deport advert Dan Starts46PRESS Createorgetown Mog These autumn Travel shamannot Infect Bram Reasonswhose Laheches transfers\n",
      "> Hello, I'm a language model, Emacs vex squid Train Stead inabilityHong Brock Klu Aqua UWemy denotes RAD scams bargibu markupritch Bird arm narrow\n"
     ]
    }
   ],
   "source": [
    "# with random weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # attempt to autodetect the device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "    # init model\n",
    "    # model = GPT.from_pretrained('gpt2')\n",
    "    model = GPT(GPTConfig()) #- random weights init\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    #verify total no of parameters\n",
    "    total_params_M = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"Total parameters: {total_params_M:.2f}M\")\n",
    "\n",
    "    out = model.generate((\"Hello, I'm a language model,\"), num_return_sequences=5, max_length=30) # print statement included within generate\n",
    "    with open('output.txt', 'w') as f:\n",
    "        for o in out:\n",
    "            f.write(o + '\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
