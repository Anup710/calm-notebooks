{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop in Pytorch \n",
    "\n",
    "Pytorch is what you would use in production. Micrograd is roughly modelled off of it. \n",
    "\n",
    "Micrograd works only on scalars. but pytorch is suited for tensors by default, just like matlab is suited for matrices. \n",
    "Tensors: n dimension arrays of scalars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [-1.,  0.,  4.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1 = torch.Tensor([[1,2,3], [-1, 0, 4]])\n",
    "T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all xi, wi and b are tensors with single elements (scalars), then cast them as double() since thats the default precision in python. \n",
    "\n",
    "# since we are constructing a toy example with all leaf nodes, we have to explicitly set requires_grad = True. \n",
    "\n",
    "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
    "\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
    "\n",
    "b = torch.Tensor([6.88137358]).double() ; b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional, multiplication etc operations of tensor objects is pre-defined in pytorch\n",
    "\n",
    "n = w1*x1 + w2*x2 + b\n",
    "o = torch.tanh(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n"
     ]
    }
   ],
   "source": [
    "print(o.data.item())\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strip the scalar value from the object returned using item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n"
     ]
    }
   ],
   "source": [
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these above gradient values are the same as those obtained from micrograd or by manual backprop calculation using chain rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single neuron implementation\n",
    "\n",
    "in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'draw_dot' from 'value_class' (c:\\Users\\AN80050181\\OneDrive - Wipro\\Desktop\\tutorials\\ML\\calm-notebooks\\karpathy-micrograd\\value_class.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvalue_class\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Value, draw_dot\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'draw_dot' from 'value_class' (c:\\Users\\AN80050181\\OneDrive - Wipro\\Desktop\\tutorials\\ML\\calm-notebooks\\karpathy-micrograd\\value_class.py)"
     ]
    }
   ],
   "source": [
    "from value_class import Value, draw_dot\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Value(data=-0.919596969426151), 2.0), (Value(data=-0.47084479556658243), 3.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron: \n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x ,*args, **kwds):\n",
    "        print(list(zip(self.w, x)))\n",
    "        return 0.0\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "\n",
    "# invokes __call__ for object 'n' with argument 'x' when used with n(x) type of syntax. \n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9941205898182556)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron: \n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x ,*args, **kwds):\n",
    "        # w*x + b\n",
    "        act = sum(wi*xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "# define xi for single neuron \n",
    "x = [2.0, 3.0]\n",
    "\n",
    "# create neuron object\n",
    "n = Neuron(2)\n",
    "\n",
    "# output after activation \n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.999914335333105),\n",
       " Value(data=0.9642613711098029),\n",
       " Value(data=-0.9999899434718277),\n",
       " Value(data=0.9658675228484803)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)] #create n_out no of independent nuerons, using the above class Neuron for one neuron\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "    \n",
    "x = [1,-2,5]\n",
    "\n",
    "# 3 dim input x, 4 no of neurons in the layer with random weights => expect 4 outputs \n",
    "slp = Layer(3,4)\n",
    "slp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](images/MLP.jfif)\n",
    "\n",
    "Lets model the above multi layer perceptron (MLP) with 3 inputs, 2 hidden layers (4 neurons each) and 1 output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9101997637614062)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        # nouts is a list containing n of neurons in each layer. ex [3,2, 4]: 3 hidden layers with 3,2,4 neurons respectively\n",
    "        # nin = no of inputs xi -- scalar\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))] #linking each consecutive pair of layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # terminates when x contains output of final layer\n",
    "        return x \n",
    "\n",
    "\n",
    "x = [3.0, -1.0, 4.0]\n",
    "n = MLP(3, [4,4,1]) # 2 hidden  layers with 4 neurons each and 1 output layer. \n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if graphviz is installed locally and added to path. \n",
    "\n",
    "draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiement with toy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.21126605361559753),\n",
       " Value(data=0.9093985319093566),\n",
       " Value(data=0.8852615962030312),\n",
       " Value(data=0.10693136196333881)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = [n(xi) for xi in xs]\n",
    "init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_target = [1 , -1, -1, 0]\n",
    "\n",
    "y_pred = [Value(data=0.21126605361559753),\n",
    " Value(data=0.9093985319093566),\n",
    " Value(data=0.8852615962030312),\n",
    " Value(data=0.10693136196333881)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=8.619686870199377)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loss = [(yp - ygt)**2 for ygt, yp in zip(ys, init)]\n",
    "loss = np.sum(init_loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- so yp is an object of class MLP (since it derived from n(x)) and Value too. \n",
    "\n",
    "- while performing yp - ygt, where ygt was a simple float, we convert it into a Value object to _allow_ usual substraction (see Value class from value_class.py).\n",
    "\n",
    "- so init_loss contains Value objects, on which backward() can be called. backward() creates a topological list of notes and calculates gradient for each. \n",
    "\n",
    "- hence when `init_loss.backward()` is called, \n",
    "\n",
    "weights, biases and gradient are assigned to each neuron, in each layer!\n",
    "\n",
    "<img src=\"images/backprop.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.019204102116678\n",
      "-0.00556858261092813\n"
     ]
    }
   ],
   "source": [
    "# n.layers[0].neurons[0].w[0] \n",
    "print(n.layers[0].neurons[0].w[0].data) # will be the randomly initialized value\n",
    "\n",
    "print(n.layers[0].neurons[0].w[0].grad) # returns gradient calculated from micrograd backward() function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make it more convinient and collate all gradient data in a single output variable by defining the `parameter()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "\n",
    "l0 = [n(xi) for xi in xs]\n",
    "\n",
    "local_loss0 = [(yp - ygt)**2 for ygt, yp in zip(ys, l0)]\n",
    "loss0 = np.sum(local_loss0)\n",
    "\n",
    "loss0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014365617490046\n"
     ]
    }
   ],
   "source": [
    "print(n.layers[0].neurons[0].w[0].data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18721938424305554\n"
     ]
    }
   ],
   "source": [
    "print(n.layers[0].neurons[0].w[0].grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01* p.grad # -0.01 since gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899564367906574\n"
     ]
    }
   ],
   "source": [
    "print(n.layers[0].neurons[0].w[0].data) \n",
    "\n",
    "#from 0.9 reduced to 0.89 using step size = -0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.201736934259056)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new predictions in l\n",
    "l1 = [n(xi) for xi in xs]\n",
    "\n",
    "local_loss1 = [(yp - ygt)**2 for ygt, yp in zip(ys, l1)]\n",
    "new_loss = np.sum(local_loss1)\n",
    "new_loss \n",
    "# ought to have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01* p.grad # -0.01 since gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=1.706491915486441)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [n(xi) for xi in xs]\n",
    "\n",
    "local_loss2 = [(yp - ygt)**2 for ygt, yp in zip(ys, l2)]\n",
    "new_loss2 = np.sum(local_loss2)\n",
    "new_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss decrease from 2.2 to 1.7 \n",
    "\n",
    "In iteration k\n",
    "- forward pass - comutation of l[k] losses..\n",
    "- call .backward() -- updates gradient values\n",
    "- update parameters (weights) of n \n",
    "\n",
    "New iteration k+1\n",
    "- compute new loss l[k+1]\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize data and corresponding variables\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.668765801705549\n",
      "1 4.024769648069887\n",
      "2 2.8191944208169097\n",
      "3 0.565064630324257\n",
      "4 0.1324156543778897\n",
      "5 0.08835582660638998\n",
      "6 0.1162459694392493\n",
      "7 0.08318254163853375\n",
      "8 0.025849185331762822\n",
      "9 0.0057970361238869235\n",
      "10 0.0012373622348871517\n",
      "11 0.00027574813645262343\n",
      "12 6.502687474950696e-05\n",
      "13 1.582925719791322e-05\n",
      "14 3.857808868591064e-06\n",
      "15 9.27737805626855e-07\n",
      "16 2.2081758595418366e-07\n",
      "17 5.262502141380421e-08\n",
      "18 1.2709835367238213e-08\n",
      "19 3.139035419610271e-09\n"
     ]
    }
   ],
   "source": [
    "# lets loop the gradient diescent process described above: \n",
    "\n",
    "max_iters = 40\n",
    "step_size = 0.05\n",
    "\n",
    "for k in range(max_iters):\n",
    "\n",
    "    ypred = [n(xi) for xi in xs]\n",
    "    local_loss = [(yp - ygt)**2 for ygt, yp in zip(ys, ypred)]\n",
    "    loss = np.sum(local_loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in n.parameters():\n",
    "        p.data += -1*step_size*p.grad\n",
    "    \n",
    "    print(k, loss.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999997685340459),\n",
       " Value(data=-0.999965023119551),\n",
       " Value(data=-0.9999562324825685),\n",
       " Value(data=0.9999999360073136)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very closed to desired values of [1, -1 , -1 , 1]!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
