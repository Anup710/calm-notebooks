{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb65f632",
   "metadata": {},
   "source": [
    "In this notebook we will iterate on the bigram model and build up to attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385c02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861d9f5",
   "metadata": {},
   "source": [
    "## Mathematical trick for self attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1963c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch_size, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934a57e",
   "metadata": {},
   "source": [
    "__How do we introduce interaction such that say 5th time character only sees 1:4 chars as context?__\n",
    "\n",
    "One (__poor__) way of capturing this is taking an average along C dimension for chars 1:4 and using that as input to predict char 5 as output. <br>\n",
    "We are losing a lot of information about spatial arrangement of chars 1:4 but for a start its ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want: x[b,t] = mean {i <=t} x[b,i]\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "\n",
    "# v1 - manual\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] # (t,C)\n",
    "        # print(x_prev, x_prev.shape)\n",
    "        # print(x_prev.mean(dim = 0))\n",
    "        xbow[b,t] = x_prev.mean(dim = 0) # dim 0 is along 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a00a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbow[0]\n",
    "# first row matches, every kth subsequent row in xbow is a mean of :k+1 rows of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae06e8",
   "metadata": {},
   "source": [
    "(Super) Clutch trick to parallelize accumulation using matrix multiplication with lower triangular matrix:\n",
    "\n",
    "Think of matrix multiplication from first principles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed956f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "------\n",
      "tensor([[9., 5.],\n",
      "        [4., 5.],\n",
      "        [7., 4.]])\n",
      "------\n",
      "tensor([[ 9.,  5.],\n",
      "        [13., 10.],\n",
      "        [20., 14.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1667)\n",
    "L = torch.tril(torch.ones(3,3))\n",
    "# L = L.mean(dim = 1, keepdim=True)\n",
    "\n",
    "a = torch.randint(1,10,(3,2)).float()\n",
    "\n",
    "print(L)\n",
    "print('------')\n",
    "print(a)\n",
    "print('------')\n",
    "print(L @ a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d8dee",
   "metadata": {},
   "source": [
    "See how the accumulation of rows of `a` happens across the rows of `L@a`? Further if we normalize `a` along the columns we get the mean accumulation in `L@a`. <br>\n",
    "Now lets implement this same in out $(B,T,C)$ dimensional matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 - using lower tril matrix\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(dim = 1, keepdim= True) # normalize \n",
    "\n",
    "# weighted aggregation through matrix multi seen above\n",
    "xbow2 = wei @ x # (T,T) @ (B,T,C) => pytorch adds batch dimension \n",
    "\n",
    "# verify\n",
    "# xbow[0], xbow2[0] --  same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb665cea",
   "metadata": {},
   "source": [
    "`(T,T) @ (B,T,C)` $\\implies$ pytorch adds batch dimension $\\implies$ `(B,T,T) @ (B,T,C)` $\\implies$ `(B) + (T,T @ T,C)` $\\implies$ `(B,T,C)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3 - using softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
